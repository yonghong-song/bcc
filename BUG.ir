; ModuleID = 'sscanf'
source_filename = "sscanf"

@fmt = private unnamed_addr constant [5 x i8] c"%i%n\00", align 1
@fmt.1 = private unnamed_addr constant [5 x i8] c"0x%x\00", align 1

; Function Attrs: nounwind
declare i32 @sscanf(i8* nocapture readonly, i8* nocapture readonly, ...) local_unnamed_addr #0

; Function Attrs: nounwind
define i32 @reader0(i8* nocapture readonly, i32*) local_unnamed_addr #0 {
  %3 = alloca i32, align 4
  store i32 0, i32* %3, align 4
  %4 = tail call i32 (i8*, i8*, ...) @sscanf(i8* %0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @fmt, i64 0, i64 0), i32* %1, i32* nonnull %3)
  %5 = icmp eq i32 %4, 1
  br i1 %5, label %7, label %6

; <label>:6:                                      ; preds = %2
  ret i32 -1

; <label>:7:                                      ; preds = %2
  %8 = load volatile i32, i32* %3, align 4
  ret i32 0
}

; Function Attrs: nounwind
define i32 @writer0(i8* nocapture, i64, i32* nocapture readonly) local_unnamed_addr #0 {
  %4 = load i32, i32* %2, align 4
  %5 = tail call i32 (i8*, i64, i8*, ...) @snprintf(i8* %0, i64 %1, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @fmt.1, i64 0, i64 0), i32 %4)
  ret i32 %5
}

; Function Attrs: nounwind
declare i32 @snprintf(i8* nocapture, i64, i8* nocapture readonly, ...) local_unnamed_addr #0

attributes #0 = { nounwind }
; ModuleID = '/virtual/main.c'
source_filename = "/virtual/main.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "bpf-pc-linux"

%struct.probe_SyS_nanosleep_1_events_table_t = type { i32, i32, i32 (i8*, i8*, i32)*, i32 (i8*, i32, i8*, i32)*, i32 }
%struct.pt_regs = type { i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 }
%struct.probe_SyS_nanosleep_1_data_t = type { i32, i32, [16 x i8] }

@_version = dso_local global i32 264963, section "version", align 4, !dbg !0
@probe_SyS_nanosleep_1_events = dso_local local_unnamed_addr global %struct.probe_SyS_nanosleep_1_events_table_t zeroinitializer, section "maps/perf_output", align 8, !dbg !30
@_license = dso_local global [4 x i8] c"GPL\00", section "license", align 1, !dbg !48
@llvm.used = appending global [9 x i8*] [i8* getelementptr inbounds ([4 x i8], [4 x i8]* @_license, i32 0, i32 0), i8* bitcast (i32* @_version to i8*), i8* bitcast (i64 (i8*, i64, i64, i64)* @bpf_dext_pkt to i8*), i8* bitcast (void (i8*, i64, i64, i64, i64)* @bpf_dins_pkt to i8*), i8* bitcast (i32 (i8*, i64, i64, i64, i64)* @bpf_l3_csum_replace_ to i8*), i8* bitcast (i32 (i8*, i64, i64, i64, i64)* @bpf_l4_csum_replace_ to i8*), i8* bitcast (i32 (i64, i8*)* @bpf_map_delete_elem_ to i8*), i8* bitcast (i8* (i64, i8*)* @bpf_map_lookup_elem_ to i8*), i8* bitcast (i32 (i64, i8*, i8*, i64)* @bpf_map_update_elem_ to i8*)], section "llvm.metadata"

; Function Attrs: alwaysinline nounwind readonly
define internal i64 @bpf_dext_pkt(i8*, i64, i64, i64) #0 section "helpers" !dbg !102 {
  call void @llvm.dbg.value(metadata i8* %0, metadata !106, metadata !DIExpression()), !dbg !110
  call void @llvm.dbg.value(metadata i64 %1, metadata !107, metadata !DIExpression()), !dbg !111
  call void @llvm.dbg.value(metadata i64 %2, metadata !108, metadata !DIExpression()), !dbg !112
  call void @llvm.dbg.value(metadata i64 %3, metadata !109, metadata !DIExpression()), !dbg !113
  %5 = icmp eq i64 %2, 0, !dbg !114
  %6 = icmp eq i64 %3, 8, !dbg !116
  %7 = and i1 %5, %6, !dbg !117
  br i1 %7, label %8, label %10, !dbg !117

; <label>:8:                                      ; preds = %4
  %9 = tail call i64 @llvm.bpf.load.byte(i8* %0, i64 %1), !dbg !118
  br label %78, !dbg !120

; <label>:10:                                     ; preds = %4
  %11 = add i64 %3, %2, !dbg !121
  %12 = icmp ult i64 %11, 9, !dbg !123
  br i1 %12, label %13, label %22, !dbg !124

; <label>:13:                                     ; preds = %10
  %14 = tail call i64 @llvm.bpf.load.byte(i8* %0, i64 %1), !dbg !125
  %15 = sub i64 8, %11, !dbg !127
  %16 = lshr i64 %14, %15, !dbg !128
  %17 = icmp ult i64 %3, 64, !dbg !129
  %18 = shl nsw i64 -1, %3, !dbg !129
  %19 = xor i64 %18, -1, !dbg !129
  %20 = select i1 %17, i64 %19, i64 -1, !dbg !129
  %21 = and i64 %16, %20, !dbg !130
  br label %78, !dbg !131

; <label>:22:                                     ; preds = %10
  %23 = icmp eq i64 %3, 16, !dbg !132
  %24 = and i1 %5, %23, !dbg !134
  br i1 %24, label %25, label %27, !dbg !134

; <label>:25:                                     ; preds = %22
  %26 = tail call i64 @llvm.bpf.load.half(i8* %0, i64 %1), !dbg !135
  br label %78, !dbg !137

; <label>:27:                                     ; preds = %22
  %28 = icmp ult i64 %11, 17, !dbg !138
  br i1 %28, label %29, label %38, !dbg !140

; <label>:29:                                     ; preds = %27
  %30 = tail call i64 @llvm.bpf.load.half(i8* %0, i64 %1), !dbg !141
  %31 = sub i64 16, %11, !dbg !143
  %32 = lshr i64 %30, %31, !dbg !144
  %33 = icmp ult i64 %3, 64, !dbg !145
  %34 = shl nsw i64 -1, %3, !dbg !145
  %35 = xor i64 %34, -1, !dbg !145
  %36 = select i1 %33, i64 %35, i64 -1, !dbg !145
  %37 = and i64 %32, %36, !dbg !146
  br label %78, !dbg !147

; <label>:38:                                     ; preds = %27
  %39 = icmp eq i64 %3, 32, !dbg !148
  %40 = and i1 %5, %39, !dbg !150
  br i1 %40, label %41, label %43, !dbg !150

; <label>:41:                                     ; preds = %38
  %42 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %1), !dbg !151
  br label %78, !dbg !153

; <label>:43:                                     ; preds = %38
  %44 = icmp ult i64 %11, 33, !dbg !154
  br i1 %44, label %45, label %54, !dbg !156

; <label>:45:                                     ; preds = %43
  %46 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %1), !dbg !157
  %47 = sub i64 32, %11, !dbg !159
  %48 = lshr i64 %46, %47, !dbg !160
  %49 = icmp ult i64 %3, 64, !dbg !161
  %50 = shl nsw i64 -1, %3, !dbg !161
  %51 = xor i64 %50, -1, !dbg !161
  %52 = select i1 %49, i64 %51, i64 -1, !dbg !161
  %53 = and i64 %48, %52, !dbg !162
  br label %78, !dbg !163

; <label>:54:                                     ; preds = %43
  %55 = icmp eq i64 %3, 64, !dbg !164
  %56 = and i1 %5, %55, !dbg !166
  br i1 %56, label %57, label %63, !dbg !166

; <label>:57:                                     ; preds = %54
  call void @llvm.dbg.value(metadata i8* %0, metadata !167, metadata !DIExpression()) #6, !dbg !173
  call void @llvm.dbg.value(metadata i64 %1, metadata !172, metadata !DIExpression()) #6, !dbg !176
  %58 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %1) #6, !dbg !177
  %59 = shl i64 %58, 32, !dbg !178
  %60 = add i64 %1, 4, !dbg !179
  %61 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %60) #6, !dbg !180
  %62 = or i64 %59, %61, !dbg !181
  br label %78, !dbg !182

; <label>:63:                                     ; preds = %54
  %64 = icmp ult i64 %11, 65, !dbg !183
  br i1 %64, label %65, label %78, !dbg !185

; <label>:65:                                     ; preds = %63
  call void @llvm.dbg.value(metadata i8* %0, metadata !167, metadata !DIExpression()) #6, !dbg !186
  call void @llvm.dbg.value(metadata i64 %1, metadata !172, metadata !DIExpression()) #6, !dbg !189
  %66 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %1) #6, !dbg !190
  %67 = shl i64 %66, 32, !dbg !191
  %68 = add i64 %1, 4, !dbg !192
  %69 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %68) #6, !dbg !193
  %70 = or i64 %67, %69, !dbg !194
  %71 = sub i64 64, %11, !dbg !195
  %72 = lshr i64 %70, %71, !dbg !196
  %73 = icmp ult i64 %3, 64, !dbg !197
  %74 = shl nsw i64 -1, %3, !dbg !197
  %75 = xor i64 %74, -1, !dbg !197
  %76 = select i1 %73, i64 %75, i64 -1, !dbg !197
  %77 = and i64 %72, %76, !dbg !198
  br label %78, !dbg !199

; <label>:78:                                     ; preds = %63, %65, %57, %45, %41, %29, %25, %13, %8
  %79 = phi i64 [ %9, %8 ], [ %21, %13 ], [ %26, %25 ], [ %37, %29 ], [ %42, %41 ], [ %53, %45 ], [ %62, %57 ], [ %77, %65 ], [ 0, %63 ], !dbg !200
  ret i64 %79, !dbg !201
}

; Function Attrs: alwaysinline nounwind readonly
declare dso_local i64 @llvm.bpf.load.byte(i8*, i64) #1

; Function Attrs: alwaysinline nounwind readonly
declare dso_local i64 @llvm.bpf.load.half(i8*, i64) #1

; Function Attrs: alwaysinline nounwind readonly
declare dso_local i64 @llvm.bpf.load.word(i8*, i64) #1

; Function Attrs: alwaysinline nounwind
define internal void @bpf_dins_pkt(i8*, i64, i64, i64, i64) #2 section "helpers" !dbg !202 {
  %6 = alloca i64, align 8
  %7 = alloca i8, align 1
  %8 = alloca i16, align 2
  %9 = alloca i16, align 2
  %10 = alloca i32, align 4
  %11 = alloca i32, align 4
  %12 = alloca i64, align 8
  %13 = alloca i64, align 8
  call void @llvm.dbg.value(metadata i8* %0, metadata !206, metadata !DIExpression()), !dbg !237
  call void @llvm.dbg.value(metadata i64 %1, metadata !207, metadata !DIExpression()), !dbg !238
  call void @llvm.dbg.value(metadata i64 %2, metadata !208, metadata !DIExpression()), !dbg !239
  call void @llvm.dbg.value(metadata i64 %3, metadata !209, metadata !DIExpression()), !dbg !240
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  store i64 %4, i64* %6, align 8, !tbaa !242
  %14 = icmp eq i64 %2, 0, !dbg !246
  %15 = icmp eq i64 %3, 8, !dbg !247
  %16 = and i1 %14, %15, !dbg !248
  br i1 %16, label %17, label %20, !dbg !248

; <label>:17:                                     ; preds = %5
  %18 = bitcast i64* %6 to i8*, !dbg !249
  %19 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %18, i64 1, i64 0) #6, !dbg !251
  br label %127, !dbg !252

; <label>:20:                                     ; preds = %5
  %21 = add i64 %3, %2, !dbg !253
  %22 = icmp ult i64 %21, 9, !dbg !254
  br i1 %22, label %23, label %40, !dbg !255

; <label>:23:                                     ; preds = %20
  call void @llvm.dbg.value(metadata i8* %7, metadata !211, metadata !DIExpression(DW_OP_deref)), !dbg !256
  call void @llvm.lifetime.start.p0i8(i64 1, i8* nonnull %7) #6, !dbg !257
  %24 = tail call i64 @llvm.bpf.load.byte(i8* %0, i64 %1), !dbg !258
  %25 = trunc i64 %24 to i8, !dbg !258
  call void @llvm.dbg.value(metadata i8 %25, metadata !211, metadata !DIExpression()), !dbg !256
  %26 = icmp ult i64 %3, 64, !dbg !259
  %27 = shl nsw i64 -1, %3, !dbg !259
  %28 = xor i64 %27, -1, !dbg !259
  %29 = select i1 %26, i64 %28, i64 -1, !dbg !259
  %30 = sub i64 8, %21, !dbg !260
  %31 = shl i64 %29, %30, !dbg !261
  call void @llvm.dbg.value(metadata i8 %25, metadata !211, metadata !DIExpression()), !dbg !256
  %32 = trunc i64 %31 to i8, !dbg !262
  %33 = xor i8 %32, -1, !dbg !262
  %34 = and i8 %25, %33, !dbg !262
  call void @llvm.dbg.value(metadata i8 %34, metadata !211, metadata !DIExpression()), !dbg !256
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  %35 = and i64 %29, %4, !dbg !263
  %36 = shl i64 %35, %30, !dbg !264
  call void @llvm.dbg.value(metadata i8 %34, metadata !211, metadata !DIExpression()), !dbg !256
  %37 = trunc i64 %36 to i8, !dbg !265
  %38 = or i8 %34, %37, !dbg !265
  call void @llvm.dbg.value(metadata i8 %38, metadata !211, metadata !DIExpression()), !dbg !256
  store i8 %38, i8* %7, align 1, !dbg !265, !tbaa !266
  call void @llvm.dbg.value(metadata i8* %7, metadata !211, metadata !DIExpression(DW_OP_deref)), !dbg !256
  %39 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %7, i64 1, i64 0) #6, !dbg !267
  call void @llvm.dbg.value(metadata i8* %7, metadata !211, metadata !DIExpression(DW_OP_deref)), !dbg !256
  call void @llvm.lifetime.end.p0i8(i64 1, i8* nonnull %7) #6, !dbg !268
  br label %127, !dbg !269

; <label>:40:                                     ; preds = %20
  %41 = icmp eq i64 %3, 16, !dbg !270
  %42 = and i1 %14, %41, !dbg !271
  br i1 %42, label %43, label %48, !dbg !271

; <label>:43:                                     ; preds = %40
  %44 = bitcast i16* %8 to i8*, !dbg !272
  call void @llvm.lifetime.start.p0i8(i64 2, i8* nonnull %44) #6, !dbg !272
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  %45 = trunc i64 %4 to i16, !dbg !273
  %46 = tail call i16 @llvm.bswap.i16(i16 %45) #6
  call void @llvm.dbg.value(metadata i16 %46, metadata !217, metadata !DIExpression()), !dbg !274
  store i16 %46, i16* %8, align 2, !dbg !274, !tbaa !275
  %47 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %44, i64 2, i64 0) #6, !dbg !277
  call void @llvm.lifetime.end.p0i8(i64 2, i8* nonnull %44) #6, !dbg !278
  br label %127, !dbg !279

; <label>:48:                                     ; preds = %40
  %49 = icmp ult i64 %21, 17, !dbg !280
  br i1 %49, label %50, label %69, !dbg !281

; <label>:50:                                     ; preds = %48
  %51 = bitcast i16* %9 to i8*, !dbg !282
  call void @llvm.lifetime.start.p0i8(i64 2, i8* nonnull %51) #6, !dbg !282
  %52 = tail call i64 @llvm.bpf.load.half(i8* %0, i64 %1), !dbg !283
  %53 = trunc i64 %52 to i16, !dbg !283
  call void @llvm.dbg.value(metadata i16 %53, metadata !222, metadata !DIExpression()), !dbg !284
  %54 = icmp ult i64 %3, 64, !dbg !285
  %55 = shl nsw i64 -1, %3, !dbg !285
  %56 = xor i64 %55, -1, !dbg !285
  %57 = select i1 %54, i64 %56, i64 -1, !dbg !285
  %58 = sub i64 16, %21, !dbg !286
  %59 = shl i64 %57, %58, !dbg !287
  call void @llvm.dbg.value(metadata i16 %53, metadata !222, metadata !DIExpression()), !dbg !284
  %60 = trunc i64 %59 to i16, !dbg !288
  %61 = xor i16 %60, -1, !dbg !288
  %62 = and i16 %53, %61, !dbg !288
  call void @llvm.dbg.value(metadata i16 %62, metadata !222, metadata !DIExpression()), !dbg !284
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  %63 = and i64 %57, %4, !dbg !289
  %64 = shl i64 %63, %58, !dbg !290
  call void @llvm.dbg.value(metadata i16 %62, metadata !222, metadata !DIExpression()), !dbg !284
  %65 = trunc i64 %64 to i16, !dbg !291
  %66 = or i16 %62, %65, !dbg !291
  call void @llvm.dbg.value(metadata i16 %66, metadata !222, metadata !DIExpression()), !dbg !284
  %67 = tail call i16 @llvm.bswap.i16(i16 %66) #6
  call void @llvm.dbg.value(metadata i16 %67, metadata !222, metadata !DIExpression()), !dbg !284
  store i16 %67, i16* %9, align 2, !dbg !292, !tbaa !275
  %68 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %51, i64 2, i64 0) #6, !dbg !293
  call void @llvm.lifetime.end.p0i8(i64 2, i8* nonnull %51) #6, !dbg !294
  br label %127, !dbg !295

; <label>:69:                                     ; preds = %48
  %70 = icmp eq i64 %3, 32, !dbg !296
  %71 = and i1 %14, %70, !dbg !297
  br i1 %71, label %72, label %77, !dbg !297

; <label>:72:                                     ; preds = %69
  %73 = bitcast i32* %10 to i8*, !dbg !298
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %73) #6, !dbg !298
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  %74 = trunc i64 %4 to i32, !dbg !299
  call void @llvm.dbg.value(metadata i32 %74, metadata !300, metadata !DIExpression()) #6, !dbg !305
  call void @llvm.dbg.value(metadata i32 %74, metadata !307, metadata !DIExpression()) #6, !dbg !310
  %75 = tail call i32 @llvm.bswap.i32(i32 %74) #6, !dbg !312
  call void @llvm.dbg.value(metadata i32 %75, metadata !225, metadata !DIExpression()), !dbg !313
  store i32 %75, i32* %10, align 4, !dbg !313, !tbaa !314
  %76 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %73, i64 4, i64 0) #6, !dbg !316
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %73) #6, !dbg !317
  br label %127, !dbg !318

; <label>:77:                                     ; preds = %69
  %78 = icmp ult i64 %21, 33, !dbg !319
  br i1 %78, label %79, label %98, !dbg !320

; <label>:79:                                     ; preds = %77
  %80 = bitcast i32* %11 to i8*, !dbg !321
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %80) #6, !dbg !321
  %81 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %1), !dbg !322
  %82 = trunc i64 %81 to i32, !dbg !322
  call void @llvm.dbg.value(metadata i32 %82, metadata !228, metadata !DIExpression()), !dbg !323
  %83 = icmp ult i64 %3, 64, !dbg !324
  %84 = shl nsw i64 -1, %3, !dbg !324
  %85 = xor i64 %84, -1, !dbg !324
  %86 = select i1 %83, i64 %85, i64 -1, !dbg !324
  %87 = sub i64 32, %21, !dbg !325
  %88 = shl i64 %86, %87, !dbg !326
  call void @llvm.dbg.value(metadata i32 %82, metadata !228, metadata !DIExpression()), !dbg !323
  %89 = trunc i64 %88 to i32, !dbg !327
  %90 = xor i32 %89, -1, !dbg !327
  %91 = and i32 %82, %90, !dbg !327
  call void @llvm.dbg.value(metadata i32 %91, metadata !228, metadata !DIExpression()), !dbg !323
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  %92 = and i64 %86, %4, !dbg !328
  %93 = shl i64 %92, %87, !dbg !329
  call void @llvm.dbg.value(metadata i32 %91, metadata !228, metadata !DIExpression()), !dbg !323
  %94 = trunc i64 %93 to i32, !dbg !330
  %95 = or i32 %91, %94, !dbg !330
  call void @llvm.dbg.value(metadata i32 %95, metadata !228, metadata !DIExpression()), !dbg !323
  call void @llvm.dbg.value(metadata i32 %95, metadata !300, metadata !DIExpression()) #6, !dbg !331
  call void @llvm.dbg.value(metadata i32 %95, metadata !307, metadata !DIExpression()) #6, !dbg !333
  %96 = tail call i32 @llvm.bswap.i32(i32 %95) #6, !dbg !335
  call void @llvm.dbg.value(metadata i32 %96, metadata !228, metadata !DIExpression()), !dbg !323
  store i32 %96, i32* %11, align 4, !dbg !336, !tbaa !314
  %97 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %80, i64 4, i64 0) #6, !dbg !337
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %80) #6, !dbg !338
  br label %127, !dbg !339

; <label>:98:                                     ; preds = %77
  %99 = icmp eq i64 %3, 64, !dbg !340
  %100 = and i1 %14, %99, !dbg !341
  br i1 %100, label %101, label %105, !dbg !341

; <label>:101:                                    ; preds = %98
  %102 = bitcast i64* %12 to i8*, !dbg !342
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %102) #6, !dbg !342
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  call void @llvm.dbg.value(metadata i64 %4, metadata !343, metadata !DIExpression()) #6, !dbg !348
  call void @llvm.dbg.value(metadata i64 %4, metadata !350, metadata !DIExpression()) #6, !dbg !353
  %103 = tail call i64 @llvm.bswap.i64(i64 %4) #6, !dbg !355
  call void @llvm.dbg.value(metadata i64 %103, metadata !231, metadata !DIExpression()), !dbg !356
  store i64 %103, i64* %12, align 8, !dbg !356, !tbaa !242
  %104 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %102, i64 8, i64 0) #6, !dbg !357
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %102) #6, !dbg !358
  br label %127, !dbg !359

; <label>:105:                                    ; preds = %98
  %106 = icmp ult i64 %21, 65, !dbg !360
  br i1 %106, label %107, label %127, !dbg !361

; <label>:107:                                    ; preds = %105
  %108 = bitcast i64* %13 to i8*, !dbg !362
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %108) #6, !dbg !362
  call void @llvm.dbg.value(metadata i8* %0, metadata !167, metadata !DIExpression()) #6, !dbg !363
  call void @llvm.dbg.value(metadata i64 %1, metadata !172, metadata !DIExpression()) #6, !dbg !365
  %109 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %1) #6, !dbg !366
  %110 = shl i64 %109, 32, !dbg !367
  %111 = add i64 %1, 4, !dbg !368
  %112 = tail call i64 @llvm.bpf.load.word(i8* %0, i64 %111) #6, !dbg !369
  %113 = or i64 %110, %112, !dbg !370
  call void @llvm.dbg.value(metadata i64 %113, metadata !234, metadata !DIExpression()), !dbg !371
  %114 = icmp ult i64 %3, 64, !dbg !372
  %115 = shl nsw i64 -1, %3, !dbg !372
  %116 = xor i64 %115, -1, !dbg !372
  %117 = select i1 %114, i64 %116, i64 -1, !dbg !372
  %118 = sub i64 64, %21, !dbg !373
  %119 = shl i64 %117, %118, !dbg !374
  %120 = xor i64 %119, -1, !dbg !375
  call void @llvm.dbg.value(metadata i64 %113, metadata !234, metadata !DIExpression()), !dbg !371
  %121 = and i64 %113, %120, !dbg !376
  call void @llvm.dbg.value(metadata i64 %121, metadata !234, metadata !DIExpression()), !dbg !371
  call void @llvm.dbg.value(metadata i64 %4, metadata !210, metadata !DIExpression()), !dbg !241
  %122 = and i64 %117, %4, !dbg !377
  %123 = shl i64 %122, %118, !dbg !378
  call void @llvm.dbg.value(metadata i64 %121, metadata !234, metadata !DIExpression()), !dbg !371
  %124 = or i64 %121, %123, !dbg !379
  call void @llvm.dbg.value(metadata i64 %124, metadata !234, metadata !DIExpression()), !dbg !371
  call void @llvm.dbg.value(metadata i64 %124, metadata !343, metadata !DIExpression()) #6, !dbg !380
  call void @llvm.dbg.value(metadata i64 %124, metadata !350, metadata !DIExpression()) #6, !dbg !382
  %125 = tail call i64 @llvm.bswap.i64(i64 %124) #6, !dbg !384
  call void @llvm.dbg.value(metadata i64 %125, metadata !234, metadata !DIExpression()), !dbg !371
  store i64 %125, i64* %13, align 8, !dbg !385, !tbaa !242
  %126 = call i32 inttoptr (i64 9 to i32 (i8*, i64, i8*, i64, i64)*)(i8* %0, i64 %1, i8* nonnull %108, i64 8, i64 0) #6, !dbg !386
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %108) #6, !dbg !387
  br label %127, !dbg !388

; <label>:127:                                    ; preds = %23, %50, %79, %105, %107, %101, %72, %43, %17
  ret void, !dbg !389
}

; Function Attrs: alwaysinline argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64, i8* nocapture) #3

; Function Attrs: alwaysinline argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64, i8* nocapture) #3

; Function Attrs: alwaysinline nounwind
define internal i8* @bpf_map_lookup_elem_(i64, i8*) #2 section "helpers" !dbg !390 {
  call void @llvm.dbg.value(metadata i64 %0, metadata !397, metadata !DIExpression()), !dbg !399
  call void @llvm.dbg.value(metadata i8* %1, metadata !398, metadata !DIExpression()), !dbg !400
  %3 = inttoptr i64 %0 to i8*, !dbg !401
  %4 = tail call i8* inttoptr (i64 1 to i8* (i8*, i8*)*)(i8* %3, i8* %1) #6, !dbg !402
  ret i8* %4, !dbg !403
}

; Function Attrs: alwaysinline nounwind
define internal i32 @bpf_map_update_elem_(i64, i8*, i8*, i64) #2 section "helpers" !dbg !404 {
  call void @llvm.dbg.value(metadata i64 %0, metadata !408, metadata !DIExpression()), !dbg !412
  call void @llvm.dbg.value(metadata i8* %1, metadata !409, metadata !DIExpression()), !dbg !413
  call void @llvm.dbg.value(metadata i8* %2, metadata !410, metadata !DIExpression()), !dbg !414
  call void @llvm.dbg.value(metadata i64 %3, metadata !411, metadata !DIExpression()), !dbg !415
  %5 = inttoptr i64 %0 to i8*, !dbg !416
  %6 = tail call i32 inttoptr (i64 2 to i32 (i8*, i8*, i8*, i64)*)(i8* %5, i8* %1, i8* %2, i64 %3) #6, !dbg !417
  ret i32 %6, !dbg !418
}

; Function Attrs: alwaysinline nounwind
define internal i32 @bpf_map_delete_elem_(i64, i8*) #2 section "helpers" !dbg !419 {
  call void @llvm.dbg.value(metadata i64 %0, metadata !423, metadata !DIExpression()), !dbg !425
  call void @llvm.dbg.value(metadata i8* %1, metadata !424, metadata !DIExpression()), !dbg !426
  %3 = inttoptr i64 %0 to i8*, !dbg !427
  %4 = tail call i32 inttoptr (i64 3 to i32 (i8*, i8*)*)(i8* %3, i8* %1) #6, !dbg !428
  ret i32 %4, !dbg !429
}

; Function Attrs: alwaysinline nounwind
define internal i32 @bpf_l3_csum_replace_(i8*, i64, i64, i64, i64) #2 section "helpers" !dbg !430 {
  call void @llvm.dbg.value(metadata i8* %0, metadata !434, metadata !DIExpression()), !dbg !439
  call void @llvm.dbg.value(metadata i64 %1, metadata !435, metadata !DIExpression()), !dbg !440
  call void @llvm.dbg.value(metadata i64 %2, metadata !436, metadata !DIExpression()), !dbg !441
  call void @llvm.dbg.value(metadata i64 %3, metadata !437, metadata !DIExpression()), !dbg !442
  call void @llvm.dbg.value(metadata i64 %4, metadata !438, metadata !DIExpression()), !dbg !443
  %6 = trunc i64 %4 to i4, !dbg !444
  switch i4 %6, label %27 [
    i4 2, label %7
    i4 4, label %15
    i4 -8, label %23
  ], !dbg !444

; <label>:7:                                      ; preds = %5
  %8 = trunc i64 %2 to i16, !dbg !445
  %9 = tail call i16 @llvm.bswap.i16(i16 %8) #6
  %10 = zext i16 %9 to i64, !dbg !447
  %11 = trunc i64 %3 to i16, !dbg !448
  %12 = tail call i16 @llvm.bswap.i16(i16 %11) #6
  %13 = zext i16 %12 to i64, !dbg !449
  %14 = tail call i32 inttoptr (i64 10 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %10, i64 %13, i64 %4) #6, !dbg !450
  br label %29, !dbg !451

; <label>:15:                                     ; preds = %5
  %16 = trunc i64 %2 to i32, !dbg !452
  call void @llvm.dbg.value(metadata i32 %16, metadata !300, metadata !DIExpression()) #6, !dbg !453
  call void @llvm.dbg.value(metadata i32 %16, metadata !307, metadata !DIExpression()) #6, !dbg !455
  %17 = tail call i32 @llvm.bswap.i32(i32 %16) #6, !dbg !457
  %18 = zext i32 %17 to i64, !dbg !458
  %19 = trunc i64 %3 to i32, !dbg !459
  call void @llvm.dbg.value(metadata i32 %19, metadata !300, metadata !DIExpression()) #6, !dbg !460
  call void @llvm.dbg.value(metadata i32 %19, metadata !307, metadata !DIExpression()) #6, !dbg !462
  %20 = tail call i32 @llvm.bswap.i32(i32 %19) #6, !dbg !464
  %21 = zext i32 %20 to i64, !dbg !465
  %22 = tail call i32 inttoptr (i64 10 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %18, i64 %21, i64 %4) #6, !dbg !466
  br label %29, !dbg !467

; <label>:23:                                     ; preds = %5
  call void @llvm.dbg.value(metadata i64 %2, metadata !343, metadata !DIExpression()) #6, !dbg !468
  call void @llvm.dbg.value(metadata i64 %2, metadata !350, metadata !DIExpression()) #6, !dbg !470
  %24 = tail call i64 @llvm.bswap.i64(i64 %2) #6, !dbg !472
  call void @llvm.dbg.value(metadata i64 %3, metadata !343, metadata !DIExpression()) #6, !dbg !473
  call void @llvm.dbg.value(metadata i64 %3, metadata !350, metadata !DIExpression()) #6, !dbg !475
  %25 = tail call i64 @llvm.bswap.i64(i64 %3) #6, !dbg !477
  %26 = tail call i32 inttoptr (i64 10 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %24, i64 %25, i64 %4) #6, !dbg !478
  br label %29, !dbg !479

; <label>:27:                                     ; preds = %5
  %28 = tail call i32 inttoptr (i64 10 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %2, i64 %3, i64 %4) #6, !dbg !480
  br label %29, !dbg !481

; <label>:29:                                     ; preds = %27, %23, %15, %7
  %30 = phi i32 [ %28, %27 ], [ %26, %23 ], [ %22, %15 ], [ %14, %7 ], !dbg !482
  ret i32 %30, !dbg !483
}

; Function Attrs: alwaysinline nounwind
define internal i32 @bpf_l4_csum_replace_(i8*, i64, i64, i64, i64) #2 section "helpers" !dbg !484 {
  call void @llvm.dbg.value(metadata i8* %0, metadata !486, metadata !DIExpression()), !dbg !491
  call void @llvm.dbg.value(metadata i64 %1, metadata !487, metadata !DIExpression()), !dbg !492
  call void @llvm.dbg.value(metadata i64 %2, metadata !488, metadata !DIExpression()), !dbg !493
  call void @llvm.dbg.value(metadata i64 %3, metadata !489, metadata !DIExpression()), !dbg !494
  call void @llvm.dbg.value(metadata i64 %4, metadata !490, metadata !DIExpression()), !dbg !495
  %6 = trunc i64 %4 to i4, !dbg !496
  switch i4 %6, label %27 [
    i4 2, label %7
    i4 4, label %15
    i4 -8, label %23
  ], !dbg !496

; <label>:7:                                      ; preds = %5
  %8 = trunc i64 %2 to i16, !dbg !497
  %9 = tail call i16 @llvm.bswap.i16(i16 %8) #6
  %10 = zext i16 %9 to i64, !dbg !499
  %11 = trunc i64 %3 to i16, !dbg !500
  %12 = tail call i16 @llvm.bswap.i16(i16 %11) #6
  %13 = zext i16 %12 to i64, !dbg !501
  %14 = tail call i32 inttoptr (i64 11 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %10, i64 %13, i64 %4) #6, !dbg !502
  br label %29, !dbg !503

; <label>:15:                                     ; preds = %5
  %16 = trunc i64 %2 to i32, !dbg !504
  call void @llvm.dbg.value(metadata i32 %16, metadata !300, metadata !DIExpression()) #6, !dbg !505
  call void @llvm.dbg.value(metadata i32 %16, metadata !307, metadata !DIExpression()) #6, !dbg !507
  %17 = tail call i32 @llvm.bswap.i32(i32 %16) #6, !dbg !509
  %18 = zext i32 %17 to i64, !dbg !510
  %19 = trunc i64 %3 to i32, !dbg !511
  call void @llvm.dbg.value(metadata i32 %19, metadata !300, metadata !DIExpression()) #6, !dbg !512
  call void @llvm.dbg.value(metadata i32 %19, metadata !307, metadata !DIExpression()) #6, !dbg !514
  %20 = tail call i32 @llvm.bswap.i32(i32 %19) #6, !dbg !516
  %21 = zext i32 %20 to i64, !dbg !517
  %22 = tail call i32 inttoptr (i64 11 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %18, i64 %21, i64 %4) #6, !dbg !518
  br label %29, !dbg !519

; <label>:23:                                     ; preds = %5
  call void @llvm.dbg.value(metadata i64 %2, metadata !343, metadata !DIExpression()) #6, !dbg !520
  call void @llvm.dbg.value(metadata i64 %2, metadata !350, metadata !DIExpression()) #6, !dbg !522
  %24 = tail call i64 @llvm.bswap.i64(i64 %2) #6, !dbg !524
  call void @llvm.dbg.value(metadata i64 %3, metadata !343, metadata !DIExpression()) #6, !dbg !525
  call void @llvm.dbg.value(metadata i64 %3, metadata !350, metadata !DIExpression()) #6, !dbg !527
  %25 = tail call i64 @llvm.bswap.i64(i64 %3) #6, !dbg !529
  %26 = tail call i32 inttoptr (i64 11 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %24, i64 %25, i64 %4) #6, !dbg !530
  br label %29, !dbg !531

; <label>:27:                                     ; preds = %5
  %28 = tail call i32 inttoptr (i64 11 to i32 (i8*, i64, i64, i64, i64)*)(i8* %0, i64 %1, i64 %2, i64 %3, i64 %4) #6, !dbg !532
  br label %29, !dbg !533

; <label>:29:                                     ; preds = %27, %23, %15, %7
  %30 = phi i32 [ %28, %27 ], [ %26, %23 ], [ %22, %15 ], [ %14, %7 ], !dbg !534
  ret i32 %30, !dbg !535
}

; Function Attrs: alwaysinline nounwind
define dso_local i32 @probe_SyS_nanosleep_1(%struct.pt_regs*) local_unnamed_addr #2 section ".bpf.fn.probe_SyS_nanosleep_1" !dbg !536 {
  %2 = alloca %struct.probe_SyS_nanosleep_1_data_t, align 4
  call void @llvm.dbg.value(metadata %struct.pt_regs* %0, metadata !565, metadata !DIExpression()), !dbg !578
  %3 = tail call i64 inttoptr (i64 14 to i64 ()*)() #6, !dbg !579
  call void @llvm.dbg.value(metadata i64 %3, metadata !566, metadata !DIExpression()), !dbg !580
  %4 = lshr i64 %3, 32, !dbg !581
  %5 = trunc i64 %4 to i32, !dbg !582
  call void @llvm.dbg.value(metadata i32 %5, metadata !567, metadata !DIExpression()), !dbg !583
  %6 = icmp eq i32 %5, 3027415, !dbg !584
  br i1 %6, label %18, label %7, !dbg !586

; <label>:7:                                      ; preds = %1
  %8 = trunc i64 %3 to i32, !dbg !587
  call void @llvm.dbg.value(metadata i32 %8, metadata !568, metadata !DIExpression()), !dbg !588
  %9 = bitcast %struct.probe_SyS_nanosleep_1_data_t* %2 to i8*, !dbg !589
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %9) #6, !dbg !589
  %10 = getelementptr inbounds %struct.probe_SyS_nanosleep_1_data_t, %struct.probe_SyS_nanosleep_1_data_t* %2, i64 0, i32 2, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 4 %10, i8 0, i64 16, i1 false), !dbg !590
  %11 = getelementptr inbounds %struct.probe_SyS_nanosleep_1_data_t, %struct.probe_SyS_nanosleep_1_data_t* %2, i64 0, i32 0, !dbg !591
  store i32 %5, i32* %11, align 4, !dbg !592, !tbaa !593
  %12 = getelementptr inbounds %struct.probe_SyS_nanosleep_1_data_t, %struct.probe_SyS_nanosleep_1_data_t* %2, i64 0, i32 1, !dbg !595
  store i32 %8, i32* %12, align 4, !dbg !596, !tbaa !597
  %13 = call i32 inttoptr (i64 16 to i32 (i8*, i32)*)(i8* nonnull %10, i32 16) #6, !dbg !598
  %14 = bitcast %struct.pt_regs* %0 to i8*, !dbg !599
  %15 = call i64 @llvm.bpf.pseudo(i64 1, i64 3), !dbg !600
  %16 = inttoptr i64 %15 to i8*, !dbg !600
  %17 = call i32 inttoptr (i64 25 to i32 (i8*, i8*, i64, i8*, i32)*)(i8* %14, i8* %16, i64 4294967295, i8* nonnull %9, i32 24) #6, !dbg !601
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %9) #6, !dbg !602
  br label %18

; <label>:18:                                     ; preds = %1, %7
  ret i32 0, !dbg !602
}

; Function Attrs: alwaysinline argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1) #3

; Function Attrs: alwaysinline nounwind
declare i64 @llvm.bpf.pseudo(i64, i64) #4

; Function Attrs: alwaysinline nounwind readnone speculatable
declare i32 @llvm.bswap.i32(i32) #5

; Function Attrs: alwaysinline nounwind readnone speculatable
declare i64 @llvm.bswap.i64(i64) #5

; Function Attrs: alwaysinline nounwind readnone speculatable
declare void @llvm.dbg.value(metadata, metadata, metadata) #5

; Function Attrs: alwaysinline nounwind readnone speculatable
declare i16 @llvm.bswap.i16(i16) #5

attributes #0 = { alwaysinline nounwind readonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { alwaysinline nounwind readonly }
attributes #2 = { alwaysinline nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { alwaysinline argmemonly nounwind }
attributes #4 = { alwaysinline nounwind }
attributes #5 = { alwaysinline nounwind readnone speculatable }
attributes #6 = { nounwind }

!llvm.dbg.cu = !{!2}
!llvm.module.flags = !{!98, !99, !100}
!llvm.ident = !{!101}

!0 = !DIGlobalVariableExpression(var: !1, expr: !DIExpression())
!1 = distinct !DIGlobalVariable(name: "_version", scope: !2, file: !57, line: 234, type: !7, isLocal: false, isDefinition: true)
!2 = distinct !DICompileUnit(language: DW_LANG_C99, file: !3, producer: "clang version 8.0.0 (https://github.com/yonghong-song/clang.git 6d4408ce50e9bacfce46e2e32eb4b2318e2e6992) (https://github.com/yonghong-song/llvm.git d043791034148b50b5479124613a622f2b17cb7e)", isOptimized: true, runtimeVersion: 0, emissionKind: FullDebug, enums: !4, retainedTypes: !24, globals: !29, nameTableKind: None)
!3 = !DIFile(filename: "/virtual/main.c", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "0ac169f959e9e02af8441047809dd641", source: "#if defined(BPF_LICENSE)\0A#error BPF_LICENSE cannot be specified through cflags\0A#endif\0A#if !defined(CONFIG_CC_STACKPROTECTOR)\0A#if defined(CONFIG_CC_STACKPROTECTOR_AUTO) \5C\0A    || defined(CONFIG_CC_STACKPROTECTOR_REGULAR) \5C\0A    || defined(CONFIG_CC_STACKPROTECTOR_STRONG)\0A#define CONFIG_CC_STACKPROTECTOR\0A#endif\0A#endif\0A\0A#include <linux/ptrace.h>\0A#include <linux/sched.h>        /* For TASK_COMM_LEN */\0A\0A\0Astruct probe_SyS_nanosleep_1_data_t\0A{\0A\0A\0A        u32 tgid;\0A        u32 pid;\0A        char comm[TASK_COMM_LEN];\0A\0A\0A\0A};\0A\0ABPF_PERF_OUTPUT(probe_SyS_nanosleep_1_events);\0A\0A\0A__attribute__((section(\22.bpf.fn.probe_SyS_nanosleep_1\22)))\0Aint probe_SyS_nanosleep_1(struct pt_regs *ctx)\0A{\0A\0A        u64 __pid_tgid = bpf_get_current_pid_tgid();\0A        u32 __tgid = __pid_tgid >> 32;\0A        u32 __pid = __pid_tgid; // implicit cast to u32 for bottom half\0A        \0A        if (__tgid == 3027415) { return 0; }\0A                \0A        \0A        \0A        if (!(1)) return 0;\0A\0A        struct probe_SyS_nanosleep_1_data_t __data = {0};\0A        \0A        \0A        __data.tgid = __tgid;\0A        __data.pid = __pid;\0A        bpf_get_current_comm(&__data.comm, sizeof(__data.comm));\0A\0A\0A        bpf_perf_event_output(ctx, bpf_pseudo_fd(1, 3), CUR_CPU_IDENTIFIER, &__data, sizeof(__data));\0A        return 0;\0A}\0A\0A#include <bcc/footer.h>\0A")
!4 = !{!5, !11, !17}
!5 = !DICompositeType(tag: DW_TAG_enumeration_type, name: "hrtimer_restart", file: !6, line: 43, baseType: !7, size: 32, elements: !8)
!6 = !DIFile(filename: "/lib/modules/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f/build/include/linux/hrtimer.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "4243d0566173049da2d63740ed93ff80", source: "/*\0A *  include/linux/hrtimer.h\0A *\0A *  hrtimers - High-resolution kernel timers\0A *\0A *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>\0A *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar\0A *\0A *  data type definitions, declarations, prototypes\0A *\0A *  Started by: Thomas Gleixner and Ingo Molnar\0A *\0A *  For licencing details see kernel-base/COPYING\0A */\0A#ifndef _LINUX_HRTIMER_H\0A#define _LINUX_HRTIMER_H\0A\0A#include <linux/rbtree.h>\0A#include <linux/ktime.h>\0A#include <linux/init.h>\0A#include <linux/list.h>\0A#include <linux/percpu.h>\0A#include <linux/timer.h>\0A#include <linux/timerqueue.h>\0A\0Astruct hrtimer_clock_base;\0Astruct hrtimer_cpu_base;\0A\0A/*\0A * Mode arguments of xxx_hrtimer functions:\0A */\0Aenum hrtimer_mode {\0A\09HRTIMER_MODE_ABS = 0x0,\09\09/* Time value is absolute */\0A\09HRTIMER_MODE_REL = 0x1,\09\09/* Time value is relative to now */\0A\09HRTIMER_MODE_PINNED = 0x02,\09/* Timer is bound to CPU */\0A\09HRTIMER_MODE_ABS_PINNED = 0x02,\0A\09HRTIMER_MODE_REL_PINNED = 0x03,\0A};\0A\0A/*\0A * Return values for the callback function\0A */\0Aenum hrtimer_restart {\0A\09HRTIMER_NORESTART,\09/* Timer is not restarted */\0A\09HRTIMER_RESTART,\09/* Timer must be restarted */\0A};\0A\0A/*\0A * Values to track state of the timer\0A *\0A * Possible states:\0A *\0A * 0x00\09\09inactive\0A * 0x01\09\09enqueued into rbtree\0A *\0A * The callback state is not part of the timer->state because clearing it would\0A * mean touching the timer after the callback, this makes it impossible to free\0A * the timer from the callback function.\0A *\0A * Therefore we track the callback state in:\0A *\0A *\09timer->base->cpu_base->running == timer\0A *\0A * On SMP it is possible to have a \22callback function running and enqueued\22\0A * status. It happens for example when a posix timer expired and the callback\0A * queued a signal. Between dropping the lock which protects the posix timer\0A * and reacquiring the base lock of the hrtimer, another CPU can deliver the\0A * signal and rearm the timer.\0A *\0A * All state transitions are protected by cpu_base->lock.\0A */\0A#define HRTIMER_STATE_INACTIVE\090x00\0A#define HRTIMER_STATE_ENQUEUED\090x01\0A\0A/**\0A * struct hrtimer - the basic hrtimer structure\0A * @node:\09timerqueue node, which also manages node.expires,\0A *\09\09the absolute expiry time in the hrtimers internal\0A *\09\09representation. The time is related to the clock on\0A *\09\09which the timer is based. Is setup by adding\0A *\09\09slack to the _softexpires value. For non range timers\0A *\09\09identical to _softexpires.\0A * @_softexpires: the absolute earliest expiry time of the hrtimer.\0A *\09\09The time which was given as expiry time when the timer\0A *\09\09was armed.\0A * @function:\09timer expiry callback function\0A * @base:\09pointer to the timer base (per cpu and per clock)\0A * @state:\09state information (See bit values above)\0A * @is_rel:\09Set if the timer was armed relative\0A *\0A * The hrtimer structure must be initialized by hrtimer_init()\0A */\0Astruct hrtimer {\0A\09struct timerqueue_node\09\09node;\0A\09ktime_t\09\09\09\09_softexpires;\0A\09enum hrtimer_restart\09\09(*function)(struct hrtimer *);\0A\09struct hrtimer_clock_base\09*base;\0A\09u8\09\09\09\09state;\0A\09u8\09\09\09\09is_rel;\0A};\0A\0A/**\0A * struct hrtimer_sleeper - simple sleeper structure\0A * @timer:\09embedded timer structure\0A * @task:\09task to wake up\0A *\0A * task is set to NULL, when the timer expires.\0A */\0Astruct hrtimer_sleeper {\0A\09struct hrtimer timer;\0A\09struct task_struct *task;\0A};\0A\0A#ifdef CONFIG_64BIT\0A# define HRTIMER_CLOCK_BASE_ALIGN\0964\0A#else\0A# define HRTIMER_CLOCK_BASE_ALIGN\0932\0A#endif\0A\0A/**\0A * struct hrtimer_clock_base - the timer base for a specific clock\0A * @cpu_base:\09\09per cpu clock base\0A * @index:\09\09clock type index for per_cpu support when moving a\0A *\09\09\09timer to a base on another cpu.\0A * @clockid:\09\09clock id for per_cpu support\0A * @active:\09\09red black tree root node for the active timers\0A * @get_time:\09\09function to retrieve the current time of the clock\0A * @offset:\09\09offset of this clock to the monotonic base\0A */\0Astruct hrtimer_clock_base {\0A\09struct hrtimer_cpu_base\09*cpu_base;\0A\09int\09\09\09index;\0A\09clockid_t\09\09clockid;\0A\09struct timerqueue_head\09active;\0A\09ktime_t\09\09\09(*get_time)(void);\0A\09ktime_t\09\09\09offset;\0A} __attribute__((__aligned__(HRTIMER_CLOCK_BASE_ALIGN)));\0A\0Aenum  hrtimer_base_type {\0A\09HRTIMER_BASE_MONOTONIC,\0A\09HRTIMER_BASE_REALTIME,\0A\09HRTIMER_BASE_BOOTTIME,\0A\09HRTIMER_BASE_TAI,\0A\09HRTIMER_MAX_CLOCK_BASES,\0A};\0A\0A/*\0A * struct hrtimer_cpu_base - the per cpu clock bases\0A * @lock:\09\09lock protecting the base and associated clock bases\0A *\09\09\09and timers\0A * @seq:\09\09seqcount around __run_hrtimer\0A * @running:\09\09pointer to the currently running hrtimer\0A * @cpu:\09\09cpu number\0A * @active_bases:\09Bitfield to mark bases with active timers\0A * @clock_was_set_seq:\09Sequence counter of clock was set events\0A * @migration_enabled:\09The migration of hrtimers to other cpus is enabled\0A * @nohz_active:\09The nohz functionality is enabled\0A * @expires_next:\09absolute time of the next event which was scheduled\0A *\09\09\09via clock_set_next_event()\0A * @next_timer:\09\09Pointer to the first expiring timer\0A * @in_hrtirq:\09\09hrtimer_interrupt() is currently executing\0A * @hres_active:\09State of high resolution mode\0A * @hang_detected:\09The last hrtimer interrupt detected a hang\0A * @nr_events:\09\09Total number of hrtimer interrupt events\0A * @nr_retries:\09\09Total number of hrtimer interrupt retries\0A * @nr_hangs:\09\09Total number of hrtimer interrupt hangs\0A * @max_hang_time:\09Maximum time spent in hrtimer_interrupt\0A * @clock_base:\09\09array of clock bases for this cpu\0A *\0A * Note: next_timer is just an optimization for __remove_hrtimer().\0A *\09 Do not dereference the pointer because it is not reliable on\0A *\09 cross cpu removals.\0A */\0Astruct hrtimer_cpu_base {\0A\09raw_spinlock_t\09\09\09lock;\0A\09seqcount_t\09\09\09seq;\0A\09struct hrtimer\09\09\09*running;\0A\09unsigned int\09\09\09cpu;\0A\09unsigned int\09\09\09active_bases;\0A\09unsigned int\09\09\09clock_was_set_seq;\0A\09bool\09\09\09\09migration_enabled;\0A\09bool\09\09\09\09nohz_active;\0A#ifdef CONFIG_HIGH_RES_TIMERS\0A\09unsigned int\09\09\09in_hrtirq\09: 1,\0A\09\09\09\09\09hres_active\09: 1,\0A\09\09\09\09\09hang_detected\09: 1;\0A\09ktime_t\09\09\09\09expires_next;\0A\09struct hrtimer\09\09\09*next_timer;\0A\09unsigned int\09\09\09nr_events;\0A\09unsigned int\09\09\09nr_retries;\0A\09unsigned int\09\09\09nr_hangs;\0A\09unsigned int\09\09\09max_hang_time;\0A#endif\0A\09struct hrtimer_clock_base\09clock_base[HRTIMER_MAX_CLOCK_BASES];\0A} ____cacheline_aligned;\0A\0Astatic inline void hrtimer_set_expires(struct hrtimer *timer, ktime_t time)\0A{\0A\09BUILD_BUG_ON(sizeof(struct hrtimer_clock_base) > HRTIMER_CLOCK_BASE_ALIGN);\0A\0A\09timer->node.expires = time;\0A\09timer->_softexpires = time;\0A}\0A\0Astatic inline void hrtimer_set_expires_range(struct hrtimer *timer, ktime_t time, ktime_t delta)\0A{\0A\09timer->_softexpires = time;\0A\09timer->node.expires = ktime_add_safe(time, delta);\0A}\0A\0Astatic inline void hrtimer_set_expires_range_ns(struct hrtimer *timer, ktime_t time, u64 delta)\0A{\0A\09timer->_softexpires = time;\0A\09timer->node.expires = ktime_add_safe(time, ns_to_ktime(delta));\0A}\0A\0Astatic inline void hrtimer_set_expires_tv64(struct hrtimer *timer, s64 tv64)\0A{\0A\09timer->node.expires = tv64;\0A\09timer->_softexpires = tv64;\0A}\0A\0Astatic inline void hrtimer_add_expires(struct hrtimer *timer, ktime_t time)\0A{\0A\09timer->node.expires = ktime_add_safe(timer->node.expires, time);\0A\09timer->_softexpires = ktime_add_safe(timer->_softexpires, time);\0A}\0A\0Astatic inline void hrtimer_add_expires_ns(struct hrtimer *timer, u64 ns)\0A{\0A\09timer->node.expires = ktime_add_ns(timer->node.expires, ns);\0A\09timer->_softexpires = ktime_add_ns(timer->_softexpires, ns);\0A}\0A\0Astatic inline ktime_t hrtimer_get_expires(const struct hrtimer *timer)\0A{\0A\09return timer->node.expires;\0A}\0A\0Astatic inline ktime_t hrtimer_get_softexpires(const struct hrtimer *timer)\0A{\0A\09return timer->_softexpires;\0A}\0A\0Astatic inline s64 hrtimer_get_expires_tv64(const struct hrtimer *timer)\0A{\0A\09return timer->node.expires;\0A}\0Astatic inline s64 hrtimer_get_softexpires_tv64(const struct hrtimer *timer)\0A{\0A\09return timer->_softexpires;\0A}\0A\0Astatic inline s64 hrtimer_get_expires_ns(const struct hrtimer *timer)\0A{\0A\09return ktime_to_ns(timer->node.expires);\0A}\0A\0Astatic inline ktime_t hrtimer_expires_remaining(const struct hrtimer *timer)\0A{\0A\09return ktime_sub(timer->node.expires, timer->base->get_time());\0A}\0A\0Astatic inline ktime_t hrtimer_cb_get_time(struct hrtimer *timer)\0A{\0A\09return timer->base->get_time();\0A}\0A\0A#ifdef CONFIG_HIGH_RES_TIMERS\0Astruct clock_event_device;\0A\0Aextern void hrtimer_interrupt(struct clock_event_device *dev);\0A\0Astatic inline int hrtimer_is_hres_active(struct hrtimer *timer)\0A{\0A\09return timer->base->cpu_base->hres_active;\0A}\0A\0Aextern void hrtimer_peek_ahead_timers(void);\0A\0A/*\0A * The resolution of the clocks. The resolution value is returned in\0A * the clock_getres() system call to give application programmers an\0A * idea of the (in)accuracy of timers. Timer values are rounded up to\0A * this resolution values.\0A */\0A# define HIGH_RES_NSEC\09\091\0A# define KTIME_HIGH_RES\09\09(HIGH_RES_NSEC)\0A# define MONOTONIC_RES_NSEC\09HIGH_RES_NSEC\0A# define KTIME_MONOTONIC_RES\09KTIME_HIGH_RES\0A\0Aextern void clock_was_set_delayed(void);\0A\0Aextern unsigned int hrtimer_resolution;\0A\0A#else\0A\0A# define MONOTONIC_RES_NSEC\09LOW_RES_NSEC\0A# define KTIME_MONOTONIC_RES\09KTIME_LOW_RES\0A\0A#define hrtimer_resolution\09(unsigned int)LOW_RES_NSEC\0A\0Astatic inline void hrtimer_peek_ahead_timers(void) { }\0A\0Astatic inline int hrtimer_is_hres_active(struct hrtimer *timer)\0A{\0A\09return 0;\0A}\0A\0Astatic inline void clock_was_set_delayed(void) { }\0A\0A#endif\0A\0Astatic inline ktime_t\0A__hrtimer_expires_remaining_adjusted(const struct hrtimer *timer, ktime_t now)\0A{\0A\09ktime_t rem = ktime_sub(timer->node.expires, now);\0A\0A\09/*\0A\09 * Adjust relative timers for the extra we added in\0A\09 * hrtimer_start_range_ns() to prevent short timeouts.\0A\09 */\0A\09if (IS_ENABLED(CONFIG_TIME_LOW_RES) && timer->is_rel)\0A\09\09rem -= hrtimer_resolution;\0A\09return rem;\0A}\0A\0Astatic inline ktime_t\0Ahrtimer_expires_remaining_adjusted(const struct hrtimer *timer)\0A{\0A\09return __hrtimer_expires_remaining_adjusted(timer,\0A\09\09\09\09\09\09    timer->base->get_time());\0A}\0A\0Aextern void clock_was_set(void);\0A#ifdef CONFIG_TIMERFD\0Aextern void timerfd_clock_was_set(void);\0A#else\0Astatic inline void timerfd_clock_was_set(void) { }\0A#endif\0Aextern void hrtimers_resume(void);\0A\0ADECLARE_PER_CPU(struct tick_device, tick_cpu_device);\0A\0A\0A/* Exported timer functions: */\0A\0A/* Initialize timers: */\0Aextern void hrtimer_init(struct hrtimer *timer, clockid_t which_clock,\0A\09\09\09 enum hrtimer_mode mode);\0A\0A#ifdef CONFIG_DEBUG_OBJECTS_TIMERS\0Aextern void hrtimer_init_on_stack(struct hrtimer *timer, clockid_t which_clock,\0A\09\09\09\09  enum hrtimer_mode mode);\0A\0Aextern void destroy_hrtimer_on_stack(struct hrtimer *timer);\0A#else\0Astatic inline void hrtimer_init_on_stack(struct hrtimer *timer,\0A\09\09\09\09\09 clockid_t which_clock,\0A\09\09\09\09\09 enum hrtimer_mode mode)\0A{\0A\09hrtimer_init(timer, which_clock, mode);\0A}\0Astatic inline void destroy_hrtimer_on_stack(struct hrtimer *timer) { }\0A#endif\0A\0A/* Basic timer operations: */\0Aextern void hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,\0A\09\09\09\09   u64 range_ns, const enum hrtimer_mode mode);\0A\0A/**\0A * hrtimer_start - (re)start an hrtimer on the current CPU\0A * @timer:\09the timer to be added\0A * @tim:\09expiry time\0A * @mode:\09expiry mode: absolute (HRTIMER_MODE_ABS) or\0A *\09\09relative (HRTIMER_MODE_REL)\0A */\0Astatic inline void hrtimer_start(struct hrtimer *timer, ktime_t tim,\0A\09\09\09\09 const enum hrtimer_mode mode)\0A{\0A\09hrtimer_start_range_ns(timer, tim, 0, mode);\0A}\0A\0Aextern int hrtimer_cancel(struct hrtimer *timer);\0Aextern int hrtimer_try_to_cancel(struct hrtimer *timer);\0A\0Astatic inline void hrtimer_start_expires(struct hrtimer *timer,\0A\09\09\09\09\09 enum hrtimer_mode mode)\0A{\0A\09u64 delta;\0A\09ktime_t soft, hard;\0A\09soft = hrtimer_get_softexpires(timer);\0A\09hard = hrtimer_get_expires(timer);\0A\09delta = ktime_to_ns(ktime_sub(hard, soft));\0A\09hrtimer_start_range_ns(timer, soft, delta, mode);\0A}\0A\0Astatic inline void hrtimer_restart(struct hrtimer *timer)\0A{\0A\09hrtimer_start_expires(timer, HRTIMER_MODE_ABS);\0A}\0A\0A/* Query timers: */\0Aextern ktime_t __hrtimer_get_remaining(const struct hrtimer *timer, bool adjust);\0A\0Astatic inline ktime_t hrtimer_get_remaining(const struct hrtimer *timer)\0A{\0A\09return __hrtimer_get_remaining(timer, false);\0A}\0A\0Aextern u64 hrtimer_get_next_event(void);\0A\0Aextern bool hrtimer_active(const struct hrtimer *timer);\0A\0A/*\0A * Helper function to check, whether the timer is on one of the queues\0A */\0Astatic inline int hrtimer_is_queued(struct hrtimer *timer)\0A{\0A\09return timer->state & HRTIMER_STATE_ENQUEUED;\0A}\0A\0A/*\0A * Helper function to check, whether the timer is running the callback\0A * function\0A */\0Astatic inline int hrtimer_callback_running(struct hrtimer *timer)\0A{\0A\09return timer->base->cpu_base->running == timer;\0A}\0A\0A/* Forward a hrtimer so it expires after now: */\0Aextern u64\0Ahrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval);\0A\0A/**\0A * hrtimer_forward_now - forward the timer expiry so it expires after now\0A * @timer:\09hrtimer to forward\0A * @interval:\09the interval to forward\0A *\0A * Forward the timer expiry so it will expire after the current time\0A * of the hrtimer clock base. Returns the number of overruns.\0A *\0A * Can be safely called from the callback function of @timer. If\0A * called from other contexts @timer must neither be enqueued nor\0A * running the callback and the caller needs to take care of\0A * serialization.\0A *\0A * Note: This only updates the timer expiry value and does not requeue\0A * the timer.\0A */\0Astatic inline u64 hrtimer_forward_now(struct hrtimer *timer,\0A\09\09\09\09      ktime_t interval)\0A{\0A\09return hrtimer_forward(timer, timer->base->get_time(), interval);\0A}\0A\0A/* Precise sleep: */\0Aextern long hrtimer_nanosleep(struct timespec *rqtp,\0A\09\09\09      struct timespec __user *rmtp,\0A\09\09\09      const enum hrtimer_mode mode,\0A\09\09\09      const clockid_t clockid);\0Aextern long hrtimer_nanosleep_restart(struct restart_block *restart_block);\0A\0Aextern void hrtimer_init_sleeper(struct hrtimer_sleeper *sl,\0A\09\09\09\09 struct task_struct *tsk);\0A\0Aextern int schedule_hrtimeout_range(ktime_t *expires, u64 delta,\0A\09\09\09\09\09\09const enum hrtimer_mode mode);\0Aextern int schedule_hrtimeout_range_clock(ktime_t *expires,\0A\09\09\09\09\09  u64 delta,\0A\09\09\09\09\09  const enum hrtimer_mode mode,\0A\09\09\09\09\09  int clock);\0Aextern int schedule_hrtimeout(ktime_t *expires, const enum hrtimer_mode mode);\0A\0A/* Soft interrupt function to run the hrtimer queues: */\0Aextern void hrtimer_run_queues(void);\0A\0A/* Bootup initialization: */\0Aextern void __init hrtimers_init(void);\0A\0A/* Show pending timers: */\0Aextern void sysrq_timer_list_show(void);\0A\0Aint hrtimers_prepare_cpu(unsigned int cpu);\0A#ifdef CONFIG_HOTPLUG_CPU\0Aint hrtimers_dead_cpu(unsigned int cpu);\0A#else\0A#define hrtimers_dead_cpu\09NULL\0A#endif\0A\0A#endif\0A")
!7 = !DIBasicType(name: "unsigned int", size: 32, encoding: DW_ATE_unsigned)
!8 = !{!9, !10}
!9 = !DIEnumerator(name: "HRTIMER_NORESTART", value: 0, isUnsigned: true)
!10 = !DIEnumerator(name: "HRTIMER_RESTART", value: 1, isUnsigned: true)
!11 = !DICompositeType(tag: DW_TAG_enumeration_type, name: "page_entry_size", file: !12, line: 350, baseType: !7, size: 32, elements: !13)
!12 = !DIFile(filename: "/lib/modules/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f/build/include/linux/mm.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "172260053be71720d7fe0de25f7db0fa", source: "#ifndef _LINUX_MM_H\0A#define _LINUX_MM_H\0A\0A#include <linux/errno.h>\0A\0A#ifdef __KERNEL__\0A\0A#include <linux/mmdebug.h>\0A#include <linux/gfp.h>\0A#include <linux/bug.h>\0A#include <linux/list.h>\0A#include <linux/mmzone.h>\0A#include <linux/rbtree.h>\0A#include <linux/atomic.h>\0A#include <linux/debug_locks.h>\0A#include <linux/mm_types.h>\0A#include <linux/range.h>\0A#include <linux/pfn.h>\0A#include <linux/percpu-refcount.h>\0A#include <linux/bit_spinlock.h>\0A#include <linux/shrinker.h>\0A#include <linux/resource.h>\0A#include <linux/page_ext.h>\0A#include <linux/err.h>\0A#include <linux/page_ref.h>\0A\0Astruct mempolicy;\0Astruct anon_vma;\0Astruct anon_vma_chain;\0Astruct file_ra_state;\0Astruct user_struct;\0Astruct writeback_control;\0Astruct bdi_writeback;\0A\0Avoid init_mm_internals(void);\0A\0A#ifndef CONFIG_NEED_MULTIPLE_NODES\09/* Don't use mapnrs, do it properly */\0Aextern unsigned long max_mapnr;\0A\0Astatic inline void set_max_mapnr(unsigned long limit)\0A{\0A\09max_mapnr = limit;\0A}\0A#else\0Astatic inline void set_max_mapnr(unsigned long limit) { }\0A#endif\0A\0Aextern unsigned long totalram_pages;\0Aextern void * high_memory;\0Aextern int page_cluster;\0A\0A#ifdef CONFIG_SYSCTL\0Aextern int sysctl_legacy_va_layout;\0A#else\0A#define sysctl_legacy_va_layout 0\0A#endif\0A\0A#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\0Aextern const int mmap_rnd_bits_min;\0Aextern const int mmap_rnd_bits_max;\0Aextern int mmap_rnd_bits __read_mostly;\0A#endif\0A#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\0Aextern const int mmap_rnd_compat_bits_min;\0Aextern const int mmap_rnd_compat_bits_max;\0Aextern int mmap_rnd_compat_bits __read_mostly;\0A#endif\0A\0A#include <asm/page.h>\0A#include <asm/pgtable.h>\0A#include <asm/processor.h>\0A\0A#ifndef __pa_symbol\0A#define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))\0A#endif\0A\0A#ifndef page_to_virt\0A#define page_to_virt(x)\09__va(PFN_PHYS(page_to_pfn(x)))\0A#endif\0A\0A#ifndef lm_alias\0A#define lm_alias(x)\09__va(__pa_symbol(x))\0A#endif\0A\0A/*\0A * To prevent common memory management code establishing\0A * a zero page mapping on a read fault.\0A * This macro should be defined within <asm/pgtable.h>.\0A * s390 does this to prevent multiplexing of hardware bits\0A * related to the physical page in case of virtualization.\0A */\0A#ifndef mm_forbids_zeropage\0A#define mm_forbids_zeropage(X)\09(0)\0A#endif\0A\0A/*\0A * Default maximum number of active map areas, this limits the number of vmas\0A * per mm struct. Users can overwrite this number by sysctl but there is a\0A * problem.\0A *\0A * When a program's coredump is generated as ELF format, a section is created\0A * per a vma. In ELF, the number of sections is represented in unsigned short.\0A * This means the number of sections should be smaller than 65535 at coredump.\0A * Because the kernel adds some informative sections to a image of program at\0A * generating coredump, we need some margin. The number of extra sections is\0A * 1-3 now and depends on arch. We use \225\22 as safe margin, here.\0A *\0A * ELF extended numbering allows more than 65535 sections, so 16-bit bound is\0A * not a hard limit any more. Although some userspace tools can be surprised by\0A * that.\0A */\0A#define MAPCOUNT_ELF_CORE_MARGIN\09(5)\0A#define DEFAULT_MAX_MAP_COUNT\09(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\0A\0Aextern int sysctl_max_map_count;\0A\0Aextern unsigned long sysctl_user_reserve_kbytes;\0Aextern unsigned long sysctl_admin_reserve_kbytes;\0A\0Aextern int sysctl_overcommit_memory;\0Aextern int sysctl_overcommit_ratio;\0Aextern unsigned long sysctl_overcommit_kbytes;\0A\0Aextern int overcommit_ratio_handler(struct ctl_table *, int, void __user *,\0A\09\09\09\09    size_t *, loff_t *);\0Aextern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,\0A\09\09\09\09    size_t *, loff_t *);\0A\0A#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\0A\0A/* to align the pointer to the (next) page boundary */\0A#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\0A\0A/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */\0A#define PAGE_ALIGNED(addr)\09IS_ALIGNED((unsigned long)(addr), PAGE_SIZE)\0A\0A/*\0A * Linux kernel virtual memory manager primitives.\0A * The idea being to have a \22virtual\22 mm in the same way\0A * we have a virtual fs - giving a cleaner interface to the\0A * mm details, and allowing different kinds of memory mappings\0A * (from shared memory to executable loading to arbitrary\0A * mmap() functions).\0A */\0A\0Aextern struct kmem_cache *vm_area_cachep;\0A\0A#ifndef CONFIG_MMU\0Aextern struct rb_root nommu_region_tree;\0Aextern struct rw_semaphore nommu_region_sem;\0A\0Aextern unsigned int kobjsize(const void *objp);\0A#endif\0A\0A/*\0A * vm_flags in vm_area_struct, see mm_types.h.\0A * When changing, update also include/trace/events/mmflags.h\0A */\0A#define VM_NONE\09\090x00000000\0A\0A#define VM_READ\09\090x00000001\09/* currently active flags */\0A#define VM_WRITE\090x00000002\0A#define VM_EXEC\09\090x00000004\0A#define VM_SHARED\090x00000008\0A\0A/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */\0A#define VM_MAYREAD\090x00000010\09/* limits for mprotect() etc */\0A#define VM_MAYWRITE\090x00000020\0A#define VM_MAYEXEC\090x00000040\0A#define VM_MAYSHARE\090x00000080\0A\0A#define VM_GROWSDOWN\090x00000100\09/* general info on the segment */\0A#define VM_UFFD_MISSING\090x00000200\09/* missing pages tracking */\0A#define VM_PFNMAP\090x00000400\09/* Page-ranges managed without \22struct page\22, just pure PFN */\0A#define VM_DENYWRITE\090x00000800\09/* ETXTBSY on write attempts.. */\0A#define VM_UFFD_WP\090x00001000\09/* wrprotect pages tracking */\0A\0A#define VM_LOCKED\090x00002000\0A#define VM_IO           0x00004000\09/* Memory mapped I/O or similar */\0A\0A\09\09\09\09\09/* Used by sys_madvise() */\0A#define VM_SEQ_READ\090x00008000\09/* App will access data sequentially */\0A#define VM_RAND_READ\090x00010000\09/* App will not benefit from clustered reads */\0A\0A#define VM_DONTCOPY\090x00020000      /* Do not copy this vma on fork */\0A#define VM_DONTEXPAND\090x00040000\09/* Cannot expand with mremap() */\0A#define VM_LOCKONFAULT\090x00080000\09/* Lock the pages covered when they are faulted in */\0A#define VM_ACCOUNT\090x00100000\09/* Is a VM accounted object */\0A#define VM_NORESERVE\090x00200000\09/* should the VM suppress accounting */\0A#define VM_HUGETLB\090x00400000\09/* Huge TLB Page VM */\0A#define VM_ARCH_1\090x01000000\09/* Architecture-specific flag */\0A#define VM_ARCH_2\090x02000000\0A#define VM_DONTDUMP\090x04000000\09/* Do not include in the core dump */\0A\0A#ifdef CONFIG_MEM_SOFT_DIRTY\0A# define VM_SOFTDIRTY\090x08000000\09/* Not soft dirty clean area */\0A#else\0A# define VM_SOFTDIRTY\090\0A#endif\0A\0A#define VM_MIXEDMAP\090x10000000\09/* Can contain \22struct page\22 and pure PFN pages */\0A#define VM_HUGEPAGE\090x20000000\09/* MADV_HUGEPAGE marked this vma */\0A#define VM_NOHUGEPAGE\090x40000000\09/* MADV_NOHUGEPAGE marked this vma */\0A#define VM_MERGEABLE\090x80000000\09/* KSM may merge identical pages */\0A\0A#ifdef CONFIG_ARCH_USES_HIGH_VMA_FLAGS\0A#define VM_HIGH_ARCH_BIT_0\0932\09/* bit only usable on 64-bit architectures */\0A#define VM_HIGH_ARCH_BIT_1\0933\09/* bit only usable on 64-bit architectures */\0A#define VM_HIGH_ARCH_BIT_2\0934\09/* bit only usable on 64-bit architectures */\0A#define VM_HIGH_ARCH_BIT_3\0935\09/* bit only usable on 64-bit architectures */\0A#define VM_HIGH_ARCH_0\09BIT(VM_HIGH_ARCH_BIT_0)\0A#define VM_HIGH_ARCH_1\09BIT(VM_HIGH_ARCH_BIT_1)\0A#define VM_HIGH_ARCH_2\09BIT(VM_HIGH_ARCH_BIT_2)\0A#define VM_HIGH_ARCH_3\09BIT(VM_HIGH_ARCH_BIT_3)\0A#endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */\0A\0A#if defined(CONFIG_X86)\0A# define VM_PAT\09\09VM_ARCH_1\09/* PAT reserves whole VMA at once (x86) */\0A#if defined (CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS)\0A# define VM_PKEY_SHIFT\09VM_HIGH_ARCH_BIT_0\0A# define VM_PKEY_BIT0\09VM_HIGH_ARCH_0\09/* A protection key is a 4-bit value */\0A# define VM_PKEY_BIT1\09VM_HIGH_ARCH_1\0A# define VM_PKEY_BIT2\09VM_HIGH_ARCH_2\0A# define VM_PKEY_BIT3\09VM_HIGH_ARCH_3\0A#endif\0A#elif defined(CONFIG_PPC)\0A# define VM_SAO\09\09VM_ARCH_1\09/* Strong Access Ordering (powerpc) */\0A#elif defined(CONFIG_PARISC)\0A# define VM_GROWSUP\09VM_ARCH_1\0A#elif defined(CONFIG_METAG)\0A# define VM_GROWSUP\09VM_ARCH_1\0A#elif defined(CONFIG_IA64)\0A# define VM_GROWSUP\09VM_ARCH_1\0A#elif !defined(CONFIG_MMU)\0A# define VM_MAPPED_COPY\09VM_ARCH_1\09/* T if mapped copy of data (nommu mmap) */\0A#endif\0A\0A#if defined(CONFIG_X86)\0A/* MPX specific bounds table or bounds directory */\0A# define VM_MPX\09\09VM_ARCH_2\0A#endif\0A\0A#ifndef VM_GROWSUP\0A# define VM_GROWSUP\09VM_NONE\0A#endif\0A\0A/* Bits set in the VMA until the stack is in its final location */\0A#define VM_STACK_INCOMPLETE_SETUP\09(VM_RAND_READ | VM_SEQ_READ)\0A\0A#ifndef VM_STACK_DEFAULT_FLAGS\09\09/* arch can override this */\0A#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\0A#endif\0A\0A#ifdef CONFIG_STACK_GROWSUP\0A#define VM_STACK\09VM_GROWSUP\0A#else\0A#define VM_STACK\09VM_GROWSDOWN\0A#endif\0A\0A#define VM_STACK_FLAGS\09(VM_STACK | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\0A\0A/*\0A * Special vmas that are non-mergable, non-mlock()able.\0A * Note: mm/huge_memory.c VM_NO_THP depends on this definition.\0A */\0A#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)\0A\0A/* This mask defines which mm->def_flags a process can inherit its parent */\0A#define VM_INIT_DEF_MASK\09VM_NOHUGEPAGE\0A\0A/* This mask is used to clear all the VMA flags used by mlock */\0A#define VM_LOCKED_CLEAR_MASK\09(~(VM_LOCKED | VM_LOCKONFAULT))\0A\0A/*\0A * mapping from the currently active vm_flags protection bits (the\0A * low four bits) to a page protection mask..\0A */\0Aextern pgprot_t protection_map[16];\0A\0A#define FAULT_FLAG_WRITE\090x01\09/* Fault was a write access */\0A#define FAULT_FLAG_MKWRITE\090x02\09/* Fault was mkwrite of existing pte */\0A#define FAULT_FLAG_ALLOW_RETRY\090x04\09/* Retry fault if blocking */\0A#define FAULT_FLAG_RETRY_NOWAIT\090x08\09/* Don't drop mmap_sem and wait when retrying */\0A#define FAULT_FLAG_KILLABLE\090x10\09/* The fault task is in SIGKILL killable region */\0A#define FAULT_FLAG_TRIED\090x20\09/* Second try */\0A#define FAULT_FLAG_USER\09\090x40\09/* The fault originated in userspace */\0A#define FAULT_FLAG_REMOTE\090x80\09/* faulting for non current tsk/mm */\0A#define FAULT_FLAG_INSTRUCTION  0x100\09/* The fault was during an instruction fetch */\0A\0A#define FAULT_FLAG_TRACE \5C\0A\09{ FAULT_FLAG_WRITE,\09\09\22WRITE\22 }, \5C\0A\09{ FAULT_FLAG_MKWRITE,\09\09\22MKWRITE\22 }, \5C\0A\09{ FAULT_FLAG_ALLOW_RETRY,\09\22ALLOW_RETRY\22 }, \5C\0A\09{ FAULT_FLAG_RETRY_NOWAIT,\09\22RETRY_NOWAIT\22 }, \5C\0A\09{ FAULT_FLAG_KILLABLE,\09\09\22KILLABLE\22 }, \5C\0A\09{ FAULT_FLAG_TRIED,\09\09\22TRIED\22 }, \5C\0A\09{ FAULT_FLAG_USER,\09\09\22USER\22 }, \5C\0A\09{ FAULT_FLAG_REMOTE,\09\09\22REMOTE\22 }, \5C\0A\09{ FAULT_FLAG_INSTRUCTION,\09\22INSTRUCTION\22 }\0A\0A/*\0A * vm_fault is filled by the the pagefault handler and passed to the vma's\0A * ->fault function. The vma's ->fault is responsible for returning a bitmask\0A * of VM_FAULT_xxx flags that give details about how the fault was handled.\0A *\0A * MM layer fills up gfp_mask for page allocations but fault handler might\0A * alter it if its implementation requires a different allocation context.\0A *\0A * pgoff should be used in favour of virtual_address, if possible.\0A */\0Astruct vm_fault {\0A\09struct vm_area_struct *vma;\09/* Target VMA */\0A\09unsigned int flags;\09\09/* FAULT_FLAG_xxx flags */\0A\09gfp_t gfp_mask;\09\09\09/* gfp mask to be used for allocations */\0A\09pgoff_t pgoff;\09\09\09/* Logical page offset based on vma */\0A\09unsigned long address;\09\09/* Faulting virtual address */\0A\09pmd_t *pmd;\09\09\09/* Pointer to pmd entry matching\0A\09\09\09\09\09 * the 'address' */\0A\09pud_t *pud;\09\09\09/* Pointer to pud entry matching\0A\09\09\09\09\09 * the 'address'\0A\09\09\09\09\09 */\0A\09pte_t orig_pte;\09\09\09/* Value of PTE at the time of fault */\0A\0A\09struct page *cow_page;\09\09/* Page handler may use for COW fault */\0A\09struct mem_cgroup *memcg;\09/* Cgroup cow_page belongs to */\0A\09struct page *page;\09\09/* ->fault handlers should return a\0A\09\09\09\09\09 * page here, unless VM_FAULT_NOPAGE\0A\09\09\09\09\09 * is set (which is also implied by\0A\09\09\09\09\09 * VM_FAULT_ERROR).\0A\09\09\09\09\09 */\0A\09/* These three entries are valid only while holding ptl lock */\0A\09pte_t *pte;\09\09\09/* Pointer to pte entry matching\0A\09\09\09\09\09 * the 'address'. NULL if the page\0A\09\09\09\09\09 * table hasn't been allocated.\0A\09\09\09\09\09 */\0A\09spinlock_t *ptl;\09\09/* Page table lock.\0A\09\09\09\09\09 * Protects pte page table if 'pte'\0A\09\09\09\09\09 * is not NULL, otherwise pmd.\0A\09\09\09\09\09 */\0A\09pgtable_t prealloc_pte;\09\09/* Pre-allocated pte page table.\0A\09\09\09\09\09 * vm_ops->map_pages() calls\0A\09\09\09\09\09 * alloc_set_pte() from atomic context.\0A\09\09\09\09\09 * do_fault_around() pre-allocates\0A\09\09\09\09\09 * page table to avoid allocation from\0A\09\09\09\09\09 * atomic context.\0A\09\09\09\09\09 */\0A};\0A\0A/* page entry size for vm->huge_fault() */\0Aenum page_entry_size {\0A\09PE_SIZE_PTE = 0,\0A\09PE_SIZE_PMD,\0A\09PE_SIZE_PUD,\0A};\0A\0A/*\0A * These are the virtual MM functions - opening of an area, closing and\0A * unmapping it (needed to keep files on disk up-to-date etc), pointer\0A * to the functions called when a no-page or a wp-page exception occurs. \0A */\0Astruct vm_operations_struct {\0A\09void (*open)(struct vm_area_struct * area);\0A\09void (*close)(struct vm_area_struct * area);\0A\09int (*mremap)(struct vm_area_struct * area);\0A\09int (*fault)(struct vm_fault *vmf);\0A\09int (*huge_fault)(struct vm_fault *vmf, enum page_entry_size pe_size);\0A\09void (*map_pages)(struct vm_fault *vmf,\0A\09\09\09pgoff_t start_pgoff, pgoff_t end_pgoff);\0A\0A\09/* notification that a previously read-only page is about to become\0A\09 * writable, if an error is returned it will cause a SIGBUS */\0A\09int (*page_mkwrite)(struct vm_fault *vmf);\0A\0A\09/* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */\0A\09int (*pfn_mkwrite)(struct vm_fault *vmf);\0A\0A\09/* called by access_process_vm when get_user_pages() fails, typically\0A\09 * for use by special VMAs that can switch between memory and hardware\0A\09 */\0A\09int (*access)(struct vm_area_struct *vma, unsigned long addr,\0A\09\09      void *buf, int len, int write);\0A\0A\09/* Called by the /proc/PID/maps code to ask the vma whether it\0A\09 * has a special name.  Returning non-NULL will also cause this\0A\09 * vma to be dumped unconditionally. */\0A\09const char *(*name)(struct vm_area_struct *vma);\0A\0A#ifdef CONFIG_NUMA\0A\09/*\0A\09 * set_policy() op must add a reference to any non-NULL @new mempolicy\0A\09 * to hold the policy upon return.  Caller should pass NULL @new to\0A\09 * remove a policy and fall back to surrounding context--i.e. do not\0A\09 * install a MPOL_DEFAULT policy, nor the task or system default\0A\09 * mempolicy.\0A\09 */\0A\09int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\0A\0A\09/*\0A\09 * get_policy() op must add reference [mpol_get()] to any policy at\0A\09 * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure\0A\09 * in mm/mempolicy.c will do this automatically.\0A\09 * get_policy() must NOT add a ref if the policy at (vma,addr) is not\0A\09 * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.\0A\09 * If no [shared/vma] mempolicy exists at the addr, get_policy() op\0A\09 * must return NULL--i.e., do not \22fallback\22 to task or system default\0A\09 * policy.\0A\09 */\0A\09struct mempolicy *(*get_policy)(struct vm_area_struct *vma,\0A\09\09\09\09\09unsigned long addr);\0A#endif\0A\09/*\0A\09 * Called by vm_normal_page() for special PTEs to find the\0A\09 * page for @addr.  This is useful if the default behavior\0A\09 * (using pte_page()) would not find the correct page.\0A\09 */\0A\09struct page *(*find_special_page)(struct vm_area_struct *vma,\0A\09\09\09\09\09  unsigned long addr);\0A};\0A\0Astruct mmu_gather;\0Astruct inode;\0A\0A#define page_private(page)\09\09((page)->private)\0A#define set_page_private(page, v)\09((page)->private = (v))\0A\0A#if !defined(__HAVE_ARCH_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)\0Astatic inline int pmd_devmap(pmd_t pmd)\0A{\0A\09return 0;\0A}\0Astatic inline int pud_devmap(pud_t pud)\0A{\0A\09return 0;\0A}\0A#endif\0A\0A/*\0A * FIXME: take this include out, include page-flags.h in\0A * files which need it (119 of them)\0A */\0A#include <linux/page-flags.h>\0A#include <linux/huge_mm.h>\0A\0A/*\0A * Methods to modify the page usage count.\0A *\0A * What counts for a page usage:\0A * - cache mapping   (page->mapping)\0A * - private data    (page->private)\0A * - page mapped in a task's page tables, each mapping\0A *   is counted separately\0A *\0A * Also, many kernel routines increase the page count before a critical\0A * routine so they can be sure the page doesn't go away from under them.\0A */\0A\0A/*\0A * Drop a ref, return true if the refcount fell to zero (the page has no users)\0A */\0Astatic inline int put_page_testzero(struct page *page)\0A{\0A\09VM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\0A\09return page_ref_dec_and_test(page);\0A}\0A\0A/*\0A * Try to grab a ref unless the page has a refcount of zero, return false if\0A * that is the case.\0A * This can be called when MMU is off so it must not access\0A * any of the virtual mappings.\0A */\0Astatic inline int get_page_unless_zero(struct page *page)\0A{\0A\09return page_ref_add_unless(page, 1, 0);\0A}\0A\0Aextern int page_is_ram(unsigned long pfn);\0A\0Aenum {\0A\09REGION_INTERSECTS,\0A\09REGION_DISJOINT,\0A\09REGION_MIXED,\0A};\0A\0Aint region_intersects(resource_size_t offset, size_t size, unsigned long flags,\0A\09\09      unsigned long desc);\0A\0A/* Support for virtually mapped pages */\0Astruct page *vmalloc_to_page(const void *addr);\0Aunsigned long vmalloc_to_pfn(const void *addr);\0A\0A/*\0A * Determine if an address is within the vmalloc range\0A *\0A * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there\0A * is no special casing required.\0A */\0Astatic inline bool is_vmalloc_addr(const void *x)\0A{\0A#ifdef CONFIG_MMU\0A\09unsigned long addr = (unsigned long)x;\0A\0A\09return addr >= VMALLOC_START && addr < VMALLOC_END;\0A#else\0A\09return false;\0A#endif\0A}\0A#ifdef CONFIG_MMU\0Aextern int is_vmalloc_or_module_addr(const void *x);\0A#else\0Astatic inline int is_vmalloc_or_module_addr(const void *x)\0A{\0A\09return 0;\0A}\0A#endif\0A\0Aextern void *kvmalloc_node(size_t size, gfp_t flags, int node);\0Astatic inline void *kvmalloc(size_t size, gfp_t flags)\0A{\0A\09return kvmalloc_node(size, flags, NUMA_NO_NODE);\0A}\0Astatic inline void *kvzalloc_node(size_t size, gfp_t flags, int node)\0A{\0A\09return kvmalloc_node(size, flags | __GFP_ZERO, node);\0A}\0Astatic inline void *kvzalloc(size_t size, gfp_t flags)\0A{\0A\09return kvmalloc(size, flags | __GFP_ZERO);\0A}\0A\0Aextern void kvfree(const void *addr);\0A\0Astatic inline atomic_t *compound_mapcount_ptr(struct page *page)\0A{\0A\09return &page[1].compound_mapcount;\0A}\0A\0Astatic inline int compound_mapcount(struct page *page)\0A{\0A\09VM_BUG_ON_PAGE(!PageCompound(page), page);\0A\09page = compound_head(page);\0A\09return atomic_read(compound_mapcount_ptr(page)) + 1;\0A}\0A\0A/*\0A * The atomic page->_mapcount, starts from -1: so that transitions\0A * both from it and to it can be tracked, using atomic_inc_and_test\0A * and atomic_add_negative(-1).\0A */\0Astatic inline void page_mapcount_reset(struct page *page)\0A{\0A\09atomic_set(&(page)->_mapcount, -1);\0A}\0A\0Aint __page_mapcount(struct page *page);\0A\0Astatic inline int page_mapcount(struct page *page)\0A{\0A\09VM_BUG_ON_PAGE(PageSlab(page), page);\0A\0A\09if (unlikely(PageCompound(page)))\0A\09\09return __page_mapcount(page);\0A\09return atomic_read(&page->_mapcount) + 1;\0A}\0A\0A#ifdef CONFIG_TRANSPARENT_HUGEPAGE\0Aint total_mapcount(struct page *page);\0Aint page_trans_huge_mapcount(struct page *page, int *total_mapcount);\0A#else\0Astatic inline int total_mapcount(struct page *page)\0A{\0A\09return page_mapcount(page);\0A}\0Astatic inline int page_trans_huge_mapcount(struct page *page,\0A\09\09\09\09\09   int *total_mapcount)\0A{\0A\09int mapcount = page_mapcount(page);\0A\09if (total_mapcount)\0A\09\09*total_mapcount = mapcount;\0A\09return mapcount;\0A}\0A#endif\0A\0Astatic inline struct page *virt_to_head_page(const void *x)\0A{\0A\09struct page *page = virt_to_page(x);\0A\0A\09return compound_head(page);\0A}\0A\0Avoid __put_page(struct page *page);\0A\0Avoid put_pages_list(struct list_head *pages);\0A\0Avoid split_page(struct page *page, unsigned int order);\0A\0A/*\0A * Compound pages have a destructor function.  Provide a\0A * prototype for that function and accessor functions.\0A * These are _only_ valid on the head of a compound page.\0A */\0Atypedef void compound_page_dtor(struct page *);\0A\0A/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */\0Aenum compound_dtor_id {\0A\09NULL_COMPOUND_DTOR,\0A\09COMPOUND_PAGE_DTOR,\0A#ifdef CONFIG_HUGETLB_PAGE\0A\09HUGETLB_PAGE_DTOR,\0A#endif\0A#ifdef CONFIG_TRANSPARENT_HUGEPAGE\0A\09TRANSHUGE_PAGE_DTOR,\0A#endif\0A\09NR_COMPOUND_DTORS,\0A};\0Aextern compound_page_dtor * const compound_page_dtors[];\0A\0Astatic inline void set_compound_page_dtor(struct page *page,\0A\09\09enum compound_dtor_id compound_dtor)\0A{\0A\09VM_BUG_ON_PAGE(compound_dtor >= NR_COMPOUND_DTORS, page);\0A\09page[1].compound_dtor = compound_dtor;\0A}\0A\0Astatic inline compound_page_dtor *get_compound_page_dtor(struct page *page)\0A{\0A\09VM_BUG_ON_PAGE(page[1].compound_dtor >= NR_COMPOUND_DTORS, page);\0A\09return compound_page_dtors[page[1].compound_dtor];\0A}\0A\0Astatic inline unsigned int compound_order(struct page *page)\0A{\0A\09if (!PageHead(page))\0A\09\09return 0;\0A\09return page[1].compound_order;\0A}\0A\0Astatic inline void set_compound_order(struct page *page, unsigned int order)\0A{\0A\09page[1].compound_order = order;\0A}\0A\0Avoid free_compound_page(struct page *page);\0A\0A#ifdef CONFIG_MMU\0A/*\0A * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when\0A * servicing faults for write access.  In the normal case, do always want\0A * pte_mkwrite.  But get_user_pages can cause write faults for mappings\0A * that do not have writing enabled, when used by access_process_vm.\0A */\0Astatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\0A{\0A\09if (likely(vma->vm_flags & VM_WRITE))\0A\09\09pte = pte_mkwrite(pte);\0A\09return pte;\0A}\0A\0Aint alloc_set_pte(struct vm_fault *vmf, struct mem_cgroup *memcg,\0A\09\09struct page *page);\0Aint finish_fault(struct vm_fault *vmf);\0Aint finish_mkwrite_fault(struct vm_fault *vmf);\0A#endif\0A\0A/*\0A * Multiple processes may \22see\22 the same page. E.g. for untouched\0A * mappings of /dev/null, all processes see the same page full of\0A * zeroes, and text pages of executables and shared libraries have\0A * only one copy in memory, at most, normally.\0A *\0A * For the non-reserved pages, page_count(page) denotes a reference count.\0A *   page_count() == 0 means the page is free. page->lru is then used for\0A *   freelist management in the buddy allocator.\0A *   page_count() > 0  means the page has been allocated.\0A *\0A * Pages are allocated by the slab allocator in order to provide memory\0A * to kmalloc and kmem_cache_alloc. In this case, the management of the\0A * page, and the fields in 'struct page' are the responsibility of mm/slab.c\0A * unless a particular usage is carefully commented. (the responsibility of\0A * freeing the kmalloc memory is the caller's, of course).\0A *\0A * A page may be used by anyone else who does a __get_free_page().\0A * In this case, page_count still tracks the references, and should only\0A * be used through the normal accessor functions. The top bits of page->flags\0A * and page->virtual store page management information, but all other fields\0A * are unused and could be used privately, carefully. The management of this\0A * page is the responsibility of the one who allocated it, and those who have\0A * subsequently been given references to it.\0A *\0A * The other pages (we may call them \22pagecache pages\22) are completely\0A * managed by the Linux memory manager: I/O, buffers, swapping etc.\0A * The following discussion applies only to them.\0A *\0A * A pagecache page contains an opaque `private' member, which belongs to the\0A * page's address_space. Usually, this is the address of a circular list of\0A * the page's disk buffers. PG_private must be set to tell the VM to call\0A * into the filesystem to release these pages.\0A *\0A * A page may belong to an inode's memory mapping. In this case, page->mapping\0A * is the pointer to the inode, and page->index is the file offset of the page,\0A * in units of PAGE_SIZE.\0A *\0A * If pagecache pages are not associated with an inode, they are said to be\0A * anonymous pages. These may become associated with the swapcache, and in that\0A * case PG_swapcache is set, and page->private is an offset into the swapcache.\0A *\0A * In either case (swapcache or inode backed), the pagecache itself holds one\0A * reference to the page. Setting PG_private should also increment the\0A * refcount. The each user mapping also has a reference to the page.\0A *\0A * The pagecache pages are stored in a per-mapping radix tree, which is\0A * rooted at mapping->page_tree, and indexed by offset.\0A * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space\0A * lists, we instead now tag pages as dirty/writeback in the radix tree.\0A *\0A * All pagecache pages may be subject to I/O:\0A * - inode pages may need to be read from disk,\0A * - inode pages which have been modified and are MAP_SHARED may need\0A *   to be written back to the inode on disk,\0A * - anonymous pages (including MAP_PRIVATE file mappings) which have been\0A *   modified may need to be swapped out to swap space and (later) to be read\0A *   back into memory.\0A */\0A\0A/*\0A * The zone field is never updated after free_area_init_core()\0A * sets it, so none of the operations on it need to be atomic.\0A */\0A\0A/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */\0A#define SECTIONS_PGOFF\09\09((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\0A#define NODES_PGOFF\09\09(SECTIONS_PGOFF - NODES_WIDTH)\0A#define ZONES_PGOFF\09\09(NODES_PGOFF - ZONES_WIDTH)\0A#define LAST_CPUPID_PGOFF\09(ZONES_PGOFF - LAST_CPUPID_WIDTH)\0A\0A/*\0A * Define the bit shifts to access each section.  For non-existent\0A * sections we define the shift as 0; that plus a 0 mask ensures\0A * the compiler will optimise away reference to them.\0A */\0A#define SECTIONS_PGSHIFT\09(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\0A#define NODES_PGSHIFT\09\09(NODES_PGOFF * (NODES_WIDTH != 0))\0A#define ZONES_PGSHIFT\09\09(ZONES_PGOFF * (ZONES_WIDTH != 0))\0A#define LAST_CPUPID_PGSHIFT\09(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))\0A\0A/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */\0A#ifdef NODE_NOT_IN_PAGE_FLAGS\0A#define ZONEID_SHIFT\09\09(SECTIONS_SHIFT + ZONES_SHIFT)\0A#define ZONEID_PGOFF\09\09((SECTIONS_PGOFF < ZONES_PGOFF)? \5C\0A\09\09\09\09\09\09SECTIONS_PGOFF : ZONES_PGOFF)\0A#else\0A#define ZONEID_SHIFT\09\09(NODES_SHIFT + ZONES_SHIFT)\0A#define ZONEID_PGOFF\09\09((NODES_PGOFF < ZONES_PGOFF)? \5C\0A\09\09\09\09\09\09NODES_PGOFF : ZONES_PGOFF)\0A#endif\0A\0A#define ZONEID_PGSHIFT\09\09(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\0A\0A#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\0A#error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\0A#endif\0A\0A#define ZONES_MASK\09\09((1UL << ZONES_WIDTH) - 1)\0A#define NODES_MASK\09\09((1UL << NODES_WIDTH) - 1)\0A#define SECTIONS_MASK\09\09((1UL << SECTIONS_WIDTH) - 1)\0A#define LAST_CPUPID_MASK\09((1UL << LAST_CPUPID_SHIFT) - 1)\0A#define ZONEID_MASK\09\09((1UL << ZONEID_SHIFT) - 1)\0A\0Astatic inline enum zone_type page_zonenum(const struct page *page)\0A{\0A\09return (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\0A}\0A\0A#ifdef CONFIG_ZONE_DEVICE\0Avoid get_zone_device_page(struct page *page);\0Avoid put_zone_device_page(struct page *page);\0Astatic inline bool is_zone_device_page(const struct page *page)\0A{\0A\09return page_zonenum(page) == ZONE_DEVICE;\0A}\0A#else\0Astatic inline void get_zone_device_page(struct page *page)\0A{\0A}\0Astatic inline void put_zone_device_page(struct page *page)\0A{\0A}\0Astatic inline bool is_zone_device_page(const struct page *page)\0A{\0A\09return false;\0A}\0A#endif\0A\0Astatic inline void get_page(struct page *page)\0A{\0A\09page = compound_head(page);\0A\09/*\0A\09 * Getting a normal page or the head of a compound page\0A\09 * requires to already have an elevated page->_refcount.\0A\09 */\0A\09VM_BUG_ON_PAGE(page_ref_count(page) <= 0, page);\0A\09page_ref_inc(page);\0A\0A\09if (unlikely(is_zone_device_page(page)))\0A\09\09get_zone_device_page(page);\0A}\0A\0Astatic inline void put_page(struct page *page)\0A{\0A\09page = compound_head(page);\0A\0A\09if (put_page_testzero(page))\0A\09\09__put_page(page);\0A\0A\09if (unlikely(is_zone_device_page(page)))\0A\09\09put_zone_device_page(page);\0A}\0A\0A#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\0A#define SECTION_IN_PAGE_FLAGS\0A#endif\0A\0A/*\0A * The identification function is mainly used by the buddy allocator for\0A * determining if two pages could be buddies. We are not really identifying\0A * the zone since we could be using the section number id if we do not have\0A * node id available in page flags.\0A * We only guarantee that it will return the same value for two combinable\0A * pages in a zone.\0A */\0Astatic inline int page_zone_id(struct page *page)\0A{\0A\09return (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\0A}\0A\0Astatic inline int zone_to_nid(struct zone *zone)\0A{\0A#ifdef CONFIG_NUMA\0A\09return zone->node;\0A#else\0A\09return 0;\0A#endif\0A}\0A\0A#ifdef NODE_NOT_IN_PAGE_FLAGS\0Aextern int page_to_nid(const struct page *page);\0A#else\0Astatic inline int page_to_nid(const struct page *page)\0A{\0A\09return (page->flags >> NODES_PGSHIFT) & NODES_MASK;\0A}\0A#endif\0A\0A#ifdef CONFIG_NUMA_BALANCING\0Astatic inline int cpu_pid_to_cpupid(int cpu, int pid)\0A{\0A\09return ((cpu & LAST__CPU_MASK) << LAST__PID_SHIFT) | (pid & LAST__PID_MASK);\0A}\0A\0Astatic inline int cpupid_to_pid(int cpupid)\0A{\0A\09return cpupid & LAST__PID_MASK;\0A}\0A\0Astatic inline int cpupid_to_cpu(int cpupid)\0A{\0A\09return (cpupid >> LAST__PID_SHIFT) & LAST__CPU_MASK;\0A}\0A\0Astatic inline int cpupid_to_nid(int cpupid)\0A{\0A\09return cpu_to_node(cpupid_to_cpu(cpupid));\0A}\0A\0Astatic inline bool cpupid_pid_unset(int cpupid)\0A{\0A\09return cpupid_to_pid(cpupid) == (-1 & LAST__PID_MASK);\0A}\0A\0Astatic inline bool cpupid_cpu_unset(int cpupid)\0A{\0A\09return cpupid_to_cpu(cpupid) == (-1 & LAST__CPU_MASK);\0A}\0A\0Astatic inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)\0A{\0A\09return (task_pid & LAST__PID_MASK) == cpupid_to_pid(cpupid);\0A}\0A\0A#define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task->pid, cpupid)\0A#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\0Astatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\0A{\0A\09return xchg(&page->_last_cpupid, cpupid & LAST_CPUPID_MASK);\0A}\0A\0Astatic inline int page_cpupid_last(struct page *page)\0A{\0A\09return page->_last_cpupid;\0A}\0Astatic inline void page_cpupid_reset_last(struct page *page)\0A{\0A\09page->_last_cpupid = -1 & LAST_CPUPID_MASK;\0A}\0A#else\0Astatic inline int page_cpupid_last(struct page *page)\0A{\0A\09return (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;\0A}\0A\0Aextern int page_cpupid_xchg_last(struct page *page, int cpupid);\0A\0Astatic inline void page_cpupid_reset_last(struct page *page)\0A{\0A\09page->flags |= LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT;\0A}\0A#endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */\0A#else /* !CONFIG_NUMA_BALANCING */\0Astatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\0A{\0A\09return page_to_nid(page); /* XXX */\0A}\0A\0Astatic inline int page_cpupid_last(struct page *page)\0A{\0A\09return page_to_nid(page); /* XXX */\0A}\0A\0Astatic inline int cpupid_to_nid(int cpupid)\0A{\0A\09return -1;\0A}\0A\0Astatic inline int cpupid_to_pid(int cpupid)\0A{\0A\09return -1;\0A}\0A\0Astatic inline int cpupid_to_cpu(int cpupid)\0A{\0A\09return -1;\0A}\0A\0Astatic inline int cpu_pid_to_cpupid(int nid, int pid)\0A{\0A\09return -1;\0A}\0A\0Astatic inline bool cpupid_pid_unset(int cpupid)\0A{\0A\09return 1;\0A}\0A\0Astatic inline void page_cpupid_reset_last(struct page *page)\0A{\0A}\0A\0Astatic inline bool cpupid_match_pid(struct task_struct *task, int cpupid)\0A{\0A\09return false;\0A}\0A#endif /* CONFIG_NUMA_BALANCING */\0A\0Astatic inline struct zone *page_zone(const struct page *page)\0A{\0A\09return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\0A}\0A\0Astatic inline pg_data_t *page_pgdat(const struct page *page)\0A{\0A\09return NODE_DATA(page_to_nid(page));\0A}\0A\0A#ifdef SECTION_IN_PAGE_FLAGS\0Astatic inline void set_page_section(struct page *page, unsigned long section)\0A{\0A\09page->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\0A\09page->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\0A}\0A\0Astatic inline unsigned long page_to_section(const struct page *page)\0A{\0A\09return (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\0A}\0A#endif\0A\0Astatic inline void set_page_zone(struct page *page, enum zone_type zone)\0A{\0A\09page->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\0A\09page->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\0A}\0A\0Astatic inline void set_page_node(struct page *page, unsigned long node)\0A{\0A\09page->flags &= ~(NODES_MASK << NODES_PGSHIFT);\0A\09page->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\0A}\0A\0Astatic inline void set_page_links(struct page *page, enum zone_type zone,\0A\09unsigned long node, unsigned long pfn)\0A{\0A\09set_page_zone(page, zone);\0A\09set_page_node(page, node);\0A#ifdef SECTION_IN_PAGE_FLAGS\0A\09set_page_section(page, pfn_to_section_nr(pfn));\0A#endif\0A}\0A\0A#ifdef CONFIG_MEMCG\0Astatic inline struct mem_cgroup *page_memcg(struct page *page)\0A{\0A\09return page->mem_cgroup;\0A}\0Astatic inline struct mem_cgroup *page_memcg_rcu(struct page *page)\0A{\0A\09WARN_ON_ONCE(!rcu_read_lock_held());\0A\09return READ_ONCE(page->mem_cgroup);\0A}\0A#else\0Astatic inline struct mem_cgroup *page_memcg(struct page *page)\0A{\0A\09return NULL;\0A}\0Astatic inline struct mem_cgroup *page_memcg_rcu(struct page *page)\0A{\0A\09WARN_ON_ONCE(!rcu_read_lock_held());\0A\09return NULL;\0A}\0A#endif\0A\0A/*\0A * Some inline functions in vmstat.h depend on page_zone()\0A */\0A#include <linux/vmstat.h>\0A\0Astatic __always_inline void *lowmem_page_address(const struct page *page)\0A{\0A\09return page_to_virt(page);\0A}\0A\0A#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\0A#define HASHED_PAGE_VIRTUAL\0A#endif\0A\0A#if defined(WANT_PAGE_VIRTUAL)\0Astatic inline void *page_address(const struct page *page)\0A{\0A\09return page->virtual;\0A}\0Astatic inline void set_page_address(struct page *page, void *address)\0A{\0A\09page->virtual = address;\0A}\0A#define page_address_init()  do { } while(0)\0A#endif\0A\0A#if defined(HASHED_PAGE_VIRTUAL)\0Avoid *page_address(const struct page *page);\0Avoid set_page_address(struct page *page, void *virtual);\0Avoid page_address_init(void);\0A#endif\0A\0A#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\0A#define page_address(page) lowmem_page_address(page)\0A#define set_page_address(page, address)  do { } while(0)\0A#define page_address_init()  do { } while(0)\0A#endif\0A\0Aextern void *page_rmapping(struct page *page);\0Aextern struct anon_vma *page_anon_vma(struct page *page);\0Aextern struct address_space *page_mapping(struct page *page);\0A\0Aextern struct address_space *__page_file_mapping(struct page *);\0A\0Astatic inline\0Astruct address_space *page_file_mapping(struct page *page)\0A{\0A\09if (unlikely(PageSwapCache(page)))\0A\09\09return __page_file_mapping(page);\0A\0A\09return page->mapping;\0A}\0A\0Aextern pgoff_t __page_file_index(struct page *page);\0A\0A/*\0A * Return the pagecache index of the passed page.  Regular pagecache pages\0A * use ->index whereas swapcache pages use swp_offset(->private)\0A */\0Astatic inline pgoff_t page_index(struct page *page)\0A{\0A\09if (unlikely(PageSwapCache(page)))\0A\09\09return __page_file_index(page);\0A\09return page->index;\0A}\0A\0Abool page_mapped(struct page *page);\0Astruct address_space *page_mapping(struct page *page);\0A\0A/*\0A * Return true only if the page has been allocated with\0A * ALLOC_NO_WATERMARKS and the low watermark was not\0A * met implying that the system is under some pressure.\0A */\0Astatic inline bool page_is_pfmemalloc(struct page *page)\0A{\0A\09/*\0A\09 * Page index cannot be this large so this must be\0A\09 * a pfmemalloc page.\0A\09 */\0A\09return page->index == -1UL;\0A}\0A\0A/*\0A * Only to be called by the page allocator on a freshly allocated\0A * page.\0A */\0Astatic inline void set_page_pfmemalloc(struct page *page)\0A{\0A\09page->index = -1UL;\0A}\0A\0Astatic inline void clear_page_pfmemalloc(struct page *page)\0A{\0A\09page->index = 0;\0A}\0A\0A/*\0A * Different kinds of faults, as returned by handle_mm_fault().\0A * Used to decide whether a process gets delivered SIGBUS or\0A * just gets major/minor fault counters bumped up.\0A */\0A\0A#define VM_FAULT_OOM\090x0001\0A#define VM_FAULT_SIGBUS\090x0002\0A#define VM_FAULT_MAJOR\090x0004\0A#define VM_FAULT_WRITE\090x0008\09/* Special case for get_user_pages */\0A#define VM_FAULT_HWPOISON 0x0010\09/* Hit poisoned small page */\0A#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */\0A#define VM_FAULT_SIGSEGV 0x0040\0A\0A#define VM_FAULT_NOPAGE\090x0100\09/* ->fault installed the pte, not return page */\0A#define VM_FAULT_LOCKED\090x0200\09/* ->fault locked the returned page */\0A#define VM_FAULT_RETRY\090x0400\09/* ->fault blocked, must retry */\0A#define VM_FAULT_FALLBACK 0x0800\09/* huge page fault failed, fall back to small */\0A#define VM_FAULT_DONE_COW   0x1000\09/* ->fault has fully handled COW */\0A\0A#define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */\0A\0A#define VM_FAULT_ERROR\09(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | \5C\0A\09\09\09 VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE | \5C\0A\09\09\09 VM_FAULT_FALLBACK)\0A\0A#define VM_FAULT_RESULT_TRACE \5C\0A\09{ VM_FAULT_OOM,\09\09\09\22OOM\22 }, \5C\0A\09{ VM_FAULT_SIGBUS,\09\09\22SIGBUS\22 }, \5C\0A\09{ VM_FAULT_MAJOR,\09\09\22MAJOR\22 }, \5C\0A\09{ VM_FAULT_WRITE,\09\09\22WRITE\22 }, \5C\0A\09{ VM_FAULT_HWPOISON,\09\09\22HWPOISON\22 }, \5C\0A\09{ VM_FAULT_HWPOISON_LARGE,\09\22HWPOISON_LARGE\22 }, \5C\0A\09{ VM_FAULT_SIGSEGV,\09\09\22SIGSEGV\22 }, \5C\0A\09{ VM_FAULT_NOPAGE,\09\09\22NOPAGE\22 }, \5C\0A\09{ VM_FAULT_LOCKED,\09\09\22LOCKED\22 }, \5C\0A\09{ VM_FAULT_RETRY,\09\09\22RETRY\22 }, \5C\0A\09{ VM_FAULT_FALLBACK,\09\09\22FALLBACK\22 }, \5C\0A\09{ VM_FAULT_DONE_COW,\09\09\22DONE_COW\22 }\0A\0A/* Encode hstate index for a hwpoisoned large page */\0A#define VM_FAULT_SET_HINDEX(x) ((x) << 12)\0A#define VM_FAULT_GET_HINDEX(x) (((x) >> 12) & 0xf)\0A\0A/*\0A * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.\0A */\0Aextern void pagefault_out_of_memory(void);\0A\0A#define offset_in_page(p)\09((unsigned long)(p) & ~PAGE_MASK)\0A\0A/*\0A * Flags passed to show_mem() and show_free_areas() to suppress output in\0A * various contexts.\0A */\0A#define SHOW_MEM_FILTER_NODES\09\09(0x0001u)\09/* disallowed nodes */\0A\0Aextern void show_free_areas(unsigned int flags, nodemask_t *nodemask);\0A\0Aextern bool can_do_mlock(void);\0Aextern int user_shm_lock(size_t, struct user_struct *);\0Aextern void user_shm_unlock(size_t, struct user_struct *);\0A\0A/*\0A * Parameter block passed down to zap_pte_range in exceptional cases.\0A */\0Astruct zap_details {\0A\09struct address_space *check_mapping;\09/* Check page->mapping if set */\0A\09pgoff_t\09first_index;\09\09\09/* Lowest page->index to unmap */\0A\09pgoff_t last_index;\09\09\09/* Highest page->index to unmap */\0A};\0A\0Astruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\0A\09\09pte_t pte);\0Astruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\0A\09\09\09\09pmd_t pmd);\0A\0Aint zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\0A\09\09unsigned long size);\0Avoid zap_page_range(struct vm_area_struct *vma, unsigned long address,\0A\09\09unsigned long size);\0Avoid unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,\0A\09\09unsigned long start, unsigned long end);\0A\0A/**\0A * mm_walk - callbacks for walk_page_range\0A * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry\0A *\09       this handler should only handle pud_trans_huge() puds.\0A *\09       the pmd_entry or pte_entry callbacks will be used for\0A *\09       regular PUDs.\0A * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry\0A *\09       this handler is required to be able to handle\0A *\09       pmd_trans_huge() pmds.  They may simply choose to\0A *\09       split_huge_page() instead of handling it explicitly.\0A * @pte_entry: if set, called for each non-empty PTE (4th-level) entry\0A * @pte_hole: if set, called for each hole at all levels\0A * @hugetlb_entry: if set, called for each hugetlb entry\0A * @test_walk: caller specific callback function to determine whether\0A *             we walk over the current vma or not. Returning 0\0A *             value means \22do page table walk over the current vma,\22\0A *             and a negative one means \22abort current page table walk\0A *             right now.\22 1 means \22skip the current vma.\22\0A * @mm:        mm_struct representing the target process of page table walk\0A * @vma:       vma currently walked (NULL if walking outside vmas)\0A * @private:   private data for callbacks' usage\0A *\0A * (see the comment on walk_page_range() for more details)\0A */\0Astruct mm_walk {\0A\09int (*pud_entry)(pud_t *pud, unsigned long addr,\0A\09\09\09 unsigned long next, struct mm_walk *walk);\0A\09int (*pmd_entry)(pmd_t *pmd, unsigned long addr,\0A\09\09\09 unsigned long next, struct mm_walk *walk);\0A\09int (*pte_entry)(pte_t *pte, unsigned long addr,\0A\09\09\09 unsigned long next, struct mm_walk *walk);\0A\09int (*pte_hole)(unsigned long addr, unsigned long next,\0A\09\09\09struct mm_walk *walk);\0A\09int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,\0A\09\09\09     unsigned long addr, unsigned long next,\0A\09\09\09     struct mm_walk *walk);\0A\09int (*test_walk)(unsigned long addr, unsigned long next,\0A\09\09\09struct mm_walk *walk);\0A\09struct mm_struct *mm;\0A\09struct vm_area_struct *vma;\0A\09void *private;\0A};\0A\0Aint walk_page_range(unsigned long addr, unsigned long end,\0A\09\09struct mm_walk *walk);\0Aint walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);\0Avoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\0A\09\09unsigned long end, unsigned long floor, unsigned long ceiling);\0Aint copy_page_range(struct mm_struct *dst, struct mm_struct *src,\0A\09\09\09struct vm_area_struct *vma);\0Avoid unmap_mapping_range(struct address_space *mapping,\0A\09\09loff_t const holebegin, loff_t const holelen, int even_cows);\0Aint follow_pte_pmd(struct mm_struct *mm, unsigned long address,\0A\09\09\09     pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp);\0Aint follow_pfn(struct vm_area_struct *vma, unsigned long address,\0A\09unsigned long *pfn);\0Aint follow_phys(struct vm_area_struct *vma, unsigned long address,\0A\09\09unsigned int flags, unsigned long *prot, resource_size_t *phys);\0Aint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\0A\09\09\09void *buf, int len, int write);\0A\0Astatic inline void unmap_shared_mapping_range(struct address_space *mapping,\0A\09\09loff_t const holebegin, loff_t const holelen)\0A{\0A\09unmap_mapping_range(mapping, holebegin, holelen, 0);\0A}\0A\0Aextern void truncate_pagecache(struct inode *inode, loff_t new);\0Aextern void truncate_setsize(struct inode *inode, loff_t newsize);\0Avoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);\0Avoid truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);\0Aint truncate_inode_page(struct address_space *mapping, struct page *page);\0Aint generic_error_remove_page(struct address_space *mapping, struct page *page);\0Aint invalidate_inode_page(struct page *page);\0A\0A#ifdef CONFIG_MMU\0Aextern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,\0A\09\09unsigned int flags);\0Aextern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\0A\09\09\09    unsigned long address, unsigned int fault_flags,\0A\09\09\09    bool *unlocked);\0A#else\0Astatic inline int handle_mm_fault(struct vm_area_struct *vma,\0A\09\09unsigned long address, unsigned int flags)\0A{\0A\09/* should never happen if there's no MMU */\0A\09BUG();\0A\09return VM_FAULT_SIGBUS;\0A}\0Astatic inline int fixup_user_fault(struct task_struct *tsk,\0A\09\09struct mm_struct *mm, unsigned long address,\0A\09\09unsigned int fault_flags, bool *unlocked)\0A{\0A\09/* should never happen if there's no MMU */\0A\09BUG();\0A\09return -EFAULT;\0A}\0A#endif\0A\0Aextern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,\0A\09\09unsigned int gup_flags);\0Aextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\0A\09\09void *buf, int len, unsigned int gup_flags);\0Aextern int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\0A\09\09unsigned long addr, void *buf, int len, unsigned int gup_flags);\0A\0Along get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\0A\09\09\09    unsigned long start, unsigned long nr_pages,\0A\09\09\09    unsigned int gup_flags, struct page **pages,\0A\09\09\09    struct vm_area_struct **vmas, int *locked);\0Along get_user_pages(unsigned long start, unsigned long nr_pages,\0A\09\09\09    unsigned int gup_flags, struct page **pages,\0A\09\09\09    struct vm_area_struct **vmas);\0Along get_user_pages_locked(unsigned long start, unsigned long nr_pages,\0A\09\09    unsigned int gup_flags, struct page **pages, int *locked);\0Along get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\0A\09\09    struct page **pages, unsigned int gup_flags);\0Aint get_user_pages_fast(unsigned long start, int nr_pages, int write,\0A\09\09\09struct page **pages);\0A\0A/* Container for pinned pfns / pages */\0Astruct frame_vector {\0A\09unsigned int nr_allocated;\09/* Number of frames we have space for */\0A\09unsigned int nr_frames;\09/* Number of frames stored in ptrs array */\0A\09bool got_ref;\09\09/* Did we pin pages by getting page ref? */\0A\09bool is_pfns;\09\09/* Does array contain pages or pfns? */\0A\09void *ptrs[0];\09\09/* Array of pinned pfns / pages. Use\0A\09\09\09\09 * pfns_vector_pages() or pfns_vector_pfns()\0A\09\09\09\09 * for access */\0A};\0A\0Astruct frame_vector *frame_vector_create(unsigned int nr_frames);\0Avoid frame_vector_destroy(struct frame_vector *vec);\0Aint get_vaddr_frames(unsigned long start, unsigned int nr_pfns,\0A\09\09     unsigned int gup_flags, struct frame_vector *vec);\0Avoid put_vaddr_frames(struct frame_vector *vec);\0Aint frame_vector_to_pages(struct frame_vector *vec);\0Avoid frame_vector_to_pfns(struct frame_vector *vec);\0A\0Astatic inline unsigned int frame_vector_count(struct frame_vector *vec)\0A{\0A\09return vec->nr_frames;\0A}\0A\0Astatic inline struct page **frame_vector_pages(struct frame_vector *vec)\0A{\0A\09if (vec->is_pfns) {\0A\09\09int err = frame_vector_to_pages(vec);\0A\0A\09\09if (err)\0A\09\09\09return ERR_PTR(err);\0A\09}\0A\09return (struct page **)(vec->ptrs);\0A}\0A\0Astatic inline unsigned long *frame_vector_pfns(struct frame_vector *vec)\0A{\0A\09if (!vec->is_pfns)\0A\09\09frame_vector_to_pfns(vec);\0A\09return (unsigned long *)(vec->ptrs);\0A}\0A\0Astruct kvec;\0Aint get_kernel_pages(const struct kvec *iov, int nr_pages, int write,\0A\09\09\09struct page **pages);\0Aint get_kernel_page(unsigned long start, int write, struct page **pages);\0Astruct page *get_dump_page(unsigned long addr);\0A\0Aextern int try_to_release_page(struct page * page, gfp_t gfp_mask);\0Aextern void do_invalidatepage(struct page *page, unsigned int offset,\0A\09\09\09      unsigned int length);\0A\0Aint __set_page_dirty_nobuffers(struct page *page);\0Aint __set_page_dirty_no_writeback(struct page *page);\0Aint redirty_page_for_writepage(struct writeback_control *wbc,\0A\09\09\09\09struct page *page);\0Avoid account_page_dirtied(struct page *page, struct address_space *mapping);\0Avoid account_page_cleaned(struct page *page, struct address_space *mapping,\0A\09\09\09  struct bdi_writeback *wb);\0Aint set_page_dirty(struct page *page);\0Aint set_page_dirty_lock(struct page *page);\0Avoid cancel_dirty_page(struct page *page);\0Aint clear_page_dirty_for_io(struct page *page);\0A\0Aint get_cmdline(struct task_struct *task, char *buffer, int buflen);\0A\0Astatic inline bool vma_is_anonymous(struct vm_area_struct *vma)\0A{\0A\09return !vma->vm_ops;\0A}\0A\0A#ifdef CONFIG_SHMEM\0A/*\0A * The vma_is_shmem is not inline because it is used only by slow\0A * paths in userfault.\0A */\0Abool vma_is_shmem(struct vm_area_struct *vma);\0A#else\0Astatic inline bool vma_is_shmem(struct vm_area_struct *vma) { return false; }\0A#endif\0A\0Aint vma_is_stack_for_current(struct vm_area_struct *vma);\0A\0Aextern unsigned long move_page_tables(struct vm_area_struct *vma,\0A\09\09unsigned long old_addr, struct vm_area_struct *new_vma,\0A\09\09unsigned long new_addr, unsigned long len,\0A\09\09bool need_rmap_locks);\0Aextern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,\0A\09\09\09      unsigned long end, pgprot_t newprot,\0A\09\09\09      int dirty_accountable, int prot_numa);\0Aextern int mprotect_fixup(struct vm_area_struct *vma,\0A\09\09\09  struct vm_area_struct **pprev, unsigned long start,\0A\09\09\09  unsigned long end, unsigned long newflags);\0A\0A/*\0A * doesn't attempt to fault and will return short.\0A */\0Aint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\0A\09\09\09  struct page **pages);\0A/*\0A * per-process(per-mm_struct) statistics.\0A */\0Astatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\0A{\0A\09long val = atomic_long_read(&mm->rss_stat.count[member]);\0A\0A#ifdef SPLIT_RSS_COUNTING\0A\09/*\0A\09 * counter is updated in asynchronous manner and may go to minus.\0A\09 * But it's never be expected number for users.\0A\09 */\0A\09if (val < 0)\0A\09\09val = 0;\0A#endif\0A\09return (unsigned long)val;\0A}\0A\0Astatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\0A{\0A\09atomic_long_add(value, &mm->rss_stat.count[member]);\0A}\0A\0Astatic inline void inc_mm_counter(struct mm_struct *mm, int member)\0A{\0A\09atomic_long_inc(&mm->rss_stat.count[member]);\0A}\0A\0Astatic inline void dec_mm_counter(struct mm_struct *mm, int member)\0A{\0A\09atomic_long_dec(&mm->rss_stat.count[member]);\0A}\0A\0A/* Optimized variant when page is already known not to be PageAnon */\0Astatic inline int mm_counter_file(struct page *page)\0A{\0A\09if (PageSwapBacked(page))\0A\09\09return MM_SHMEMPAGES;\0A\09return MM_FILEPAGES;\0A}\0A\0Astatic inline int mm_counter(struct page *page)\0A{\0A\09if (PageAnon(page))\0A\09\09return MM_ANONPAGES;\0A\09return mm_counter_file(page);\0A}\0A\0Astatic inline unsigned long get_mm_rss(struct mm_struct *mm)\0A{\0A\09return get_mm_counter(mm, MM_FILEPAGES) +\0A\09\09get_mm_counter(mm, MM_ANONPAGES) +\0A\09\09get_mm_counter(mm, MM_SHMEMPAGES);\0A}\0A\0Astatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\0A{\0A\09return max(mm->hiwater_rss, get_mm_rss(mm));\0A}\0A\0Astatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\0A{\0A\09return max(mm->hiwater_vm, mm->total_vm);\0A}\0A\0Astatic inline void update_hiwater_rss(struct mm_struct *mm)\0A{\0A\09unsigned long _rss = get_mm_rss(mm);\0A\0A\09if ((mm)->hiwater_rss < _rss)\0A\09\09(mm)->hiwater_rss = _rss;\0A}\0A\0Astatic inline void update_hiwater_vm(struct mm_struct *mm)\0A{\0A\09if (mm->hiwater_vm < mm->total_vm)\0A\09\09mm->hiwater_vm = mm->total_vm;\0A}\0A\0Astatic inline void reset_mm_hiwater_rss(struct mm_struct *mm)\0A{\0A\09mm->hiwater_rss = get_mm_rss(mm);\0A}\0A\0Astatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\0A\09\09\09\09\09 struct mm_struct *mm)\0A{\0A\09unsigned long hiwater_rss = get_mm_hiwater_rss(mm);\0A\0A\09if (*maxrss < hiwater_rss)\0A\09\09*maxrss = hiwater_rss;\0A}\0A\0A#if defined(SPLIT_RSS_COUNTING)\0Avoid sync_mm_rss(struct mm_struct *mm);\0A#else\0Astatic inline void sync_mm_rss(struct mm_struct *mm)\0A{\0A}\0A#endif\0A\0A#ifndef __HAVE_ARCH_PTE_DEVMAP\0Astatic inline int pte_devmap(pte_t pte)\0A{\0A\09return 0;\0A}\0A#endif\0A\0Aint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);\0A\0Aextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\0A\09\09\09       spinlock_t **ptl);\0Astatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\0A\09\09\09\09    spinlock_t **ptl)\0A{\0A\09pte_t *ptep;\0A\09__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\0A\09return ptep;\0A}\0A\0A#ifdef __PAGETABLE_P4D_FOLDED\0Astatic inline int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd,\0A\09\09\09\09\09\09unsigned long address)\0A{\0A\09return 0;\0A}\0A#else\0Aint __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\0A#endif\0A\0A#ifdef __PAGETABLE_PUD_FOLDED\0Astatic inline int __pud_alloc(struct mm_struct *mm, p4d_t *p4d,\0A\09\09\09\09\09\09unsigned long address)\0A{\0A\09return 0;\0A}\0A#else\0Aint __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address);\0A#endif\0A\0A#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)\0Astatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\0A\09\09\09\09\09\09unsigned long address)\0A{\0A\09return 0;\0A}\0A\0Astatic inline void mm_nr_pmds_init(struct mm_struct *mm) {}\0A\0Astatic inline unsigned long mm_nr_pmds(struct mm_struct *mm)\0A{\0A\09return 0;\0A}\0A\0Astatic inline void mm_inc_nr_pmds(struct mm_struct *mm) {}\0Astatic inline void mm_dec_nr_pmds(struct mm_struct *mm) {}\0A\0A#else\0Aint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\0A\0Astatic inline void mm_nr_pmds_init(struct mm_struct *mm)\0A{\0A\09atomic_long_set(&mm->nr_pmds, 0);\0A}\0A\0Astatic inline unsigned long mm_nr_pmds(struct mm_struct *mm)\0A{\0A\09return atomic_long_read(&mm->nr_pmds);\0A}\0A\0Astatic inline void mm_inc_nr_pmds(struct mm_struct *mm)\0A{\0A\09atomic_long_inc(&mm->nr_pmds);\0A}\0A\0Astatic inline void mm_dec_nr_pmds(struct mm_struct *mm)\0A{\0A\09atomic_long_dec(&mm->nr_pmds);\0A}\0A#endif\0A\0Aint __pte_alloc(struct mm_struct *mm, pmd_t *pmd, unsigned long address);\0Aint __pte_alloc_kernel(pmd_t *pmd, unsigned long address);\0A\0A/*\0A * The following ifdef needed to get the 4level-fixup.h header to work.\0A * Remove it when 4level-fixup.h has been removed.\0A */\0A#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)\0A\0A#ifndef __ARCH_HAS_5LEVEL_HACK\0Astatic inline p4d_t *p4d_alloc(struct mm_struct *mm, pgd_t *pgd,\0A\09\09unsigned long address)\0A{\0A\09return (unlikely(pgd_none(*pgd)) && __p4d_alloc(mm, pgd, address)) ?\0A\09\09NULL : p4d_offset(pgd, address);\0A}\0A\0Astatic inline pud_t *pud_alloc(struct mm_struct *mm, p4d_t *p4d,\0A\09\09unsigned long address)\0A{\0A\09return (unlikely(p4d_none(*p4d)) && __pud_alloc(mm, p4d, address)) ?\0A\09\09NULL : pud_offset(p4d, address);\0A}\0A#endif /* !__ARCH_HAS_5LEVEL_HACK */\0A\0Astatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\0A{\0A\09return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\0A\09\09NULL: pmd_offset(pud, address);\0A}\0A#endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */\0A\0A#if USE_SPLIT_PTE_PTLOCKS\0A#if ALLOC_SPLIT_PTLOCKS\0Avoid __init ptlock_cache_init(void);\0Aextern bool ptlock_alloc(struct page *page);\0Aextern void ptlock_free(struct page *page);\0A\0Astatic inline spinlock_t *ptlock_ptr(struct page *page)\0A{\0A\09return page->ptl;\0A}\0A#else /* ALLOC_SPLIT_PTLOCKS */\0Astatic inline void ptlock_cache_init(void)\0A{\0A}\0A\0Astatic inline bool ptlock_alloc(struct page *page)\0A{\0A\09return true;\0A}\0A\0Astatic inline void ptlock_free(struct page *page)\0A{\0A}\0A\0Astatic inline spinlock_t *ptlock_ptr(struct page *page)\0A{\0A\09return &page->ptl;\0A}\0A#endif /* ALLOC_SPLIT_PTLOCKS */\0A\0Astatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\0A{\0A\09return ptlock_ptr(pmd_page(*pmd));\0A}\0A\0Astatic inline bool ptlock_init(struct page *page)\0A{\0A\09/*\0A\09 * prep_new_page() initialize page->private (and therefore page->ptl)\0A\09 * with 0. Make sure nobody took it in use in between.\0A\09 *\0A\09 * It can happen if arch try to use slab for page table allocation:\0A\09 * slab code uses page->slab_cache, which share storage with page->ptl.\0A\09 */\0A\09VM_BUG_ON_PAGE(*(unsigned long *)&page->ptl, page);\0A\09if (!ptlock_alloc(page))\0A\09\09return false;\0A\09spin_lock_init(ptlock_ptr(page));\0A\09return true;\0A}\0A\0A/* Reset page->mapping so free_pages_check won't complain. */\0Astatic inline void pte_lock_deinit(struct page *page)\0A{\0A\09page->mapping = NULL;\0A\09ptlock_free(page);\0A}\0A\0A#else\09/* !USE_SPLIT_PTE_PTLOCKS */\0A/*\0A * We use mm->page_table_lock to guard all pagetable pages of the mm.\0A */\0Astatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\0A{\0A\09return &mm->page_table_lock;\0A}\0Astatic inline void ptlock_cache_init(void) {}\0Astatic inline bool ptlock_init(struct page *page) { return true; }\0Astatic inline void pte_lock_deinit(struct page *page) {}\0A#endif /* USE_SPLIT_PTE_PTLOCKS */\0A\0Astatic inline void pgtable_init(void)\0A{\0A\09ptlock_cache_init();\0A\09pgtable_cache_init();\0A}\0A\0Astatic inline bool pgtable_page_ctor(struct page *page)\0A{\0A\09if (!ptlock_init(page))\0A\09\09return false;\0A\09inc_zone_page_state(page, NR_PAGETABLE);\0A\09return true;\0A}\0A\0Astatic inline void pgtable_page_dtor(struct page *page)\0A{\0A\09pte_lock_deinit(page);\0A\09dec_zone_page_state(page, NR_PAGETABLE);\0A}\0A\0A#define pte_offset_map_lock(mm, pmd, address, ptlp)\09\5C\0A({\09\09\09\09\09\09\09\5C\0A\09spinlock_t *__ptl = pte_lockptr(mm, pmd);\09\5C\0A\09pte_t *__pte = pte_offset_map(pmd, address);\09\5C\0A\09*(ptlp) = __ptl;\09\09\09\09\5C\0A\09spin_lock(__ptl);\09\09\09\09\5C\0A\09__pte;\09\09\09\09\09\09\5C\0A})\0A\0A#define pte_unmap_unlock(pte, ptl)\09do {\09\09\5C\0A\09spin_unlock(ptl);\09\09\09\09\5C\0A\09pte_unmap(pte);\09\09\09\09\09\5C\0A} while (0)\0A\0A#define pte_alloc(mm, pmd, address)\09\09\09\5C\0A\09(unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, pmd, address))\0A\0A#define pte_alloc_map(mm, pmd, address)\09\09\09\5C\0A\09(pte_alloc(mm, pmd, address) ? NULL : pte_offset_map(pmd, address))\0A\0A#define pte_alloc_map_lock(mm, pmd, address, ptlp)\09\5C\0A\09(pte_alloc(mm, pmd, address) ?\09\09\09\5C\0A\09\09 NULL : pte_offset_map_lock(mm, pmd, address, ptlp))\0A\0A#define pte_alloc_kernel(pmd, address)\09\09\09\5C\0A\09((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd, address))? \5C\0A\09\09NULL: pte_offset_kernel(pmd, address))\0A\0A#if USE_SPLIT_PMD_PTLOCKS\0A\0Astatic struct page *pmd_to_page(pmd_t *pmd)\0A{\0A\09unsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);\0A\09return virt_to_page((void *)((unsigned long) pmd & mask));\0A}\0A\0Astatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\0A{\0A\09return ptlock_ptr(pmd_to_page(pmd));\0A}\0A\0Astatic inline bool pgtable_pmd_page_ctor(struct page *page)\0A{\0A#ifdef CONFIG_TRANSPARENT_HUGEPAGE\0A\09page->pmd_huge_pte = NULL;\0A#endif\0A\09return ptlock_init(page);\0A}\0A\0Astatic inline void pgtable_pmd_page_dtor(struct page *page)\0A{\0A#ifdef CONFIG_TRANSPARENT_HUGEPAGE\0A\09VM_BUG_ON_PAGE(page->pmd_huge_pte, page);\0A#endif\0A\09ptlock_free(page);\0A}\0A\0A#define pmd_huge_pte(mm, pmd) (pmd_to_page(pmd)->pmd_huge_pte)\0A\0A#else\0A\0Astatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\0A{\0A\09return &mm->page_table_lock;\0A}\0A\0Astatic inline bool pgtable_pmd_page_ctor(struct page *page) { return true; }\0Astatic inline void pgtable_pmd_page_dtor(struct page *page) {}\0A\0A#define pmd_huge_pte(mm, pmd) ((mm)->pmd_huge_pte)\0A\0A#endif\0A\0Astatic inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)\0A{\0A\09spinlock_t *ptl = pmd_lockptr(mm, pmd);\0A\09spin_lock(ptl);\0A\09return ptl;\0A}\0A\0A/*\0A * No scalability reason to split PUD locks yet, but follow the same pattern\0A * as the PMD locks to make it easier if we decide to.  The VM should not be\0A * considered ready to switch to split PUD locks yet; there may be places\0A * which need to be converted from page_table_lock.\0A */\0Astatic inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)\0A{\0A\09return &mm->page_table_lock;\0A}\0A\0Astatic inline spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)\0A{\0A\09spinlock_t *ptl = pud_lockptr(mm, pud);\0A\0A\09spin_lock(ptl);\0A\09return ptl;\0A}\0A\0Aextern void __init pagecache_init(void);\0Aextern void free_area_init(unsigned long * zones_size);\0Aextern void free_area_init_node(int nid, unsigned long * zones_size,\0A\09\09unsigned long zone_start_pfn, unsigned long *zholes_size);\0Aextern void free_initmem(void);\0A\0A/*\0A * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)\0A * into the buddy system. The freed pages will be poisoned with pattern\0A * \22poison\22 if it's within range [0, UCHAR_MAX].\0A * Return pages freed into the buddy system.\0A */\0Aextern unsigned long free_reserved_area(void *start, void *end,\0A\09\09\09\09\09int poison, char *s);\0A\0A#ifdef\09CONFIG_HIGHMEM\0A/*\0A * Free a highmem page into the buddy system, adjusting totalhigh_pages\0A * and totalram_pages.\0A */\0Aextern void free_highmem_page(struct page *page);\0A#endif\0A\0Aextern void adjust_managed_page_count(struct page *page, long count);\0Aextern void mem_init_print_info(const char *str);\0A\0Aextern void reserve_bootmem_region(phys_addr_t start, phys_addr_t end);\0A\0A/* Free the reserved page into the buddy system, so it gets managed. */\0Astatic inline void __free_reserved_page(struct page *page)\0A{\0A\09ClearPageReserved(page);\0A\09init_page_count(page);\0A\09__free_page(page);\0A}\0A\0Astatic inline void free_reserved_page(struct page *page)\0A{\0A\09__free_reserved_page(page);\0A\09adjust_managed_page_count(page, 1);\0A}\0A\0Astatic inline void mark_page_reserved(struct page *page)\0A{\0A\09SetPageReserved(page);\0A\09adjust_managed_page_count(page, -1);\0A}\0A\0A/*\0A * Default method to free all the __init memory into the buddy system.\0A * The freed pages will be poisoned with pattern \22poison\22 if it's within\0A * range [0, UCHAR_MAX].\0A * Return pages freed into the buddy system.\0A */\0Astatic inline unsigned long free_initmem_default(int poison)\0A{\0A\09extern char __init_begin[], __init_end[];\0A\0A\09return free_reserved_area(&__init_begin, &__init_end,\0A\09\09\09\09  poison, \22unused kernel\22);\0A}\0A\0Astatic inline unsigned long get_num_physpages(void)\0A{\0A\09int nid;\0A\09unsigned long phys_pages = 0;\0A\0A\09for_each_online_node(nid)\0A\09\09phys_pages += node_present_pages(nid);\0A\0A\09return phys_pages;\0A}\0A\0A#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\0A/*\0A * With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its\0A * zones, allocate the backing mem_map and account for memory holes in a more\0A * architecture independent manner. This is a substitute for creating the\0A * zone_sizes[] and zholes_size[] arrays and passing them to\0A * free_area_init_node()\0A *\0A * An architecture is expected to register range of page frames backed by\0A * physical memory with memblock_add[_node]() before calling\0A * free_area_init_nodes() passing in the PFN each zone ends at. At a basic\0A * usage, an architecture is expected to do something like\0A *\0A * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,\0A * \09\09\09\09\09\09\09 max_highmem_pfn};\0A * for_each_valid_physical_page_range()\0A * \09memblock_add_node(base, size, nid)\0A * free_area_init_nodes(max_zone_pfns);\0A *\0A * free_bootmem_with_active_regions() calls free_bootmem_node() for each\0A * registered physical page range.  Similarly\0A * sparse_memory_present_with_active_regions() calls memory_present() for\0A * each range when SPARSEMEM is enabled.\0A *\0A * See mm/page_alloc.c for more information on each function exposed by\0A * CONFIG_HAVE_MEMBLOCK_NODE_MAP.\0A */\0Aextern void free_area_init_nodes(unsigned long *max_zone_pfn);\0Aunsigned long node_map_pfn_alignment(void);\0Aunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\0A\09\09\09\09\09\09unsigned long end_pfn);\0Aextern unsigned long absent_pages_in_range(unsigned long start_pfn,\0A\09\09\09\09\09\09unsigned long end_pfn);\0Aextern void get_pfn_range_for_nid(unsigned int nid,\0A\09\09\09unsigned long *start_pfn, unsigned long *end_pfn);\0Aextern unsigned long find_min_pfn_with_active_regions(void);\0Aextern void free_bootmem_with_active_regions(int nid,\0A\09\09\09\09\09\09unsigned long max_low_pfn);\0Aextern void sparse_memory_present_with_active_regions(int nid);\0A\0A#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\0A\0A#if !defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) && \5C\0A    !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)\0Astatic inline int __early_pfn_to_nid(unsigned long pfn,\0A\09\09\09\09\09struct mminit_pfnnid_cache *state)\0A{\0A\09return 0;\0A}\0A#else\0A/* please see mm/page_alloc.c */\0Aextern int __meminit early_pfn_to_nid(unsigned long pfn);\0A/* there is a per-arch backend function. */\0Aextern int __meminit __early_pfn_to_nid(unsigned long pfn,\0A\09\09\09\09\09struct mminit_pfnnid_cache *state);\0A#endif\0A\0Aextern void set_dma_reserve(unsigned long new_dma_reserve);\0Aextern void memmap_init_zone(unsigned long, int, unsigned long,\0A\09\09\09\09unsigned long, enum memmap_context);\0Aextern void setup_per_zone_wmarks(void);\0Aextern int __meminit init_per_zone_wmark_min(void);\0Aextern void mem_init(void);\0Aextern void __init mmap_init(void);\0Aextern void show_mem(unsigned int flags, nodemask_t *nodemask);\0Aextern long si_mem_available(void);\0Aextern void si_meminfo(struct sysinfo * val);\0Aextern void si_meminfo_node(struct sysinfo *val, int nid);\0A#ifdef __HAVE_ARCH_RESERVED_KERNEL_PAGES\0Aextern unsigned long arch_reserved_kernel_pages(void);\0A#endif\0A\0Aextern __printf(3, 4)\0Avoid warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...);\0A\0Aextern void setup_per_cpu_pageset(void);\0A\0Aextern void zone_pcp_update(struct zone *zone);\0Aextern void zone_pcp_reset(struct zone *zone);\0A\0A/* page_alloc.c */\0Aextern int min_free_kbytes;\0Aextern int watermark_scale_factor;\0A\0A/* nommu.c */\0Aextern atomic_long_t mmap_pages_allocated;\0Aextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\0A\0A/* interval_tree.c */\0Avoid vma_interval_tree_insert(struct vm_area_struct *node,\0A\09\09\09      struct rb_root *root);\0Avoid vma_interval_tree_insert_after(struct vm_area_struct *node,\0A\09\09\09\09    struct vm_area_struct *prev,\0A\09\09\09\09    struct rb_root *root);\0Avoid vma_interval_tree_remove(struct vm_area_struct *node,\0A\09\09\09      struct rb_root *root);\0Astruct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,\0A\09\09\09\09unsigned long start, unsigned long last);\0Astruct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,\0A\09\09\09\09unsigned long start, unsigned long last);\0A\0A#define vma_interval_tree_foreach(vma, root, start, last)\09\09\5C\0A\09for (vma = vma_interval_tree_iter_first(root, start, last);\09\5C\0A\09     vma; vma = vma_interval_tree_iter_next(vma, start, last))\0A\0Avoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\0A\09\09\09\09   struct rb_root *root);\0Avoid anon_vma_interval_tree_remove(struct anon_vma_chain *node,\0A\09\09\09\09   struct rb_root *root);\0Astruct anon_vma_chain *anon_vma_interval_tree_iter_first(\0A\09struct rb_root *root, unsigned long start, unsigned long last);\0Astruct anon_vma_chain *anon_vma_interval_tree_iter_next(\0A\09struct anon_vma_chain *node, unsigned long start, unsigned long last);\0A#ifdef CONFIG_DEBUG_VM_RB\0Avoid anon_vma_interval_tree_verify(struct anon_vma_chain *node);\0A#endif\0A\0A#define anon_vma_interval_tree_foreach(avc, root, start, last)\09\09 \5C\0A\09for (avc = anon_vma_interval_tree_iter_first(root, start, last); \5C\0A\09     avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))\0A\0A/* mmap.c */\0Aextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\0Aextern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,\0A\09unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,\0A\09struct vm_area_struct *expand);\0Astatic inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,\0A\09unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)\0A{\0A\09return __vma_adjust(vma, start, end, pgoff, insert, NULL);\0A}\0Aextern struct vm_area_struct *vma_merge(struct mm_struct *,\0A\09struct vm_area_struct *prev, unsigned long addr, unsigned long end,\0A\09unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,\0A\09struct mempolicy *, struct vm_userfaultfd_ctx);\0Aextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\0Aextern int __split_vma(struct mm_struct *, struct vm_area_struct *,\0A\09unsigned long addr, int new_below);\0Aextern int split_vma(struct mm_struct *, struct vm_area_struct *,\0A\09unsigned long addr, int new_below);\0Aextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\0Aextern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,\0A\09struct rb_node **, struct rb_node *);\0Aextern void unlink_file_vma(struct vm_area_struct *);\0Aextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\0A\09unsigned long addr, unsigned long len, pgoff_t pgoff,\0A\09bool *need_rmap_locks);\0Aextern void exit_mmap(struct mm_struct *);\0A\0Astatic inline int check_data_rlimit(unsigned long rlim,\0A\09\09\09\09    unsigned long new,\0A\09\09\09\09    unsigned long start,\0A\09\09\09\09    unsigned long end_data,\0A\09\09\09\09    unsigned long start_data)\0A{\0A\09if (rlim < RLIM_INFINITY) {\0A\09\09if (((new - start) + (end_data - start_data)) > rlim)\0A\09\09\09return -ENOSPC;\0A\09}\0A\0A\09return 0;\0A}\0A\0Aextern int mm_take_all_locks(struct mm_struct *mm);\0Aextern void mm_drop_all_locks(struct mm_struct *mm);\0A\0Aextern void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);\0Aextern struct file *get_mm_exe_file(struct mm_struct *mm);\0Aextern struct file *get_task_exe_file(struct task_struct *task);\0A\0Aextern bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long npages);\0Aextern void vm_stat_account(struct mm_struct *, vm_flags_t, long npages);\0A\0Aextern bool vma_is_special_mapping(const struct vm_area_struct *vma,\0A\09\09\09\09   const struct vm_special_mapping *sm);\0Aextern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,\0A\09\09\09\09   unsigned long addr, unsigned long len,\0A\09\09\09\09   unsigned long flags,\0A\09\09\09\09   const struct vm_special_mapping *spec);\0A/* This is an obsolete alternative to _install_special_mapping. */\0Aextern int install_special_mapping(struct mm_struct *mm,\0A\09\09\09\09   unsigned long addr, unsigned long len,\0A\09\09\09\09   unsigned long flags, struct page **pages);\0A\0Aextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\0A\0Aextern unsigned long mmap_region(struct file *file, unsigned long addr,\0A\09unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,\0A\09struct list_head *uf);\0Aextern unsigned long do_mmap(struct file *file, unsigned long addr,\0A\09unsigned long len, unsigned long prot, unsigned long flags,\0A\09vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,\0A\09struct list_head *uf);\0Aextern int do_munmap(struct mm_struct *, unsigned long, size_t,\0A\09\09     struct list_head *uf);\0A\0Astatic inline unsigned long\0Ado_mmap_pgoff(struct file *file, unsigned long addr,\0A\09unsigned long len, unsigned long prot, unsigned long flags,\0A\09unsigned long pgoff, unsigned long *populate,\0A\09struct list_head *uf)\0A{\0A\09return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate, uf);\0A}\0A\0A#ifdef CONFIG_MMU\0Aextern int __mm_populate(unsigned long addr, unsigned long len,\0A\09\09\09 int ignore_errors);\0Astatic inline void mm_populate(unsigned long addr, unsigned long len)\0A{\0A\09/* Ignore errors */\0A\09(void) __mm_populate(addr, len, 1);\0A}\0A#else\0Astatic inline void mm_populate(unsigned long addr, unsigned long len) {}\0A#endif\0A\0A/* These take the mm semaphore themselves */\0Aextern int __must_check vm_brk(unsigned long, unsigned long);\0Aextern int __must_check vm_brk_flags(unsigned long, unsigned long, unsigned long);\0Aextern int vm_munmap(unsigned long, size_t);\0Aextern unsigned long __must_check vm_mmap(struct file *, unsigned long,\0A        unsigned long, unsigned long,\0A        unsigned long, unsigned long);\0A\0Astruct vm_unmapped_area_info {\0A#define VM_UNMAPPED_AREA_TOPDOWN 1\0A\09unsigned long flags;\0A\09unsigned long length;\0A\09unsigned long low_limit;\0A\09unsigned long high_limit;\0A\09unsigned long align_mask;\0A\09unsigned long align_offset;\0A};\0A\0Aextern unsigned long unmapped_area(struct vm_unmapped_area_info *info);\0Aextern unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info);\0A\0A/*\0A * Search for an unmapped address range.\0A *\0A * We are looking for a range that:\0A * - does not intersect with any VMA;\0A * - is contained within the [low_limit, high_limit) interval;\0A * - is at least the desired size.\0A * - satisfies (begin_addr & align_mask) == (align_offset & align_mask)\0A */\0Astatic inline unsigned long\0Avm_unmapped_area(struct vm_unmapped_area_info *info)\0A{\0A\09if (info->flags & VM_UNMAPPED_AREA_TOPDOWN)\0A\09\09return unmapped_area_topdown(info);\0A\09else\0A\09\09return unmapped_area(info);\0A}\0A\0A/* truncate.c */\0Aextern void truncate_inode_pages(struct address_space *, loff_t);\0Aextern void truncate_inode_pages_range(struct address_space *,\0A\09\09\09\09       loff_t lstart, loff_t lend);\0Aextern void truncate_inode_pages_final(struct address_space *);\0A\0A/* generic vm_area_ops exported for stackable file systems */\0Aextern int filemap_fault(struct vm_fault *vmf);\0Aextern void filemap_map_pages(struct vm_fault *vmf,\0A\09\09pgoff_t start_pgoff, pgoff_t end_pgoff);\0Aextern int filemap_page_mkwrite(struct vm_fault *vmf);\0A\0A/* mm/page-writeback.c */\0Aint write_one_page(struct page *page, int wait);\0Avoid task_dirty_inc(struct task_struct *tsk);\0A\0A/* readahead.c */\0A#define VM_MAX_READAHEAD\09128\09/* kbytes */\0A#define VM_MIN_READAHEAD\0916\09/* kbytes (includes current page) */\0A\0Aint force_page_cache_readahead(struct address_space *mapping, struct file *filp,\0A\09\09\09pgoff_t offset, unsigned long nr_to_read);\0A\0Avoid page_cache_sync_readahead(struct address_space *mapping,\0A\09\09\09       struct file_ra_state *ra,\0A\09\09\09       struct file *filp,\0A\09\09\09       pgoff_t offset,\0A\09\09\09       unsigned long size);\0A\0Avoid page_cache_async_readahead(struct address_space *mapping,\0A\09\09\09\09struct file_ra_state *ra,\0A\09\09\09\09struct file *filp,\0A\09\09\09\09struct page *pg,\0A\09\09\09\09pgoff_t offset,\0A\09\09\09\09unsigned long size);\0A\0Aextern unsigned long stack_guard_gap;\0A/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */\0Aextern int expand_stack(struct vm_area_struct *vma, unsigned long address);\0A\0A/* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */\0Aextern int expand_downwards(struct vm_area_struct *vma,\0A\09\09unsigned long address);\0A#if VM_GROWSUP\0Aextern int expand_upwards(struct vm_area_struct *vma, unsigned long address);\0A#else\0A  #define expand_upwards(vma, address) (0)\0A#endif\0A\0A/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\0Aextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\0Aextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\0A\09\09\09\09\09     struct vm_area_struct **pprev);\0A\0A/* Look up the first VMA which intersects the interval start_addr..end_addr-1,\0A   NULL if none.  Assume start_addr < end_addr. */\0Astatic inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)\0A{\0A\09struct vm_area_struct * vma = find_vma(mm,start_addr);\0A\0A\09if (vma && end_addr <= vma->vm_start)\0A\09\09vma = NULL;\0A\09return vma;\0A}\0A\0Astatic inline unsigned long vm_start_gap(struct vm_area_struct *vma)\0A{\0A\09unsigned long vm_start = vma->vm_start;\0A\0A\09if (vma->vm_flags & VM_GROWSDOWN) {\0A\09\09vm_start -= stack_guard_gap;\0A\09\09if (vm_start > vma->vm_start)\0A\09\09\09vm_start = 0;\0A\09}\0A\09return vm_start;\0A}\0A\0Astatic inline unsigned long vm_end_gap(struct vm_area_struct *vma)\0A{\0A\09unsigned long vm_end = vma->vm_end;\0A\0A\09if (vma->vm_flags & VM_GROWSUP) {\0A\09\09vm_end += stack_guard_gap;\0A\09\09if (vm_end < vma->vm_end)\0A\09\09\09vm_end = -PAGE_SIZE;\0A\09}\0A\09return vm_end;\0A}\0A\0Astatic inline unsigned long vma_pages(struct vm_area_struct *vma)\0A{\0A\09return (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\0A}\0A\0A/* Look up the first VMA which exactly match the interval vm_start ... vm_end */\0Astatic inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,\0A\09\09\09\09unsigned long vm_start, unsigned long vm_end)\0A{\0A\09struct vm_area_struct *vma = find_vma(mm, vm_start);\0A\0A\09if (vma && (vma->vm_start != vm_start || vma->vm_end != vm_end))\0A\09\09vma = NULL;\0A\0A\09return vma;\0A}\0A\0A#ifdef CONFIG_MMU\0Apgprot_t vm_get_page_prot(unsigned long vm_flags);\0Avoid vma_set_page_prot(struct vm_area_struct *vma);\0A#else\0Astatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\0A{\0A\09return __pgprot(0);\0A}\0Astatic inline void vma_set_page_prot(struct vm_area_struct *vma)\0A{\0A\09vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\0A}\0A#endif\0A\0A#ifdef CONFIG_NUMA_BALANCING\0Aunsigned long change_prot_numa(struct vm_area_struct *vma,\0A\09\09\09unsigned long start, unsigned long end);\0A#endif\0A\0Astruct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);\0Aint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\0A\09\09\09unsigned long pfn, unsigned long size, pgprot_t);\0Aint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\0Aint vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\0A\09\09\09unsigned long pfn);\0Aint vm_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\0A\09\09\09unsigned long pfn, pgprot_t pgprot);\0Aint vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\0A\09\09\09pfn_t pfn);\0Aint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);\0A\0A\0Astruct page *follow_page_mask(struct vm_area_struct *vma,\0A\09\09\09      unsigned long address, unsigned int foll_flags,\0A\09\09\09      unsigned int *page_mask);\0A\0Astatic inline struct page *follow_page(struct vm_area_struct *vma,\0A\09\09unsigned long address, unsigned int foll_flags)\0A{\0A\09unsigned int unused_page_mask;\0A\09return follow_page_mask(vma, address, foll_flags, &unused_page_mask);\0A}\0A\0A#define FOLL_WRITE\090x01\09/* check pte is writable */\0A#define FOLL_TOUCH\090x02\09/* mark page accessed */\0A#define FOLL_GET\090x04\09/* do get_page on page */\0A#define FOLL_DUMP\090x08\09/* give error on hole if it would be zero */\0A#define FOLL_FORCE\090x10\09/* get_user_pages read/write w/o permission */\0A#define FOLL_NOWAIT\090x20\09/* if a disk transfer is needed, start the IO\0A\09\09\09\09 * and return without waiting upon it */\0A#define FOLL_POPULATE\090x40\09/* fault in page */\0A#define FOLL_SPLIT\090x80\09/* don't return transhuge pages, split them */\0A#define FOLL_HWPOISON\090x100\09/* check page is hwpoisoned */\0A#define FOLL_NUMA\090x200\09/* force NUMA hinting page fault */\0A#define FOLL_MIGRATION\090x400\09/* wait for page to replace migration entry */\0A#define FOLL_TRIED\090x800\09/* a retry, previous pass started an IO */\0A#define FOLL_MLOCK\090x1000\09/* lock present pages */\0A#define FOLL_REMOTE\090x2000\09/* we are working on non-current tsk/mm */\0A#define FOLL_COW\090x4000\09/* internal GUP flag */\0A\0Atypedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,\0A\09\09\09void *data);\0Aextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\0A\09\09\09       unsigned long size, pte_fn_t fn, void *data);\0A\0A\0A#ifdef CONFIG_PAGE_POISONING\0Aextern bool page_poisoning_enabled(void);\0Aextern void kernel_poison_pages(struct page *page, int numpages, int enable);\0Aextern bool page_is_poisoned(struct page *page);\0A#else\0Astatic inline bool page_poisoning_enabled(void) { return false; }\0Astatic inline void kernel_poison_pages(struct page *page, int numpages,\0A\09\09\09\09\09int enable) { }\0Astatic inline bool page_is_poisoned(struct page *page) { return false; }\0A#endif\0A\0A#ifdef CONFIG_DEBUG_PAGEALLOC\0Aextern bool _debug_pagealloc_enabled;\0Aextern void __kernel_map_pages(struct page *page, int numpages, int enable);\0A\0Astatic inline bool debug_pagealloc_enabled(void)\0A{\0A\09return _debug_pagealloc_enabled;\0A}\0A\0Astatic inline void\0Akernel_map_pages(struct page *page, int numpages, int enable)\0A{\0A\09if (!debug_pagealloc_enabled())\0A\09\09return;\0A\0A\09__kernel_map_pages(page, numpages, enable);\0A}\0A#ifdef CONFIG_HIBERNATION\0Aextern bool kernel_page_present(struct page *page);\0A#endif\09/* CONFIG_HIBERNATION */\0A#else\09/* CONFIG_DEBUG_PAGEALLOC */\0Astatic inline void\0Akernel_map_pages(struct page *page, int numpages, int enable) {}\0A#ifdef CONFIG_HIBERNATION\0Astatic inline bool kernel_page_present(struct page *page) { return true; }\0A#endif\09/* CONFIG_HIBERNATION */\0Astatic inline bool debug_pagealloc_enabled(void)\0A{\0A\09return false;\0A}\0A#endif\09/* CONFIG_DEBUG_PAGEALLOC */\0A\0A#ifdef __HAVE_ARCH_GATE_AREA\0Aextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\0Aextern int in_gate_area_no_mm(unsigned long addr);\0Aextern int in_gate_area(struct mm_struct *mm, unsigned long addr);\0A#else\0Astatic inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)\0A{\0A\09return NULL;\0A}\0Astatic inline int in_gate_area_no_mm(unsigned long addr) { return 0; }\0Astatic inline int in_gate_area(struct mm_struct *mm, unsigned long addr)\0A{\0A\09return 0;\0A}\0A#endif\09/* __HAVE_ARCH_GATE_AREA */\0A\0Aextern bool process_shares_mm(struct task_struct *p, struct mm_struct *mm);\0A\0A#ifdef CONFIG_SYSCTL\0Aextern int sysctl_drop_caches;\0Aint drop_caches_sysctl_handler(struct ctl_table *, int,\0A\09\09\09\09\09void __user *, size_t *, loff_t *);\0A#endif\0A\0Avoid drop_slab(void);\0Avoid drop_slab_node(int nid);\0A\0A#ifndef CONFIG_MMU\0A#define randomize_va_space 0\0A#else\0Aextern int randomize_va_space;\0A#endif\0A\0Aconst char * arch_vma_name(struct vm_area_struct *vma);\0Avoid print_vma_addr(char *prefix, unsigned long rip);\0A\0Avoid sparse_mem_maps_populate_node(struct page **map_map,\0A\09\09\09\09   unsigned long pnum_begin,\0A\09\09\09\09   unsigned long pnum_end,\0A\09\09\09\09   unsigned long map_count,\0A\09\09\09\09   int nodeid);\0A\0Astruct page *sparse_mem_map_populate(unsigned long pnum, int nid);\0Apgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\0Ap4d_t *vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node);\0Apud_t *vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node);\0Apmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\0Apte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);\0Avoid *vmemmap_alloc_block(unsigned long size, int node);\0Astruct vmem_altmap;\0Avoid *__vmemmap_alloc_block_buf(unsigned long size, int node,\0A\09\09struct vmem_altmap *altmap);\0Astatic inline void *vmemmap_alloc_block_buf(unsigned long size, int node)\0A{\0A\09return __vmemmap_alloc_block_buf(size, node, NULL);\0A}\0A\0Avoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\0Aint vmemmap_populate_basepages(unsigned long start, unsigned long end,\0A\09\09\09       int node);\0Aint vmemmap_populate(unsigned long start, unsigned long end, int node);\0Avoid vmemmap_populate_print_last(void);\0A#ifdef CONFIG_MEMORY_HOTPLUG\0Avoid vmemmap_free(unsigned long start, unsigned long end);\0A#endif\0Avoid register_page_bootmem_memmap(unsigned long section_nr, struct page *map,\0A\09\09\09\09  unsigned long size);\0A\0Aenum mf_flags {\0A\09MF_COUNT_INCREASED = 1 << 0,\0A\09MF_ACTION_REQUIRED = 1 << 1,\0A\09MF_MUST_KILL = 1 << 2,\0A\09MF_SOFT_OFFLINE = 1 << 3,\0A};\0Aextern int memory_failure(unsigned long pfn, int trapno, int flags);\0Aextern void memory_failure_queue(unsigned long pfn, int trapno, int flags);\0Aextern int unpoison_memory(unsigned long pfn);\0Aextern int get_hwpoison_page(struct page *page);\0A#define put_hwpoison_page(page)\09put_page(page)\0Aextern int sysctl_memory_failure_early_kill;\0Aextern int sysctl_memory_failure_recovery;\0Aextern void shake_page(struct page *p, int access);\0Aextern atomic_long_t num_poisoned_pages;\0Aextern int soft_offline_page(struct page *page, int flags);\0A\0A\0A/*\0A * Error handlers for various types of pages.\0A */\0Aenum mf_result {\0A\09MF_IGNORED,\09/* Error: cannot be handled */\0A\09MF_FAILED,\09/* Error: handling failed */\0A\09MF_DELAYED,\09/* Will be handled later */\0A\09MF_RECOVERED,\09/* Successfully recovered */\0A};\0A\0Aenum mf_action_page_type {\0A\09MF_MSG_KERNEL,\0A\09MF_MSG_KERNEL_HIGH_ORDER,\0A\09MF_MSG_SLAB,\0A\09MF_MSG_DIFFERENT_COMPOUND,\0A\09MF_MSG_POISONED_HUGE,\0A\09MF_MSG_HUGE,\0A\09MF_MSG_FREE_HUGE,\0A\09MF_MSG_UNMAP_FAILED,\0A\09MF_MSG_DIRTY_SWAPCACHE,\0A\09MF_MSG_CLEAN_SWAPCACHE,\0A\09MF_MSG_DIRTY_MLOCKED_LRU,\0A\09MF_MSG_CLEAN_MLOCKED_LRU,\0A\09MF_MSG_DIRTY_UNEVICTABLE_LRU,\0A\09MF_MSG_CLEAN_UNEVICTABLE_LRU,\0A\09MF_MSG_DIRTY_LRU,\0A\09MF_MSG_CLEAN_LRU,\0A\09MF_MSG_TRUNCATED_LRU,\0A\09MF_MSG_BUDDY,\0A\09MF_MSG_BUDDY_2ND,\0A\09MF_MSG_UNKNOWN,\0A};\0A\0A#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\0Aextern void clear_huge_page(struct page *page,\0A\09\09\09    unsigned long addr,\0A\09\09\09    unsigned int pages_per_huge_page);\0Aextern void copy_user_huge_page(struct page *dst, struct page *src,\0A\09\09\09\09unsigned long addr, struct vm_area_struct *vma,\0A\09\09\09\09unsigned int pages_per_huge_page);\0Aextern long copy_huge_page_from_user(struct page *dst_page,\0A\09\09\09\09const void __user *usr_src,\0A\09\09\09\09unsigned int pages_per_huge_page,\0A\09\09\09\09bool allow_pagefault);\0A#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\0A\0Aextern struct page_ext_operations debug_guardpage_ops;\0Aextern struct page_ext_operations page_poisoning_ops;\0A\0A#ifdef CONFIG_DEBUG_PAGEALLOC\0Aextern unsigned int _debug_guardpage_minorder;\0Aextern bool _debug_guardpage_enabled;\0A\0Astatic inline unsigned int debug_guardpage_minorder(void)\0A{\0A\09return _debug_guardpage_minorder;\0A}\0A\0Astatic inline bool debug_guardpage_enabled(void)\0A{\0A\09return _debug_guardpage_enabled;\0A}\0A\0Astatic inline bool page_is_guard(struct page *page)\0A{\0A\09struct page_ext *page_ext;\0A\0A\09if (!debug_guardpage_enabled())\0A\09\09return false;\0A\0A\09page_ext = lookup_page_ext(page);\0A\09if (unlikely(!page_ext))\0A\09\09return false;\0A\0A\09return test_bit(PAGE_EXT_DEBUG_GUARD, &page_ext->flags);\0A}\0A#else\0Astatic inline unsigned int debug_guardpage_minorder(void) { return 0; }\0Astatic inline bool debug_guardpage_enabled(void) { return false; }\0Astatic inline bool page_is_guard(struct page *page) { return false; }\0A#endif /* CONFIG_DEBUG_PAGEALLOC */\0A\0A#if MAX_NUMNODES > 1\0Avoid __init setup_nr_node_ids(void);\0A#else\0Astatic inline void setup_nr_node_ids(void) {}\0A#endif\0A\0A#endif /* __KERNEL__ */\0A#endif /* _LINUX_MM_H */\0A")
!13 = !{!14, !15, !16}
!14 = !DIEnumerator(name: "PE_SIZE_PTE", value: 0, isUnsigned: true)
!15 = !DIEnumerator(name: "PE_SIZE_PMD", value: 1, isUnsigned: true)
!16 = !DIEnumerator(name: "PE_SIZE_PUD", value: 2, isUnsigned: true)
!17 = !DICompositeType(tag: DW_TAG_enumeration_type, name: "uprobe_task_state", file: !18, line: 64, baseType: !7, size: 32, elements: !19)
!18 = !DIFile(filename: "/lib/modules/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f/build/include/linux/uprobes.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "ddf078523149f2b46d174639547c9d8b", source: "#ifndef _LINUX_UPROBES_H\0A#define _LINUX_UPROBES_H\0A/*\0A * User-space Probes (UProbes)\0A *\0A * This program is free software; you can redistribute it and/or modify\0A * it under the terms of the GNU General Public License as published by\0A * the Free Software Foundation; either version 2 of the License, or\0A * (at your option) any later version.\0A *\0A * This program is distributed in the hope that it will be useful,\0A * but WITHOUT ANY WARRANTY; without even the implied warranty of\0A * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\0A * GNU General Public License for more details.\0A *\0A * You should have received a copy of the GNU General Public License\0A * along with this program; if not, write to the Free Software\0A * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\0A *\0A * Copyright (C) IBM Corporation, 2008-2012\0A * Authors:\0A *\09Srikar Dronamraju\0A *\09Jim Keniston\0A * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra\0A */\0A\0A#include <linux/errno.h>\0A#include <linux/rbtree.h>\0A#include <linux/types.h>\0A#include <linux/wait.h>\0A\0Astruct vm_area_struct;\0Astruct mm_struct;\0Astruct inode;\0Astruct notifier_block;\0Astruct page;\0A\0A#define UPROBE_HANDLER_REMOVE\09\091\0A#define UPROBE_HANDLER_MASK\09\091\0A\0A#define MAX_URETPROBE_DEPTH\09\0964\0A\0Aenum uprobe_filter_ctx {\0A\09UPROBE_FILTER_REGISTER,\0A\09UPROBE_FILTER_UNREGISTER,\0A\09UPROBE_FILTER_MMAP,\0A};\0A\0Astruct uprobe_consumer {\0A\09int (*handler)(struct uprobe_consumer *self, struct pt_regs *regs);\0A\09int (*ret_handler)(struct uprobe_consumer *self,\0A\09\09\09\09unsigned long func,\0A\09\09\09\09struct pt_regs *regs);\0A\09bool (*filter)(struct uprobe_consumer *self,\0A\09\09\09\09enum uprobe_filter_ctx ctx,\0A\09\09\09\09struct mm_struct *mm);\0A\0A\09struct uprobe_consumer *next;\0A};\0A\0A#ifdef CONFIG_UPROBES\0A#include <asm/uprobes.h>\0A\0Aenum uprobe_task_state {\0A\09UTASK_RUNNING,\0A\09UTASK_SSTEP,\0A\09UTASK_SSTEP_ACK,\0A\09UTASK_SSTEP_TRAPPED,\0A};\0A\0A/*\0A * uprobe_task: Metadata of a task while it singlesteps.\0A */\0Astruct uprobe_task {\0A\09enum uprobe_task_state\09\09state;\0A\0A\09union {\0A\09\09struct {\0A\09\09\09struct arch_uprobe_task\09autask;\0A\09\09\09unsigned long\09\09vaddr;\0A\09\09};\0A\0A\09\09struct {\0A\09\09\09struct callback_head\09dup_xol_work;\0A\09\09\09unsigned long\09\09dup_xol_addr;\0A\09\09};\0A\09};\0A\0A\09struct uprobe\09\09\09*active_uprobe;\0A\09unsigned long\09\09\09xol_vaddr;\0A\0A\09struct return_instance\09\09*return_instances;\0A\09unsigned int\09\09\09depth;\0A};\0A\0Astruct return_instance {\0A\09struct uprobe\09\09*uprobe;\0A\09unsigned long\09\09func;\0A\09unsigned long\09\09stack;\09\09/* stack pointer */\0A\09unsigned long\09\09orig_ret_vaddr; /* original return address */\0A\09bool\09\09\09chained;\09/* true, if instance is nested */\0A\0A\09struct return_instance\09*next;\09\09/* keep as stack */\0A};\0A\0Aenum rp_check {\0A\09RP_CHECK_CALL,\0A\09RP_CHECK_CHAIN_CALL,\0A\09RP_CHECK_RET,\0A};\0A\0Astruct xol_area;\0A\0Astruct uprobes_state {\0A\09struct xol_area\09\09*xol_area;\0A};\0A\0Aextern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);\0Aextern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);\0Aextern bool is_swbp_insn(uprobe_opcode_t *insn);\0Aextern bool is_trap_insn(uprobe_opcode_t *insn);\0Aextern unsigned long uprobe_get_swbp_addr(struct pt_regs *regs);\0Aextern unsigned long uprobe_get_trap_addr(struct pt_regs *regs);\0Aextern int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr, uprobe_opcode_t);\0Aextern int uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);\0Aextern int uprobe_apply(struct inode *inode, loff_t offset, struct uprobe_consumer *uc, bool);\0Aextern void uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);\0Aextern int uprobe_mmap(struct vm_area_struct *vma);\0Aextern void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end);\0Aextern void uprobe_start_dup_mmap(void);\0Aextern void uprobe_end_dup_mmap(void);\0Aextern void uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm);\0Aextern void uprobe_free_utask(struct task_struct *t);\0Aextern void uprobe_copy_process(struct task_struct *t, unsigned long flags);\0Aextern int uprobe_post_sstep_notifier(struct pt_regs *regs);\0Aextern int uprobe_pre_sstep_notifier(struct pt_regs *regs);\0Aextern void uprobe_notify_resume(struct pt_regs *regs);\0Aextern bool uprobe_deny_signal(void);\0Aextern bool arch_uprobe_skip_sstep(struct arch_uprobe *aup, struct pt_regs *regs);\0Aextern void uprobe_clear_state(struct mm_struct *mm);\0Aextern int  arch_uprobe_analyze_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long addr);\0Aextern int  arch_uprobe_pre_xol(struct arch_uprobe *aup, struct pt_regs *regs);\0Aextern int  arch_uprobe_post_xol(struct arch_uprobe *aup, struct pt_regs *regs);\0Aextern bool arch_uprobe_xol_was_trapped(struct task_struct *tsk);\0Aextern int  arch_uprobe_exception_notify(struct notifier_block *self, unsigned long val, void *data);\0Aextern void arch_uprobe_abort_xol(struct arch_uprobe *aup, struct pt_regs *regs);\0Aextern unsigned long arch_uretprobe_hijack_return_addr(unsigned long trampoline_vaddr, struct pt_regs *regs);\0Aextern bool arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx, struct pt_regs *regs);\0Aextern bool arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs);\0Aextern void arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,\0A\09\09\09\09\09 void *src, unsigned long len);\0A#else /* !CONFIG_UPROBES */\0Astruct uprobes_state {\0A};\0A\0A#define uprobe_get_trap_addr(regs)\09instruction_pointer(regs)\0A\0Astatic inline int\0Auprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)\0A{\0A\09return -ENOSYS;\0A}\0Astatic inline int\0Auprobe_apply(struct inode *inode, loff_t offset, struct uprobe_consumer *uc, bool add)\0A{\0A\09return -ENOSYS;\0A}\0Astatic inline void\0Auprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)\0A{\0A}\0Astatic inline int uprobe_mmap(struct vm_area_struct *vma)\0A{\0A\09return 0;\0A}\0Astatic inline void\0Auprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end)\0A{\0A}\0Astatic inline void uprobe_start_dup_mmap(void)\0A{\0A}\0Astatic inline void uprobe_end_dup_mmap(void)\0A{\0A}\0Astatic inline void\0Auprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm)\0A{\0A}\0Astatic inline void uprobe_notify_resume(struct pt_regs *regs)\0A{\0A}\0Astatic inline bool uprobe_deny_signal(void)\0A{\0A\09return false;\0A}\0Astatic inline void uprobe_free_utask(struct task_struct *t)\0A{\0A}\0Astatic inline void uprobe_copy_process(struct task_struct *t, unsigned long flags)\0A{\0A}\0Astatic inline void uprobe_clear_state(struct mm_struct *mm)\0A{\0A}\0A#endif /* !CONFIG_UPROBES */\0A#endif\09/* _LINUX_UPROBES_H */\0A")
!19 = !{!20, !21, !22, !23}
!20 = !DIEnumerator(name: "UTASK_RUNNING", value: 0, isUnsigned: true)
!21 = !DIEnumerator(name: "UTASK_SSTEP", value: 1, isUnsigned: true)
!22 = !DIEnumerator(name: "UTASK_SSTEP_ACK", value: 2, isUnsigned: true)
!23 = !DIEnumerator(name: "UTASK_SSTEP_TRAPPED", value: 3, isUnsigned: true)
!24 = !{!25, !28}
!25 = !DIDerivedType(tag: DW_TAG_typedef, name: "u64", file: !26, line: 25, baseType: !27)
!26 = !DIFile(filename: "/lib/modules/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f/build/include/asm-generic/int-ll64.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "be4685aeb7e05c85c4dbc18556740954", source: "/*\0A * asm-generic/int-ll64.h\0A *\0A * Integer declarations for architectures which use \22long long\22\0A * for 64-bit types.\0A */\0A#ifndef _ASM_GENERIC_INT_LL64_H\0A#define _ASM_GENERIC_INT_LL64_H\0A\0A#include <uapi/asm-generic/int-ll64.h>\0A\0A\0A#ifndef __ASSEMBLY__\0A\0Atypedef signed char s8;\0Atypedef unsigned char u8;\0A\0Atypedef signed short s16;\0Atypedef unsigned short u16;\0A\0Atypedef signed int s32;\0Atypedef unsigned int u32;\0A\0Atypedef signed long long s64;\0Atypedef unsigned long long u64;\0A\0A#define S8_C(x)  x\0A#define U8_C(x)  x ## U\0A#define S16_C(x) x\0A#define U16_C(x) x ## U\0A#define S32_C(x) x\0A#define U32_C(x) x ## U\0A#define S64_C(x) x ## LL\0A#define U64_C(x) x ## ULL\0A\0A#else /* __ASSEMBLY__ */\0A\0A#define S8_C(x)  x\0A#define U8_C(x)  x\0A#define S16_C(x) x\0A#define U16_C(x) x\0A#define S32_C(x) x\0A#define U32_C(x) x\0A#define S64_C(x) x\0A#define U64_C(x) x\0A\0A#endif /* __ASSEMBLY__ */\0A\0A#endif /* _ASM_GENERIC_INT_LL64_H */\0A")
!27 = !DIBasicType(name: "long long unsigned int", size: 64, encoding: DW_ATE_unsigned)
!28 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null, size: 64)
!29 = !{!0, !30, !48, !55, !61, !66, !71, !76, !81, !83, !88, !93}
!30 = !DIGlobalVariableExpression(var: !31, expr: !DIExpression())
!31 = distinct !DIGlobalVariable(name: "probe_SyS_nanosleep_1_events", scope: !2, file: !32, line: 28, type: !33, isLocal: false, isDefinition: true)
!32 = !DIFile(filename: "/virtual/main.c", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f")
!33 = distinct !DICompositeType(tag: DW_TAG_structure_type, name: "probe_SyS_nanosleep_1_events_table_t", file: !32, line: 28, size: 256, elements: !34)
!34 = !{!35, !37, !39, !43, !47}
!35 = !DIDerivedType(tag: DW_TAG_member, name: "key", scope: !33, file: !32, line: 28, baseType: !36, size: 32)
!36 = !DIBasicType(name: "int", size: 32, encoding: DW_ATE_signed)
!37 = !DIDerivedType(tag: DW_TAG_member, name: "leaf", scope: !33, file: !32, line: 28, baseType: !38, size: 32, offset: 32)
!38 = !DIDerivedType(tag: DW_TAG_typedef, name: "u32", file: !26, line: 22, baseType: !7)
!39 = !DIDerivedType(tag: DW_TAG_member, name: "perf_submit", scope: !33, file: !32, line: 28, baseType: !40, size: 64, offset: 64)
!40 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !41, size: 64)
!41 = !DISubroutineType(types: !42)
!42 = !{!36, !28, !28, !38}
!43 = !DIDerivedType(tag: DW_TAG_member, name: "perf_submit_skb", scope: !33, file: !32, line: 28, baseType: !44, size: 64, offset: 128)
!44 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !45, size: 64)
!45 = !DISubroutineType(types: !46)
!46 = !{!36, !28, !38, !28, !38}
!47 = !DIDerivedType(tag: DW_TAG_member, name: "max_entries", scope: !33, file: !32, line: 28, baseType: !38, size: 32, offset: 192)
!48 = !DIGlobalVariableExpression(var: !49, expr: !DIExpression())
!49 = distinct !DIGlobalVariable(name: "_license", scope: !2, file: !50, line: 26, type: !51, isLocal: false, isDefinition: true)
!50 = !DIFile(filename: "/virtual/include/bcc/footer.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "405d657aaefe763baa77b6d466acedd1", source: "\0A/*\0A * Copyright (c) 2018 Clevernet, Inc.\0A *\0A * Licensed under the Apache License, Version 2.0 (the \22License\22);\0A * you may not use this file except in compliance with the License.\0A * You may obtain a copy of the License at\0A *\0A * http://www.apache.org/licenses/LICENSE-2.0\0A *\0A * Unless required by applicable law or agreed to in writing, software\0A * distributed under the License is distributed on an \22AS IS\22 BASIS,\0A * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\0A * See the License for the specific language governing permissions and\0A * limitations under the License.\0A */\0A\0A#ifndef BPF_LICENSE\0A/* No license defined, using GPL\0A * You can define your own BPF_LICENSE in your C code */\0A#define BPF_LICENSE GPL\0A#endif\0A#define ___LICENSE(s) #s\0A#define __LICENSE(s) ___LICENSE(s)\0A#define _LICENSE __LICENSE(BPF_LICENSE)\0Achar _license[] SEC(\22license\22) = _LICENSE;\0A\0A")
!51 = !DICompositeType(tag: DW_TAG_array_type, baseType: !52, size: 32, elements: !53)
!52 = !DIBasicType(name: "char", size: 8, encoding: DW_ATE_signed_char)
!53 = !{!54}
!54 = !DISubrange(count: 4)
!55 = !DIGlobalVariableExpression(var: !56, expr: !DIExpression())
!56 = distinct !DIGlobalVariable(name: "bpf_skb_store_bytes", scope: !2, file: !57, line: 440, type: !58, isLocal: true, isDefinition: true)
!57 = !DIFile(filename: "/virtual/include/bcc/helpers.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "96e71f3c459aadc9df44c06bf7c6ab06", source: "\0A/*\0A * Copyright (c) 2015 PLUMgrid, Inc.\0A *\0A * Licensed under the Apache License, Version 2.0 (the \22License\22);\0A * you may not use this file except in compliance with the License.\0A * You may obtain a copy of the License at\0A *\0A * http://www.apache.org/licenses/LICENSE-2.0\0A *\0A * Unless required by applicable law or agreed to in writing, software\0A * distributed under the License is distributed on an \22AS IS\22 BASIS,\0A * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\0A * See the License for the specific language governing permissions and\0A * limitations under the License.\0A */\0A#ifndef __BPF_HELPERS_H\0A#define __BPF_HELPERS_H\0A\0A#include <uapi/linux/bpf.h>\0A#include <uapi/linux/if_packet.h>\0A#include <linux/version.h>\0A#include <linux/log2.h>\0A\0A#ifndef CONFIG_BPF_SYSCALL\0A#error \22CONFIG_BPF_SYSCALL is undefined, please check your .config or ask your Linux distro to enable this feature\22\0A#endif\0A\0A#ifdef PERF_MAX_STACK_DEPTH\0A#define BPF_MAX_STACK_DEPTH PERF_MAX_STACK_DEPTH\0A#else\0A#define BPF_MAX_STACK_DEPTH 127\0A#endif\0A\0A/* helper macro to place programs, maps, license in\0A * different sections in elf_bpf file. Section names\0A * are interpreted by elf_bpf loader\0A */\0A#define SEC(NAME) __attribute__((section(NAME), used))\0A\0A// Changes to the macro require changes in BFrontendAction classes\0A#define BPF_F_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries, _flags) \5C\0Astruct _name##_table_t { \5C\0A  _key_type key; \5C\0A  _leaf_type leaf; \5C\0A  _leaf_type * (*lookup) (_key_type *); \5C\0A  _leaf_type * (*lookup_or_init) (_key_type *, _leaf_type *); \5C\0A  int (*update) (_key_type *, _leaf_type *); \5C\0A  int (*insert) (_key_type *, _leaf_type *); \5C\0A  int (*delete) (_key_type *); \5C\0A  void (*call) (void *, int index); \5C\0A  void (*increment) (_key_type, ...); \5C\0A  int (*get_stackid) (void *, u64); \5C\0A  u32 max_entries; \5C\0A  int flags; \5C\0A}; \5C\0A__attribute__((section(\22maps/\22 _table_type))) \5C\0Astruct _name##_table_t _name = { .flags = (_flags), .max_entries = (_max_entries) }\0A\0A#define BPF_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries) \5C\0ABPF_F_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries, 0)\0A\0A// define a table same as above but allow it to be referenced by other modules\0A#define BPF_TABLE_PUBLIC(_table_type, _key_type, _leaf_type, _name, _max_entries) \5C\0ABPF_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries); \5C\0A__attribute__((section(\22maps/export\22))) \5C\0Astruct _name##_table_t __##_name\0A\0A// define a table that is shared accross the programs in the same namespace\0A#define BPF_TABLE_SHARED(_table_type, _key_type, _leaf_type, _name, _max_entries) \5C\0ABPF_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries); \5C\0A__attribute__((section(\22maps/shared\22))) \5C\0Astruct _name##_table_t __##_name\0A\0A// Identifier for current CPU used in perf_submit and perf_read\0A// Prefer BPF_F_CURRENT_CPU flag, falls back to call helper for older kernel\0A// Can be overridden from BCC\0A#ifndef CUR_CPU_IDENTIFIER\0A#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)\0A#define CUR_CPU_IDENTIFIER BPF_F_CURRENT_CPU\0A#else\0A#define CUR_CPU_IDENTIFIER bpf_get_smp_processor_id()\0A#endif\0A#endif\0A\0A// Table for pushing custom events to userspace via ring buffer\0A#define BPF_PERF_OUTPUT(_name) \5C\0Astruct _name##_table_t { \5C\0A  int key; \5C\0A  u32 leaf; \5C\0A  /* map.perf_submit(ctx, data, data_size) */ \5C\0A  int (*perf_submit) (void *, void *, u32); \5C\0A  int (*perf_submit_skb) (void *, u32, void *, u32); \5C\0A  u32 max_entries; \5C\0A}; \5C\0A__attribute__((section(\22maps/perf_output\22))) \5C\0Astruct _name##_table_t _name = { .max_entries = 0 }\0A\0A// Table for reading hw perf cpu counters\0A#define BPF_PERF_ARRAY(_name, _max_entries) \5C\0Astruct _name##_table_t { \5C\0A  int key; \5C\0A  u32 leaf; \5C\0A  /* counter = map.perf_read(index) */ \5C\0A  u64 (*perf_read) (int); \5C\0A  int (*perf_counter_value) (int, void *, u32); \5C\0A  u32 max_entries; \5C\0A}; \5C\0A__attribute__((section(\22maps/perf_array\22))) \5C\0Astruct _name##_table_t _name = { .max_entries = (_max_entries) }\0A\0A// Table for cgroup file descriptors\0A#define BPF_CGROUP_ARRAY(_name, _max_entries) \5C\0Astruct _name##_table_t { \5C\0A  int key; \5C\0A  u32 leaf; \5C\0A  int (*check_current_task) (int); \5C\0A  u32 max_entries; \5C\0A}; \5C\0A__attribute__((section(\22maps/cgroup_array\22))) \5C\0Astruct _name##_table_t _name = { .max_entries = (_max_entries) }\0A\0A#define BPF_HASH1(_name) \5C\0A  BPF_TABLE(\22hash\22, u64, u64, _name, 10240)\0A#define BPF_HASH2(_name, _key_type) \5C\0A  BPF_TABLE(\22hash\22, _key_type, u64, _name, 10240)\0A#define BPF_HASH3(_name, _key_type, _leaf_type) \5C\0A  BPF_TABLE(\22hash\22, _key_type, _leaf_type, _name, 10240)\0A#define BPF_HASH4(_name, _key_type, _leaf_type, _size) \5C\0A  BPF_TABLE(\22hash\22, _key_type, _leaf_type, _name, _size)\0A\0A// helper for default-variable macro function\0A#define BPF_HASHX(_1, _2, _3, _4, NAME, ...) NAME\0A\0A// Define a hash function, some arguments optional\0A// BPF_HASH(name, key_type=u64, leaf_type=u64, size=10240)\0A#define BPF_HASH(...) \5C\0A  BPF_HASHX(__VA_ARGS__, BPF_HASH4, BPF_HASH3, BPF_HASH2, BPF_HASH1)(__VA_ARGS__)\0A\0A#define BPF_ARRAY1(_name) \5C\0A  BPF_TABLE(\22array\22, int, u64, _name, 10240)\0A#define BPF_ARRAY2(_name, _leaf_type) \5C\0A  BPF_TABLE(\22array\22, int, _leaf_type, _name, 10240)\0A#define BPF_ARRAY3(_name, _leaf_type, _size) \5C\0A  BPF_TABLE(\22array\22, int, _leaf_type, _name, _size)\0A\0A// helper for default-variable macro function\0A#define BPF_ARRAYX(_1, _2, _3, NAME, ...) NAME\0A\0A// Define an array function, some arguments optional\0A// BPF_ARRAY(name, leaf_type=u64, size=10240)\0A#define BPF_ARRAY(...) \5C\0A  BPF_ARRAYX(__VA_ARGS__, BPF_ARRAY3, BPF_ARRAY2, BPF_ARRAY1)(__VA_ARGS__)\0A\0A#define BPF_PERCPU_ARRAY1(_name)                        \5C\0A    BPF_TABLE(\22percpu_array\22, int, u64, _name, 10240)\0A#define BPF_PERCPU_ARRAY2(_name, _leaf_type) \5C\0A    BPF_TABLE(\22percpu_array\22, int, _leaf_type, _name, 10240)\0A#define BPF_PERCPU_ARRAY3(_name, _leaf_type, _size) \5C\0A    BPF_TABLE(\22percpu_array\22, int, _leaf_type, _name, _size)\0A\0A// helper for default-variable macro function\0A#define BPF_PERCPU_ARRAYX(_1, _2, _3, NAME, ...) NAME\0A\0A// Define an array function (per CPU), some arguments optional\0A// BPF_PERCPU_ARRAY(name, leaf_type=u64, size=10240)\0A#define BPF_PERCPU_ARRAY(...)                                           \5C\0A  BPF_PERCPU_ARRAYX(                                                    \5C\0A    __VA_ARGS__, BPF_PERCPU_ARRAY3, BPF_PERCPU_ARRAY2, BPF_PERCPU_ARRAY1) \5C\0A           (__VA_ARGS__)\0A\0A#define BPF_HIST1(_name) \5C\0A  BPF_TABLE(\22histogram\22, int, u64, _name, 64)\0A#define BPF_HIST2(_name, _key_type) \5C\0A  BPF_TABLE(\22histogram\22, _key_type, u64, _name, 64)\0A#define BPF_HIST3(_name, _key_type, _size) \5C\0A  BPF_TABLE(\22histogram\22, _key_type, u64, _name, _size)\0A#define BPF_HISTX(_1, _2, _3, NAME, ...) NAME\0A\0A// Define a histogram, some arguments optional\0A// BPF_HISTOGRAM(name, key_type=int, size=64)\0A#define BPF_HISTOGRAM(...) \5C\0A  BPF_HISTX(__VA_ARGS__, BPF_HIST3, BPF_HIST2, BPF_HIST1)(__VA_ARGS__)\0A\0A#define BPF_LPM_TRIE1(_name) \5C\0A  BPF_F_TABLE(\22lpm_trie\22, u64, u64, _name, 10240, BPF_F_NO_PREALLOC)\0A#define BPF_LPM_TRIE2(_name, _key_type) \5C\0A  BPF_F_TABLE(\22lpm_trie\22, _key_type, u64, _name, 10240, BPF_F_NO_PREALLOC)\0A#define BPF_LPM_TRIE3(_name, _key_type, _leaf_type) \5C\0A  BPF_F_TABLE(\22lpm_trie\22, _key_type, _leaf_type, _name, 10240, BPF_F_NO_PREALLOC)\0A#define BPF_LPM_TRIE4(_name, _key_type, _leaf_type, _size) \5C\0A  BPF_F_TABLE(\22lpm_trie\22, _key_type, _leaf_type, _name, _size, BPF_F_NO_PREALLOC)\0A#define BPF_LPM_TRIEX(_1, _2, _3, _4, NAME, ...) NAME\0A\0A// Define a LPM trie function, some arguments optional\0A// BPF_LPM_TRIE(name, key_type=u64, leaf_type=u64, size=10240)\0A#define BPF_LPM_TRIE(...) \5C\0A  BPF_LPM_TRIEX(__VA_ARGS__, BPF_LPM_TRIE4, BPF_LPM_TRIE3, BPF_LPM_TRIE2, BPF_LPM_TRIE1)(__VA_ARGS__)\0A\0Astruct bpf_stacktrace {\0A  u64 ip[BPF_MAX_STACK_DEPTH];\0A};\0A\0A#define BPF_STACK_TRACE(_name, _max_entries) \5C\0A  BPF_TABLE(\22stacktrace\22, int, struct bpf_stacktrace, _name, roundup_pow_of_two(_max_entries))\0A\0A#define BPF_PROG_ARRAY(_name, _max_entries) \5C\0A  BPF_TABLE(\22prog\22, u32, u32, _name, _max_entries)\0A\0A#define BPF_XDP_REDIRECT_MAP(_table_type, _leaf_type, _name, _max_entries) \5C\0Astruct _name##_table_t { \5C\0A  u32 key; \5C\0A  _leaf_type leaf; \5C\0A  /* xdp_act = map.redirect_map(index, flag) */ \5C\0A  u64 (*redirect_map) (int, int); \5C\0A  u32 max_entries; \5C\0A}; \5C\0A__attribute__((section(\22maps/\22_table_type))) \5C\0Astruct _name##_table_t _name = { .max_entries = (_max_entries) }\0A\0A#define BPF_DEVMAP(_name, _max_entries) \5C\0A  BPF_XDP_REDIRECT_MAP(\22devmap\22, int, _name, _max_entries)\0A\0A#define BPF_CPUMAP(_name, _max_entries) \5C\0A  BPF_XDP_REDIRECT_MAP(\22cpumap\22, u32, _name, _max_entries)\0A\0A// packet parsing state machine helpers\0A#define cursor_advance(_cursor, _len) \5C\0A  ({ void *_tmp = _cursor; _cursor += _len; _tmp; })\0A\0A#ifdef LINUX_VERSION_CODE_OVERRIDE\0Aunsigned _version SEC(\22version\22) = LINUX_VERSION_CODE_OVERRIDE;\0A#else\0Aunsigned _version SEC(\22version\22) = LINUX_VERSION_CODE;\0A#endif\0A\0A/* helper functions called from eBPF programs written in C */\0Astatic void *(*bpf_map_lookup_elem)(void *map, void *key) =\0A  (void *) BPF_FUNC_map_lookup_elem;\0Astatic int (*bpf_map_update_elem)(void *map, void *key, void *value, u64 flags) =\0A  (void *) BPF_FUNC_map_update_elem;\0Astatic int (*bpf_map_delete_elem)(void *map, void *key) =\0A  (void *) BPF_FUNC_map_delete_elem;\0Astatic int (*bpf_probe_read)(void *dst, u64 size, const void *unsafe_ptr) =\0A  (void *) BPF_FUNC_probe_read;\0Astatic u64 (*bpf_ktime_get_ns)(void) =\0A  (void *) BPF_FUNC_ktime_get_ns;\0Astatic u32 (*bpf_get_prandom_u32)(void) =\0A  (void *) BPF_FUNC_get_prandom_u32;\0Astatic int (*bpf_trace_printk_)(const char *fmt, u64 fmt_size, ...) =\0A  (void *) BPF_FUNC_trace_printk;\0Astatic int (*bpf_probe_read_str)(void *dst, u64 size, const void *unsafe_ptr) =\0A  (void *) BPF_FUNC_probe_read_str;\0Aint bpf_trace_printk(const char *fmt, ...) asm(\22llvm.bpf.extra\22);\0Astatic inline __attribute__((always_inline))\0Avoid bpf_tail_call_(u64 map_fd, void *ctx, int index) {\0A  ((void (*)(void *, u64, int))BPF_FUNC_tail_call)(ctx, map_fd, index);\0A}\0Astatic int (*bpf_clone_redirect)(void *ctx, int ifindex, u32 flags) =\0A  (void *) BPF_FUNC_clone_redirect;\0Astatic u64 (*bpf_get_smp_processor_id)(void) =\0A  (void *) BPF_FUNC_get_smp_processor_id;\0Astatic u64 (*bpf_get_current_pid_tgid)(void) =\0A  (void *) BPF_FUNC_get_current_pid_tgid;\0Astatic u64 (*bpf_get_current_uid_gid)(void) =\0A  (void *) BPF_FUNC_get_current_uid_gid;\0Astatic int (*bpf_get_current_comm)(void *buf, int buf_size) =\0A  (void *) BPF_FUNC_get_current_comm;\0Astatic u64 (*bpf_get_cgroup_classid)(void *ctx) =\0A  (void *) BPF_FUNC_get_cgroup_classid;\0Astatic u64 (*bpf_skb_vlan_push)(void *ctx, u16 proto, u16 vlan_tci) =\0A  (void *) BPF_FUNC_skb_vlan_push;\0Astatic u64 (*bpf_skb_vlan_pop)(void *ctx) =\0A  (void *) BPF_FUNC_skb_vlan_pop;\0Astatic int (*bpf_skb_get_tunnel_key)(void *ctx, void *to, u32 size, u64 flags) =\0A  (void *) BPF_FUNC_skb_get_tunnel_key;\0Astatic int (*bpf_skb_set_tunnel_key)(void *ctx, void *from, u32 size, u64 flags) =\0A  (void *) BPF_FUNC_skb_set_tunnel_key;\0Astatic u64 (*bpf_perf_event_read)(void *map, u64 flags) =\0A  (void *) BPF_FUNC_perf_event_read;\0Astatic int (*bpf_redirect)(int ifindex, u32 flags) =\0A  (void *) BPF_FUNC_redirect;\0Astatic u32 (*bpf_get_route_realm)(void *ctx) =\0A  (void *) BPF_FUNC_get_route_realm;\0Astatic int (*bpf_perf_event_output)(void *ctx, void *map, u64 index, void *data, u32 size) =\0A  (void *) BPF_FUNC_perf_event_output;\0Astatic int (*bpf_skb_load_bytes)(void *ctx, int offset, void *to, u32 len) =\0A  (void *) BPF_FUNC_skb_load_bytes;\0Astatic int (*bpf_perf_event_read_value)(void *map, u64 flags, void *buf, u32 buf_size) =\0A  (void *) BPF_FUNC_perf_event_read_value;\0Astatic int (*bpf_perf_prog_read_value)(void *ctx, void *buf, u32 buf_size) =\0A  (void *) BPF_FUNC_perf_prog_read_value;\0Astatic int (*bpf_current_task_under_cgroup)(void *map, int index) =\0A  (void *) BPF_FUNC_current_task_under_cgroup;\0Astatic u32 (*bpf_get_socket_cookie)(void *ctx) =\0A  (void *) BPF_FUNC_get_socket_cookie;\0Astatic u64 (*bpf_get_socket_uid)(void *ctx) =\0A  (void *) BPF_FUNC_get_socket_uid;\0Astatic int (*bpf_getsockopt)(void *ctx, int level, int optname, void *optval, int optlen) =\0A  (void *) BPF_FUNC_getsockopt;\0Astatic int (*bpf_redirect_map)(void *map, int key, int flags) =\0A  (void *) BPF_FUNC_redirect_map;\0Astatic int (*bpf_set_hash)(void *ctx, u32 hash) =\0A  (void *) BPF_FUNC_set_hash;\0Astatic int (*bpf_setsockopt)(void *ctx, int level, int optname, void *optval, int optlen) =\0A  (void *) BPF_FUNC_setsockopt;\0Astatic int (*bpf_skb_adjust_room)(void *ctx, int len_diff, u32 mode, u64 flags) =\0A  (void *) BPF_FUNC_skb_adjust_room;\0Astatic int (*bpf_skb_under_cgroup)(void *ctx, void *map, int index) =\0A  (void *) BPF_FUNC_skb_under_cgroup;\0Astatic int (*bpf_sk_redirect_map)(void *ctx, void *map, int key, int flags) =\0A  (void *) BPF_FUNC_sk_redirect_map;\0Astatic int (*bpf_sock_map_update)(void *map, void *key, void *value, unsigned long long flags) =\0A  (void *) BPF_FUNC_sock_map_update;\0Astatic int (*bpf_xdp_adjust_meta)(void *ctx, int offset) =\0A  (void *) BPF_FUNC_xdp_adjust_meta;\0A\0A/* bcc_get_stackid will return a negative value in the case of an error\0A *\0A * BPF_STACK_TRACE(_name, _size) will allocate space for _size stack traces.\0A *  -ENOMEM will be returned when this limit is reached.\0A *\0A * -EFAULT is typically returned when requesting user-space stack straces (using\0A * BPF_F_USER_STACK) for kernel threads. However, a valid stackid may be\0A * returned in some cases; consider a tracepoint or kprobe executing in the\0A * kernel context. Given this you can typically ignore -EFAULT errors when\0A * retrieving user-space stack traces.\0A */\0Astatic int (*bcc_get_stackid_)(void *ctx, void *map, u64 flags) =\0A  (void *) BPF_FUNC_get_stackid;\0Astatic inline __attribute__((always_inline))\0Aint bcc_get_stackid(uintptr_t map, void *ctx, u64 flags) {\0A  return bcc_get_stackid_(ctx, (void *)map, flags);\0A}\0A\0Astatic int (*bpf_csum_diff)(void *from, u64 from_size, void *to, u64 to_size, u64 seed) =\0A  (void *) BPF_FUNC_csum_diff;\0Astatic int (*bpf_skb_get_tunnel_opt)(void *ctx, void *md, u32 size) =\0A  (void *) BPF_FUNC_skb_get_tunnel_opt;\0Astatic int (*bpf_skb_set_tunnel_opt)(void *ctx, void *md, u32 size) =\0A  (void *) BPF_FUNC_skb_set_tunnel_opt;\0Astatic int (*bpf_skb_change_proto)(void *ctx, u16 proto, u64 flags) =\0A  (void *) BPF_FUNC_skb_change_proto;\0Astatic int (*bpf_skb_change_type)(void *ctx, u32 type) =\0A  (void *) BPF_FUNC_skb_change_type;\0Astatic u32 (*bpf_get_hash_recalc)(void *ctx) =\0A  (void *) BPF_FUNC_get_hash_recalc;\0Astatic u64 (*bpf_get_current_task)(void) =\0A  (void *) BPF_FUNC_get_current_task;\0Astatic int (*bpf_probe_write_user)(void *dst, void *src, u32 size) =\0A  (void *) BPF_FUNC_probe_write_user;\0Astatic int (*bpf_skb_change_tail)(void *ctx, u32 new_len, u64 flags) =\0A  (void *) BPF_FUNC_skb_change_tail;\0Astatic int (*bpf_skb_pull_data)(void *ctx, u32 len) =\0A  (void *) BPF_FUNC_skb_pull_data;\0Astatic int (*bpf_csum_update)(void *ctx, u16 csum) =\0A  (void *) BPF_FUNC_csum_update;\0Astatic int (*bpf_set_hash_invalid)(void *ctx) =\0A  (void *) BPF_FUNC_set_hash_invalid;\0Astatic int (*bpf_get_numa_node_id)(void) =\0A  (void *) BPF_FUNC_get_numa_node_id;\0Astatic int (*bpf_skb_change_head)(void *ctx, u32 len, u64 flags) =\0A  (void *) BPF_FUNC_skb_change_head;\0Astatic int (*bpf_xdp_adjust_head)(void *ctx, int offset) =\0A  (void *) BPF_FUNC_xdp_adjust_head;\0Astatic int (*bpf_override_return)(void *pt_regs, unsigned long rc) =\0A  (void *) BPF_FUNC_override_return;\0Astatic int (*bpf_sock_ops_cb_flags_set)(void *skops, int flags) =\0A  (void *) BPF_FUNC_sock_ops_cb_flags_set;\0Astatic int (*bpf_msg_redirect_map)(void *msg, void *map, u32 key, u64 flags) =\0A  (void *) BPF_FUNC_msg_redirect_map;\0Astatic int (*bpf_msg_apply_bytes)(void *msg, u32 bytes) =\0A  (void *) BPF_FUNC_msg_apply_bytes;\0Astatic int (*bpf_msg_cork_bytes)(void *msg, u32 bytes) =\0A  (void *) BPF_FUNC_msg_cork_bytes;\0Astatic int (*bpf_msg_pull_data)(void *msg, u32 start, u32 end, u64 flags) =\0A  (void *) BPF_FUNC_msg_pull_data;\0Astatic int (*bpf_bind)(void *ctx, void *addr, int addr_len) =\0A  (void *) BPF_FUNC_bind;\0Astatic int (*bpf_xdp_adjust_tail)(void *ctx, int offset) =\0A  (void *) BPF_FUNC_xdp_adjust_tail;\0Astatic int (*bpf_skb_get_xfrm_state)(void *ctx, u32 index, void *xfrm_state, u32 size, u64 flags) =\0A  (void *) BPF_FUNC_skb_get_xfrm_state;\0Astatic int (*bpf_get_stack)(void *ctx, void *buf, u32 size, u64 flags) =\0A  (void *) BPF_FUNC_get_stack;\0Astatic int (*bpf_skb_load_bytes_relative)(void *ctx, u32 offset, void *to, u32 len, u32 start_header) =\0A  (void *) BPF_FUNC_skb_load_bytes_relative;\0Astatic int (*bpf_fib_lookup)(void *ctx, void *params, int plen, u32 flags) =\0A  (void *) BPF_FUNC_fib_lookup;\0Astatic int (*bpf_sock_hash_update)(void *ctx, void *map, void *key, u64 flags) =\0A  (void *) BPF_FUNC_sock_hash_update;\0Astatic int (*bpf_msg_redirect_hash)(void *ctx, void *map, void *key, u64 flags) =\0A  (void *) BPF_FUNC_msg_redirect_hash;\0Astatic int (*bpf_sk_redirect_hash)(void *ctx, void *map, void *key, u64 flags) =\0A  (void *) BPF_FUNC_sk_redirect_hash;\0Astatic int (*bpf_lwt_push_encap)(void *skb, u32 type, void *hdr, u32 len) =\0A  (void *) BPF_FUNC_lwt_push_encap;\0Astatic int (*bpf_lwt_seg6_store_bytes)(void *ctx, u32 offset, const void *from, u32 len) =\0A  (void *) BPF_FUNC_lwt_seg6_store_bytes;\0Astatic int (*bpf_lwt_seg6_adjust_srh)(void *ctx, u32 offset, s32 delta) =\0A  (void *) BPF_FUNC_lwt_seg6_adjust_srh;\0Astatic int (*bpf_lwt_seg6_action)(void *ctx, u32 action, void *param, u32 param_len) =\0A  (void *) BPF_FUNC_lwt_seg6_action;\0Astatic int (*bpf_rc_keydown)(void *ctx, u32 protocol, u64 scancode, u32 toggle) =\0A  (void *) BPF_FUNC_rc_keydown;\0Astatic int (*bpf_rc_repeat)(void *ctx) =\0A  (void *) BPF_FUNC_rc_repeat;\0Astatic u64 (*bpf_skb_cgroup_id)(void *skb) =\0A  (void *) BPF_FUNC_skb_cgroup_id;\0Astatic u64 (*bpf_get_current_cgroup_id)(void) =\0A  (void *) BPF_FUNC_get_current_cgroup_id;\0Astatic u64 (*bpf_skb_ancestor_cgroup_id)(void *skb, int ancestor_level) =\0A  (void *) BPF_FUNC_skb_ancestor_cgroup_id;\0Astatic void * (*bpf_get_local_storage)(void *map, u64 flags) =\0A  (void *) BPF_FUNC_get_local_storage;\0Astatic int (*bpf_sk_select_reuseport)(void *reuse, void *map, void *key, u64 flags) =\0A  (void *) BPF_FUNC_sk_select_reuseport;\0A\0A/* llvm builtin functions that eBPF C program may use to\0A * emit BPF_LD_ABS and BPF_LD_IND instructions\0A */\0Astruct sk_buff;\0Aunsigned long long load_byte(void *skb,\0A  unsigned long long off) asm(\22llvm.bpf.load.byte\22);\0Aunsigned long long load_half(void *skb,\0A  unsigned long long off) asm(\22llvm.bpf.load.half\22);\0Aunsigned long long load_word(void *skb,\0A  unsigned long long off) asm(\22llvm.bpf.load.word\22);\0A\0A/* a helper structure used by eBPF C program\0A * to describe map attributes to elf_bpf loader\0A */\0Astruct bpf_map_def {\0A  unsigned int type;\0A  unsigned int key_size;\0A  unsigned int value_size;\0A  unsigned int max_entries;\0A};\0A\0Astatic int (*bpf_skb_store_bytes)(void *ctx, unsigned long long off, void *from,\0A                                  unsigned long long len, unsigned long long flags) =\0A  (void *) BPF_FUNC_skb_store_bytes;\0Astatic int (*bpf_l3_csum_replace)(void *ctx, unsigned long long off, unsigned long long from,\0A                                  unsigned long long to, unsigned long long flags) =\0A  (void *) BPF_FUNC_l3_csum_replace;\0Astatic int (*bpf_l4_csum_replace)(void *ctx, unsigned long long off, unsigned long long from,\0A                                  unsigned long long to, unsigned long long flags) =\0A  (void *) BPF_FUNC_l4_csum_replace;\0A\0Astatic inline __attribute__((always_inline))\0Au16 bpf_ntohs(u16 val) {\0A  /* will be recognized by gcc into rotate insn and eventually rolw 8 */\0A  return (val << 8) | (val >> 8);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Au32 bpf_ntohl(u32 val) {\0A  /* gcc will use bswapsi2 insn */\0A  return __builtin_bswap32(val);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Au64 bpf_ntohll(u64 val) {\0A  /* gcc will use bswapdi2 insn */\0A  return __builtin_bswap64(val);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Aunsigned __int128 bpf_ntoh128(unsigned __int128 val) {\0A  return (((unsigned __int128)bpf_ntohll(val) << 64) | (u64)bpf_ntohll(val >> 64));\0A}\0A\0Astatic inline __attribute__((always_inline))\0Au16 bpf_htons(u16 val) {\0A  return bpf_ntohs(val);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Au32 bpf_htonl(u32 val) {\0A  return bpf_ntohl(val);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Au64 bpf_htonll(u64 val) {\0A  return bpf_ntohll(val);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Aunsigned __int128 bpf_hton128(unsigned __int128 val) {\0A  return bpf_ntoh128(val);\0A}\0A\0Astatic inline __attribute__((always_inline))\0Au64 load_dword(void *skb, u64 off) {\0A  return ((u64)load_word(skb, off) << 32) | load_word(skb, off + 4);\0A}\0A\0Avoid bpf_store_byte(void *skb, u64 off, u64 val) asm(\22llvm.bpf.store.byte\22);\0Avoid bpf_store_half(void *skb, u64 off, u64 val) asm(\22llvm.bpf.store.half\22);\0Avoid bpf_store_word(void *skb, u64 off, u64 val) asm(\22llvm.bpf.store.word\22);\0Au64 bpf_pseudo_fd(u64, u64) asm(\22llvm.bpf.pseudo\22);\0A\0Astatic inline void __attribute__((always_inline))\0Abpf_store_dword(void *skb, u64 off, u64 val) {\0A  bpf_store_word(skb, off, (u32)val);\0A  bpf_store_word(skb, off + 4, val >> 32);\0A}\0A\0A#define MASK(_n) ((_n) < 64 ? (1ull << (_n)) - 1 : ((u64)-1LL))\0A#define MASK128(_n) ((_n) < 128 ? ((unsigned __int128)1 << (_n)) - 1 : ((unsigned __int128)-1))\0A\0Astatic inline __attribute__((always_inline))\0Aunsigned int bpf_log2(unsigned int v)\0A{\0A  unsigned int r;\0A  unsigned int shift;\0A\0A  r = (v > 0xFFFF) << 4; v >>= r;\0A  shift = (v > 0xFF) << 3; v >>= shift; r |= shift;\0A  shift = (v > 0xF) << 2; v >>= shift; r |= shift;\0A  shift = (v > 0x3) << 1; v >>= shift; r |= shift;\0A  r |= (v >> 1);\0A  return r;\0A}\0A\0Astatic inline __attribute__((always_inline))\0Aunsigned int bpf_log2l(unsigned long v)\0A{\0A  unsigned int hi = v >> 32;\0A  if (hi)\0A    return bpf_log2(hi) + 32 + 1;\0A  else\0A    return bpf_log2(v) + 1;\0A}\0A\0Astruct bpf_context;\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Au64 bpf_dext_pkt(void *pkt, u64 off, u64 bofs, u64 bsz) {\0A  if (bofs == 0 && bsz == 8) {\0A    return load_byte(pkt, off);\0A  } else if (bofs + bsz <= 8) {\0A    return load_byte(pkt, off) >> (8 - (bofs + bsz))  &  MASK(bsz);\0A  } else if (bofs == 0 && bsz == 16) {\0A    return load_half(pkt, off);\0A  } else if (bofs + bsz <= 16) {\0A    return load_half(pkt, off) >> (16 - (bofs + bsz))  &  MASK(bsz);\0A  } else if (bofs == 0 && bsz == 32) {\0A    return load_word(pkt, off);\0A  } else if (bofs + bsz <= 32) {\0A    return load_word(pkt, off) >> (32 - (bofs + bsz))  &  MASK(bsz);\0A  } else if (bofs == 0 && bsz == 64) {\0A    return load_dword(pkt, off);\0A  } else if (bofs + bsz <= 64) {\0A    return load_dword(pkt, off) >> (64 - (bofs + bsz))  &  MASK(bsz);\0A  }\0A  return 0;\0A}\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Avoid bpf_dins_pkt(void *pkt, u64 off, u64 bofs, u64 bsz, u64 val) {\0A  // The load_xxx function does a bswap before returning the short/word/dword,\0A  // so the value in register will always be host endian. However, the bytes\0A  // written back need to be in network order.\0A  if (bofs == 0 && bsz == 8) {\0A    bpf_skb_store_bytes(pkt, off, &val, 1, 0);\0A  } else if (bofs + bsz <= 8) {\0A    u8 v = load_byte(pkt, off);\0A    v &= ~(MASK(bsz) << (8 - (bofs + bsz)));\0A    v |= ((val & MASK(bsz)) << (8 - (bofs + bsz)));\0A    bpf_skb_store_bytes(pkt, off, &v, 1, 0);\0A  } else if (bofs == 0 && bsz == 16) {\0A    u16 v = bpf_htons(val);\0A    bpf_skb_store_bytes(pkt, off, &v, 2, 0);\0A  } else if (bofs + bsz <= 16) {\0A    u16 v = load_half(pkt, off);\0A    v &= ~(MASK(bsz) << (16 - (bofs + bsz)));\0A    v |= ((val & MASK(bsz)) << (16 - (bofs + bsz)));\0A    v = bpf_htons(v);\0A    bpf_skb_store_bytes(pkt, off, &v, 2, 0);\0A  } else if (bofs == 0 && bsz == 32) {\0A    u32 v = bpf_htonl(val);\0A    bpf_skb_store_bytes(pkt, off, &v, 4, 0);\0A  } else if (bofs + bsz <= 32) {\0A    u32 v = load_word(pkt, off);\0A    v &= ~(MASK(bsz) << (32 - (bofs + bsz)));\0A    v |= ((val & MASK(bsz)) << (32 - (bofs + bsz)));\0A    v = bpf_htonl(v);\0A    bpf_skb_store_bytes(pkt, off, &v, 4, 0);\0A  } else if (bofs == 0 && bsz == 64) {\0A    u64 v = bpf_htonll(val);\0A    bpf_skb_store_bytes(pkt, off, &v, 8, 0);\0A  } else if (bofs + bsz <= 64) {\0A    u64 v = load_dword(pkt, off);\0A    v &= ~(MASK(bsz) << (64 - (bofs + bsz)));\0A    v |= ((val & MASK(bsz)) << (64 - (bofs + bsz)));\0A    v = bpf_htonll(v);\0A    bpf_skb_store_bytes(pkt, off, &v, 8, 0);\0A  }\0A}\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Avoid * bpf_map_lookup_elem_(uintptr_t map, void *key) {\0A  return bpf_map_lookup_elem((void *)map, key);\0A}\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Aint bpf_map_update_elem_(uintptr_t map, void *key, void *value, u64 flags) {\0A  return bpf_map_update_elem((void *)map, key, value, flags);\0A}\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Aint bpf_map_delete_elem_(uintptr_t map, void *key) {\0A  return bpf_map_delete_elem((void *)map, key);\0A}\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Aint bpf_l3_csum_replace_(void *ctx, u64 off, u64 from, u64 to, u64 flags) {\0A  switch (flags & 0xf) {\0A    case 2:\0A      return bpf_l3_csum_replace(ctx, off, bpf_htons(from), bpf_htons(to), flags);\0A    case 4:\0A      return bpf_l3_csum_replace(ctx, off, bpf_htonl(from), bpf_htonl(to), flags);\0A    case 8:\0A      return bpf_l3_csum_replace(ctx, off, bpf_htonll(from), bpf_htonll(to), flags);\0A    default:\0A      {}\0A  }\0A  return bpf_l3_csum_replace(ctx, off, from, to, flags);\0A}\0A\0Astatic inline __attribute__((always_inline))\0ASEC(\22helpers\22)\0Aint bpf_l4_csum_replace_(void *ctx, u64 off, u64 from, u64 to, u64 flags) {\0A  switch (flags & 0xf) {\0A    case 2:\0A      return bpf_l4_csum_replace(ctx, off, bpf_htons(from), bpf_htons(to), flags);\0A    case 4:\0A      return bpf_l4_csum_replace(ctx, off, bpf_htonl(from), bpf_htonl(to), flags);\0A    case 8:\0A      return bpf_l4_csum_replace(ctx, off, bpf_htonll(from), bpf_htonll(to), flags);\0A    default:\0A      {}\0A  }\0A  return bpf_l4_csum_replace(ctx, off, from, to, flags);\0A}\0A\0Aint incr_cksum_l3(void *off, u64 oldval, u64 newval) asm(\22llvm.bpf.extra\22);\0Aint incr_cksum_l4(void *off, u64 oldval, u64 newval, u64 flags) asm(\22llvm.bpf.extra\22);\0Aint bpf_num_cpus() asm(\22llvm.bpf.extra\22);\0A\0Astruct pt_regs;\0Aint bpf_usdt_readarg(int argc, struct pt_regs *ctx, void *arg) asm(\22llvm.bpf.extra\22);\0Aint bpf_usdt_readarg_p(int argc, struct pt_regs *ctx, void *buf, u64 len) asm(\22llvm.bpf.extra\22);\0A\0A/* Scan the ARCH passed in from ARCH env variable (see kbuild_helper.cc) */\0A#if defined(__TARGET_ARCH_x86)\0A#define bpf_target_x86\0A#define bpf_target_defined\0A#elif defined(__TARGET_ARCH_s930x)\0A#define bpf_target_s930x\0A#define bpf_target_defined\0A#elif defined(__TARGET_ARCH_arm64)\0A#define bpf_target_arm64\0A#define bpf_target_defined\0A#elif defined(__TARGET_ARCH_powerpc)\0A#define bpf_target_powerpc\0A#define bpf_target_defined\0A#else\0A#undef bpf_target_defined\0A#endif\0A\0A/* Fall back to what the compiler says */\0A#ifndef bpf_target_defined\0A#if defined(__x86_64__)\0A#define bpf_target_x86\0A#elif defined(__s390x__)\0A#define bpf_target_s930x\0A#elif defined(__aarch64__)\0A#define bpf_target_arm64\0A#elif defined(__powerpc__)\0A#define bpf_target_powerpc\0A#endif\0A#endif\0A\0A#if defined(bpf_target_powerpc)\0A#define PT_REGS_PARM1(ctx)\09((ctx)->gpr[3])\0A#define PT_REGS_PARM2(ctx)\09((ctx)->gpr[4])\0A#define PT_REGS_PARM3(ctx)\09((ctx)->gpr[5])\0A#define PT_REGS_PARM4(ctx)\09((ctx)->gpr[6])\0A#define PT_REGS_PARM5(ctx)\09((ctx)->gpr[7])\0A#define PT_REGS_PARM6(ctx)\09((ctx)->gpr[8])\0A#define PT_REGS_RC(ctx)\09\09((ctx)->gpr[3])\0A#define PT_REGS_IP(ctx)\09\09((ctx)->nip)\0A#define PT_REGS_SP(ctx)\09\09((ctx)->gpr[1])\0A#elif defined(bpf_target_s930x)\0A#define PT_REGS_PARM1(x) ((x)->gprs[2])\0A#define PT_REGS_PARM2(x) ((x)->gprs[3])\0A#define PT_REGS_PARM3(x) ((x)->gprs[4])\0A#define PT_REGS_PARM4(x) ((x)->gprs[5])\0A#define PT_REGS_PARM5(x) ((x)->gprs[6])\0A#define PT_REGS_RET(x) ((x)->gprs[14])\0A#define PT_REGS_FP(x) ((x)->gprs[11]) /* Works only with CONFIG_FRAME_POINTER */\0A#define PT_REGS_RC(x) ((x)->gprs[2])\0A#define PT_REGS_SP(x) ((x)->gprs[15])\0A#define PT_REGS_IP(x) ((x)->psw.addr)\0A#elif defined(bpf_target_x86)\0A#define PT_REGS_PARM1(ctx)\09((ctx)->di)\0A#define PT_REGS_PARM2(ctx)\09((ctx)->si)\0A#define PT_REGS_PARM3(ctx)\09((ctx)->dx)\0A#define PT_REGS_PARM4(ctx)\09((ctx)->cx)\0A#define PT_REGS_PARM5(ctx)\09((ctx)->r8)\0A#define PT_REGS_PARM6(ctx)\09((ctx)->r9)\0A#define PT_REGS_FP(ctx)         ((ctx)->bp) /* Works only with CONFIG_FRAME_POINTER */\0A#define PT_REGS_RC(ctx)\09\09((ctx)->ax)\0A#define PT_REGS_IP(ctx)\09\09((ctx)->ip)\0A#define PT_REGS_SP(ctx)\09\09((ctx)->sp)\0A#elif defined(bpf_target_arm64)\0A#define PT_REGS_PARM1(x)\09((x)->regs[0])\0A#define PT_REGS_PARM2(x)\09((x)->regs[1])\0A#define PT_REGS_PARM3(x)\09((x)->regs[2])\0A#define PT_REGS_PARM4(x)\09((x)->regs[3])\0A#define PT_REGS_PARM5(x)\09((x)->regs[4])\0A#define PT_REGS_PARM6(x)\09((x)->regs[5])\0A#define PT_REGS_RET(x)\09\09((x)->regs[30])\0A#define PT_REGS_FP(x)\09\09((x)->regs[29]) /*  Works only with CONFIG_FRAME_POINTER */\0A#define PT_REGS_RC(x)\09\09((x)->regs[0])\0A#define PT_REGS_SP(x)\09\09((x)->sp)\0A#define PT_REGS_IP(x)\09\09((x)->pc)\0A#else\0A#error \22bcc does not support this platform yet\22\0A#endif\0A\0A#define lock_xadd(ptr, val) ((void)__sync_fetch_and_add(ptr, val))\0A\0A#define TRACEPOINT_PROBE(category, event) \5C\0Aint tracepoint__##category##__##event(struct tracepoint__##category##__##event *args)\0A\0A#define RAW_TRACEPOINT_PROBE(event) \5C\0Aint raw_tracepoint__##event(struct bpf_raw_tracepoint_args *ctx)\0A\0A#define TP_DATA_LOC_READ_CONST(dst, field, length)                        \5C\0A        do {                                                              \5C\0A            unsigned short __offset = args->data_loc_##field & 0xFFFF;    \5C\0A            bpf_probe_read((void *)dst, length, (char *)args + __offset); \5C\0A        } while (0);\0A\0A#define TP_DATA_LOC_READ(dst, field)                                        \5C\0A        do {                                                                \5C\0A            unsigned short __offset = args->data_loc_##field & 0xFFFF;      \5C\0A            unsigned short __length = args->data_loc_##field >> 16;         \5C\0A            bpf_probe_read((void *)dst, __length, (char *)args + __offset); \5C\0A        } while (0);\0A\0A#endif\0A")
!58 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !59, size: 64)
!59 = !DISubroutineType(types: !60)
!60 = !{!36, !28, !27, !28, !27, !27}
!61 = !DIGlobalVariableExpression(var: !62, expr: !DIExpression())
!62 = distinct !DIGlobalVariable(name: "bpf_map_lookup_elem", scope: !2, file: !57, line: 238, type: !63, isLocal: true, isDefinition: true)
!63 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !64, size: 64)
!64 = !DISubroutineType(types: !65)
!65 = !{!28, !28, !28}
!66 = !DIGlobalVariableExpression(var: !67, expr: !DIExpression())
!67 = distinct !DIGlobalVariable(name: "bpf_map_update_elem", scope: !2, file: !57, line: 240, type: !68, isLocal: true, isDefinition: true)
!68 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !69, size: 64)
!69 = !DISubroutineType(types: !70)
!70 = !{!36, !28, !28, !28, !25}
!71 = !DIGlobalVariableExpression(var: !72, expr: !DIExpression())
!72 = distinct !DIGlobalVariable(name: "bpf_map_delete_elem", scope: !2, file: !57, line: 242, type: !73, isLocal: true, isDefinition: true)
!73 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !74, size: 64)
!74 = !DISubroutineType(types: !75)
!75 = !{!36, !28, !28}
!76 = !DIGlobalVariableExpression(var: !77, expr: !DIExpression())
!77 = distinct !DIGlobalVariable(name: "bpf_l3_csum_replace", scope: !2, file: !57, line: 443, type: !78, isLocal: true, isDefinition: true)
!78 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !79, size: 64)
!79 = !DISubroutineType(types: !80)
!80 = !{!36, !28, !27, !27, !27, !27}
!81 = !DIGlobalVariableExpression(var: !82, expr: !DIExpression())
!82 = distinct !DIGlobalVariable(name: "bpf_l4_csum_replace", scope: !2, file: !57, line: 446, type: !78, isLocal: true, isDefinition: true)
!83 = !DIGlobalVariableExpression(var: !84, expr: !DIExpression())
!84 = distinct !DIGlobalVariable(name: "bpf_get_current_pid_tgid", scope: !2, file: !57, line: 263, type: !85, isLocal: true, isDefinition: true)
!85 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !86, size: 64)
!86 = !DISubroutineType(types: !87)
!87 = !{!25}
!88 = !DIGlobalVariableExpression(var: !89, expr: !DIExpression())
!89 = distinct !DIGlobalVariable(name: "bpf_get_current_comm", scope: !2, file: !57, line: 267, type: !90, isLocal: true, isDefinition: true)
!90 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !91, size: 64)
!91 = !DISubroutineType(types: !92)
!92 = !{!36, !28, !36}
!93 = !DIGlobalVariableExpression(var: !94, expr: !DIExpression())
!94 = distinct !DIGlobalVariable(name: "bpf_perf_event_output", scope: !2, file: !57, line: 285, type: !95, isLocal: true, isDefinition: true)
!95 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !96, size: 64)
!96 = !DISubroutineType(types: !97)
!97 = !{!36, !28, !28, !25, !28, !38}
!98 = !{i32 2, !"Dwarf Version", i32 5}
!99 = !{i32 2, !"Debug Info Version", i32 3}
!100 = !{i32 1, !"wchar_size", i32 4}
!101 = !{!"clang version 8.0.0 (https://github.com/yonghong-song/clang.git 6d4408ce50e9bacfce46e2e32eb4b2318e2e6992) (https://github.com/yonghong-song/llvm.git d043791034148b50b5479124613a622f2b17cb7e)"}
!102 = distinct !DISubprogram(name: "bpf_dext_pkt", scope: !57, file: !57, line: 540, type: !103, isLocal: true, isDefinition: true, scopeLine: 540, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !105)
!103 = !DISubroutineType(types: !104)
!104 = !{!25, !28, !25, !25, !25}
!105 = !{!106, !107, !108, !109}
!106 = !DILocalVariable(name: "pkt", arg: 1, scope: !102, file: !57, line: 540, type: !28)
!107 = !DILocalVariable(name: "off", arg: 2, scope: !102, file: !57, line: 540, type: !25)
!108 = !DILocalVariable(name: "bofs", arg: 3, scope: !102, file: !57, line: 540, type: !25)
!109 = !DILocalVariable(name: "bsz", arg: 4, scope: !102, file: !57, line: 540, type: !25)
!110 = !DILocation(line: 540, column: 24, scope: !102)
!111 = !DILocation(line: 540, column: 33, scope: !102)
!112 = !DILocation(line: 540, column: 42, scope: !102)
!113 = !DILocation(line: 540, column: 52, scope: !102)
!114 = !DILocation(line: 541, column: 12, scope: !115)
!115 = distinct !DILexicalBlock(scope: !102, file: !57, line: 541, column: 7)
!116 = !DILocation(line: 541, column: 24, scope: !115)
!117 = !DILocation(line: 541, column: 17, scope: !115)
!118 = !DILocation(line: 542, column: 12, scope: !119)
!119 = distinct !DILexicalBlock(scope: !115, file: !57, line: 541, column: 30)
!120 = !DILocation(line: 542, column: 5, scope: !119)
!121 = !DILocation(line: 543, column: 19, scope: !122)
!122 = distinct !DILexicalBlock(scope: !115, file: !57, line: 543, column: 14)
!123 = !DILocation(line: 543, column: 25, scope: !122)
!124 = !DILocation(line: 543, column: 14, scope: !115)
!125 = !DILocation(line: 544, column: 12, scope: !126)
!126 = distinct !DILexicalBlock(scope: !122, file: !57, line: 543, column: 31)
!127 = !DILocation(line: 544, column: 38, scope: !126)
!128 = !DILocation(line: 544, column: 32, scope: !126)
!129 = !DILocation(line: 544, column: 58, scope: !126)
!130 = !DILocation(line: 544, column: 55, scope: !126)
!131 = !DILocation(line: 544, column: 5, scope: !126)
!132 = !DILocation(line: 545, column: 31, scope: !133)
!133 = distinct !DILexicalBlock(scope: !122, file: !57, line: 545, column: 14)
!134 = !DILocation(line: 545, column: 24, scope: !133)
!135 = !DILocation(line: 546, column: 12, scope: !136)
!136 = distinct !DILexicalBlock(scope: !133, file: !57, line: 545, column: 38)
!137 = !DILocation(line: 546, column: 5, scope: !136)
!138 = !DILocation(line: 547, column: 25, scope: !139)
!139 = distinct !DILexicalBlock(scope: !133, file: !57, line: 547, column: 14)
!140 = !DILocation(line: 547, column: 14, scope: !133)
!141 = !DILocation(line: 548, column: 12, scope: !142)
!142 = distinct !DILexicalBlock(scope: !139, file: !57, line: 547, column: 32)
!143 = !DILocation(line: 548, column: 39, scope: !142)
!144 = !DILocation(line: 548, column: 32, scope: !142)
!145 = !DILocation(line: 548, column: 59, scope: !142)
!146 = !DILocation(line: 548, column: 56, scope: !142)
!147 = !DILocation(line: 548, column: 5, scope: !142)
!148 = !DILocation(line: 549, column: 31, scope: !149)
!149 = distinct !DILexicalBlock(scope: !139, file: !57, line: 549, column: 14)
!150 = !DILocation(line: 549, column: 24, scope: !149)
!151 = !DILocation(line: 550, column: 12, scope: !152)
!152 = distinct !DILexicalBlock(scope: !149, file: !57, line: 549, column: 38)
!153 = !DILocation(line: 550, column: 5, scope: !152)
!154 = !DILocation(line: 551, column: 25, scope: !155)
!155 = distinct !DILexicalBlock(scope: !149, file: !57, line: 551, column: 14)
!156 = !DILocation(line: 551, column: 14, scope: !149)
!157 = !DILocation(line: 552, column: 12, scope: !158)
!158 = distinct !DILexicalBlock(scope: !155, file: !57, line: 551, column: 32)
!159 = !DILocation(line: 552, column: 39, scope: !158)
!160 = !DILocation(line: 552, column: 32, scope: !158)
!161 = !DILocation(line: 552, column: 59, scope: !158)
!162 = !DILocation(line: 552, column: 56, scope: !158)
!163 = !DILocation(line: 552, column: 5, scope: !158)
!164 = !DILocation(line: 553, column: 31, scope: !165)
!165 = distinct !DILexicalBlock(scope: !155, file: !57, line: 553, column: 14)
!166 = !DILocation(line: 553, column: 24, scope: !165)
!167 = !DILocalVariable(name: "skb", arg: 1, scope: !168, file: !57, line: 494, type: !28)
!168 = distinct !DISubprogram(name: "load_dword", scope: !57, file: !57, line: 494, type: !169, isLocal: true, isDefinition: true, scopeLine: 494, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !171)
!169 = !DISubroutineType(types: !170)
!170 = !{!25, !28, !25}
!171 = !{!167, !172}
!172 = !DILocalVariable(name: "off", arg: 2, scope: !168, file: !57, line: 494, type: !25)
!173 = !DILocation(line: 494, column: 22, scope: !168, inlinedAt: !174)
!174 = distinct !DILocation(line: 554, column: 12, scope: !175)
!175 = distinct !DILexicalBlock(scope: !165, file: !57, line: 553, column: 38)
!176 = !DILocation(line: 494, column: 31, scope: !168, inlinedAt: !174)
!177 = !DILocation(line: 495, column: 16, scope: !168, inlinedAt: !174)
!178 = !DILocation(line: 495, column: 36, scope: !168, inlinedAt: !174)
!179 = !DILocation(line: 495, column: 64, scope: !168, inlinedAt: !174)
!180 = !DILocation(line: 495, column: 45, scope: !168, inlinedAt: !174)
!181 = !DILocation(line: 495, column: 43, scope: !168, inlinedAt: !174)
!182 = !DILocation(line: 554, column: 5, scope: !175)
!183 = !DILocation(line: 555, column: 25, scope: !184)
!184 = distinct !DILexicalBlock(scope: !165, file: !57, line: 555, column: 14)
!185 = !DILocation(line: 555, column: 14, scope: !165)
!186 = !DILocation(line: 494, column: 22, scope: !168, inlinedAt: !187)
!187 = distinct !DILocation(line: 556, column: 12, scope: !188)
!188 = distinct !DILexicalBlock(scope: !184, file: !57, line: 555, column: 32)
!189 = !DILocation(line: 494, column: 31, scope: !168, inlinedAt: !187)
!190 = !DILocation(line: 495, column: 16, scope: !168, inlinedAt: !187)
!191 = !DILocation(line: 495, column: 36, scope: !168, inlinedAt: !187)
!192 = !DILocation(line: 495, column: 64, scope: !168, inlinedAt: !187)
!193 = !DILocation(line: 495, column: 45, scope: !168, inlinedAt: !187)
!194 = !DILocation(line: 495, column: 43, scope: !168, inlinedAt: !187)
!195 = !DILocation(line: 556, column: 40, scope: !188)
!196 = !DILocation(line: 556, column: 33, scope: !188)
!197 = !DILocation(line: 556, column: 60, scope: !188)
!198 = !DILocation(line: 556, column: 57, scope: !188)
!199 = !DILocation(line: 556, column: 5, scope: !188)
!200 = !DILocation(line: 0, scope: !102)
!201 = !DILocation(line: 559, column: 1, scope: !102)
!202 = distinct !DISubprogram(name: "bpf_dins_pkt", scope: !57, file: !57, line: 563, type: !203, isLocal: true, isDefinition: true, scopeLine: 563, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !205)
!203 = !DISubroutineType(types: !204)
!204 = !{null, !28, !25, !25, !25, !25}
!205 = !{!206, !207, !208, !209, !210, !211, !217, !222, !225, !228, !231, !234}
!206 = !DILocalVariable(name: "pkt", arg: 1, scope: !202, file: !57, line: 563, type: !28)
!207 = !DILocalVariable(name: "off", arg: 2, scope: !202, file: !57, line: 563, type: !25)
!208 = !DILocalVariable(name: "bofs", arg: 3, scope: !202, file: !57, line: 563, type: !25)
!209 = !DILocalVariable(name: "bsz", arg: 4, scope: !202, file: !57, line: 563, type: !25)
!210 = !DILocalVariable(name: "val", arg: 5, scope: !202, file: !57, line: 563, type: !25)
!211 = !DILocalVariable(name: "v", scope: !212, file: !57, line: 570, type: !215)
!212 = distinct !DILexicalBlock(scope: !213, file: !57, line: 569, column: 31)
!213 = distinct !DILexicalBlock(scope: !214, file: !57, line: 569, column: 14)
!214 = distinct !DILexicalBlock(scope: !202, file: !57, line: 567, column: 7)
!215 = !DIDerivedType(tag: DW_TAG_typedef, name: "u8", file: !26, line: 16, baseType: !216)
!216 = !DIBasicType(name: "unsigned char", size: 8, encoding: DW_ATE_unsigned_char)
!217 = !DILocalVariable(name: "v", scope: !218, file: !57, line: 575, type: !220)
!218 = distinct !DILexicalBlock(scope: !219, file: !57, line: 574, column: 38)
!219 = distinct !DILexicalBlock(scope: !213, file: !57, line: 574, column: 14)
!220 = !DIDerivedType(tag: DW_TAG_typedef, name: "u16", file: !26, line: 19, baseType: !221)
!221 = !DIBasicType(name: "unsigned short", size: 16, encoding: DW_ATE_unsigned)
!222 = !DILocalVariable(name: "v", scope: !223, file: !57, line: 578, type: !220)
!223 = distinct !DILexicalBlock(scope: !224, file: !57, line: 577, column: 32)
!224 = distinct !DILexicalBlock(scope: !219, file: !57, line: 577, column: 14)
!225 = !DILocalVariable(name: "v", scope: !226, file: !57, line: 584, type: !38)
!226 = distinct !DILexicalBlock(scope: !227, file: !57, line: 583, column: 38)
!227 = distinct !DILexicalBlock(scope: !224, file: !57, line: 583, column: 14)
!228 = !DILocalVariable(name: "v", scope: !229, file: !57, line: 587, type: !38)
!229 = distinct !DILexicalBlock(scope: !230, file: !57, line: 586, column: 32)
!230 = distinct !DILexicalBlock(scope: !227, file: !57, line: 586, column: 14)
!231 = !DILocalVariable(name: "v", scope: !232, file: !57, line: 593, type: !25)
!232 = distinct !DILexicalBlock(scope: !233, file: !57, line: 592, column: 38)
!233 = distinct !DILexicalBlock(scope: !230, file: !57, line: 592, column: 14)
!234 = !DILocalVariable(name: "v", scope: !235, file: !57, line: 596, type: !25)
!235 = distinct !DILexicalBlock(scope: !236, file: !57, line: 595, column: 32)
!236 = distinct !DILexicalBlock(scope: !233, file: !57, line: 595, column: 14)
!237 = !DILocation(line: 563, column: 25, scope: !202)
!238 = !DILocation(line: 563, column: 34, scope: !202)
!239 = !DILocation(line: 563, column: 43, scope: !202)
!240 = !DILocation(line: 563, column: 53, scope: !202)
!241 = !DILocation(line: 563, column: 62, scope: !202)
!242 = !{!243, !243, i64 0}
!243 = !{!"long long", !244, i64 0}
!244 = !{!"omnipotent char", !245, i64 0}
!245 = !{!"Simple C/C++ TBAA"}
!246 = !DILocation(line: 567, column: 12, scope: !214)
!247 = !DILocation(line: 567, column: 24, scope: !214)
!248 = !DILocation(line: 567, column: 17, scope: !214)
!249 = !DILocation(line: 568, column: 35, scope: !250)
!250 = distinct !DILexicalBlock(scope: !214, file: !57, line: 567, column: 30)
!251 = !DILocation(line: 568, column: 5, scope: !250)
!252 = !DILocation(line: 569, column: 3, scope: !250)
!253 = !DILocation(line: 569, column: 19, scope: !213)
!254 = !DILocation(line: 569, column: 25, scope: !213)
!255 = !DILocation(line: 569, column: 14, scope: !214)
!256 = !DILocation(line: 570, column: 8, scope: !212)
!257 = !DILocation(line: 570, column: 5, scope: !212)
!258 = !DILocation(line: 570, column: 12, scope: !212)
!259 = !DILocation(line: 571, column: 12, scope: !212)
!260 = !DILocation(line: 571, column: 28, scope: !212)
!261 = !DILocation(line: 571, column: 22, scope: !212)
!262 = !DILocation(line: 571, column: 7, scope: !212)
!263 = !DILocation(line: 572, column: 16, scope: !212)
!264 = !DILocation(line: 572, column: 29, scope: !212)
!265 = !DILocation(line: 572, column: 7, scope: !212)
!266 = !{!244, !244, i64 0}
!267 = !DILocation(line: 573, column: 5, scope: !212)
!268 = !DILocation(line: 574, column: 3, scope: !213)
!269 = !DILocation(line: 574, column: 3, scope: !212)
!270 = !DILocation(line: 574, column: 31, scope: !219)
!271 = !DILocation(line: 574, column: 24, scope: !219)
!272 = !DILocation(line: 575, column: 5, scope: !218)
!273 = !DILocation(line: 575, column: 23, scope: !218)
!274 = !DILocation(line: 575, column: 9, scope: !218)
!275 = !{!276, !276, i64 0}
!276 = !{!"short", !244, i64 0}
!277 = !DILocation(line: 576, column: 5, scope: !218)
!278 = !DILocation(line: 577, column: 3, scope: !219)
!279 = !DILocation(line: 577, column: 3, scope: !218)
!280 = !DILocation(line: 577, column: 25, scope: !224)
!281 = !DILocation(line: 577, column: 14, scope: !219)
!282 = !DILocation(line: 578, column: 5, scope: !223)
!283 = !DILocation(line: 578, column: 13, scope: !223)
!284 = !DILocation(line: 578, column: 9, scope: !223)
!285 = !DILocation(line: 579, column: 12, scope: !223)
!286 = !DILocation(line: 579, column: 29, scope: !223)
!287 = !DILocation(line: 579, column: 22, scope: !223)
!288 = !DILocation(line: 579, column: 7, scope: !223)
!289 = !DILocation(line: 580, column: 16, scope: !223)
!290 = !DILocation(line: 580, column: 29, scope: !223)
!291 = !DILocation(line: 580, column: 7, scope: !223)
!292 = !DILocation(line: 581, column: 7, scope: !223)
!293 = !DILocation(line: 582, column: 5, scope: !223)
!294 = !DILocation(line: 583, column: 3, scope: !224)
!295 = !DILocation(line: 583, column: 3, scope: !223)
!296 = !DILocation(line: 583, column: 31, scope: !227)
!297 = !DILocation(line: 583, column: 24, scope: !227)
!298 = !DILocation(line: 584, column: 5, scope: !226)
!299 = !DILocation(line: 584, column: 23, scope: !226)
!300 = !DILocalVariable(name: "val", arg: 1, scope: !301, file: !57, line: 479, type: !38)
!301 = distinct !DISubprogram(name: "bpf_htonl", scope: !57, file: !57, line: 479, type: !302, isLocal: true, isDefinition: true, scopeLine: 479, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !304)
!302 = !DISubroutineType(types: !303)
!303 = !{!38, !38}
!304 = !{!300}
!305 = !DILocation(line: 479, column: 19, scope: !301, inlinedAt: !306)
!306 = distinct !DILocation(line: 584, column: 13, scope: !226)
!307 = !DILocalVariable(name: "val", arg: 1, scope: !308, file: !57, line: 457, type: !38)
!308 = distinct !DISubprogram(name: "bpf_ntohl", scope: !57, file: !57, line: 457, type: !302, isLocal: true, isDefinition: true, scopeLine: 457, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !309)
!309 = !{!307}
!310 = !DILocation(line: 457, column: 19, scope: !308, inlinedAt: !311)
!311 = distinct !DILocation(line: 480, column: 10, scope: !301, inlinedAt: !306)
!312 = !DILocation(line: 459, column: 10, scope: !308, inlinedAt: !311)
!313 = !DILocation(line: 584, column: 9, scope: !226)
!314 = !{!315, !315, i64 0}
!315 = !{!"int", !244, i64 0}
!316 = !DILocation(line: 585, column: 5, scope: !226)
!317 = !DILocation(line: 586, column: 3, scope: !227)
!318 = !DILocation(line: 586, column: 3, scope: !226)
!319 = !DILocation(line: 586, column: 25, scope: !230)
!320 = !DILocation(line: 586, column: 14, scope: !227)
!321 = !DILocation(line: 587, column: 5, scope: !229)
!322 = !DILocation(line: 587, column: 13, scope: !229)
!323 = !DILocation(line: 587, column: 9, scope: !229)
!324 = !DILocation(line: 588, column: 12, scope: !229)
!325 = !DILocation(line: 588, column: 29, scope: !229)
!326 = !DILocation(line: 588, column: 22, scope: !229)
!327 = !DILocation(line: 588, column: 7, scope: !229)
!328 = !DILocation(line: 589, column: 16, scope: !229)
!329 = !DILocation(line: 589, column: 29, scope: !229)
!330 = !DILocation(line: 589, column: 7, scope: !229)
!331 = !DILocation(line: 479, column: 19, scope: !301, inlinedAt: !332)
!332 = distinct !DILocation(line: 590, column: 9, scope: !229)
!333 = !DILocation(line: 457, column: 19, scope: !308, inlinedAt: !334)
!334 = distinct !DILocation(line: 480, column: 10, scope: !301, inlinedAt: !332)
!335 = !DILocation(line: 459, column: 10, scope: !308, inlinedAt: !334)
!336 = !DILocation(line: 590, column: 7, scope: !229)
!337 = !DILocation(line: 591, column: 5, scope: !229)
!338 = !DILocation(line: 592, column: 3, scope: !230)
!339 = !DILocation(line: 592, column: 3, scope: !229)
!340 = !DILocation(line: 592, column: 31, scope: !233)
!341 = !DILocation(line: 592, column: 24, scope: !233)
!342 = !DILocation(line: 593, column: 5, scope: !232)
!343 = !DILocalVariable(name: "val", arg: 1, scope: !344, file: !57, line: 484, type: !25)
!344 = distinct !DISubprogram(name: "bpf_htonll", scope: !57, file: !57, line: 484, type: !345, isLocal: true, isDefinition: true, scopeLine: 484, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !347)
!345 = !DISubroutineType(types: !346)
!346 = !{!25, !25}
!347 = !{!343}
!348 = !DILocation(line: 484, column: 20, scope: !344, inlinedAt: !349)
!349 = distinct !DILocation(line: 593, column: 13, scope: !232)
!350 = !DILocalVariable(name: "val", arg: 1, scope: !351, file: !57, line: 463, type: !25)
!351 = distinct !DISubprogram(name: "bpf_ntohll", scope: !57, file: !57, line: 463, type: !345, isLocal: true, isDefinition: true, scopeLine: 463, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !352)
!352 = !{!350}
!353 = !DILocation(line: 463, column: 20, scope: !351, inlinedAt: !354)
!354 = distinct !DILocation(line: 485, column: 10, scope: !344, inlinedAt: !349)
!355 = !DILocation(line: 465, column: 10, scope: !351, inlinedAt: !354)
!356 = !DILocation(line: 593, column: 9, scope: !232)
!357 = !DILocation(line: 594, column: 5, scope: !232)
!358 = !DILocation(line: 595, column: 3, scope: !233)
!359 = !DILocation(line: 595, column: 3, scope: !232)
!360 = !DILocation(line: 595, column: 25, scope: !236)
!361 = !DILocation(line: 595, column: 14, scope: !233)
!362 = !DILocation(line: 596, column: 5, scope: !235)
!363 = !DILocation(line: 494, column: 22, scope: !168, inlinedAt: !364)
!364 = distinct !DILocation(line: 596, column: 13, scope: !235)
!365 = !DILocation(line: 494, column: 31, scope: !168, inlinedAt: !364)
!366 = !DILocation(line: 495, column: 16, scope: !168, inlinedAt: !364)
!367 = !DILocation(line: 495, column: 36, scope: !168, inlinedAt: !364)
!368 = !DILocation(line: 495, column: 64, scope: !168, inlinedAt: !364)
!369 = !DILocation(line: 495, column: 45, scope: !168, inlinedAt: !364)
!370 = !DILocation(line: 495, column: 43, scope: !168, inlinedAt: !364)
!371 = !DILocation(line: 596, column: 9, scope: !235)
!372 = !DILocation(line: 597, column: 12, scope: !235)
!373 = !DILocation(line: 597, column: 29, scope: !235)
!374 = !DILocation(line: 597, column: 22, scope: !235)
!375 = !DILocation(line: 597, column: 10, scope: !235)
!376 = !DILocation(line: 597, column: 7, scope: !235)
!377 = !DILocation(line: 598, column: 16, scope: !235)
!378 = !DILocation(line: 598, column: 29, scope: !235)
!379 = !DILocation(line: 598, column: 7, scope: !235)
!380 = !DILocation(line: 484, column: 20, scope: !344, inlinedAt: !381)
!381 = distinct !DILocation(line: 599, column: 9, scope: !235)
!382 = !DILocation(line: 463, column: 20, scope: !351, inlinedAt: !383)
!383 = distinct !DILocation(line: 485, column: 10, scope: !344, inlinedAt: !381)
!384 = !DILocation(line: 465, column: 10, scope: !351, inlinedAt: !383)
!385 = !DILocation(line: 599, column: 7, scope: !235)
!386 = !DILocation(line: 600, column: 5, scope: !235)
!387 = !DILocation(line: 601, column: 3, scope: !236)
!388 = !DILocation(line: 601, column: 3, scope: !235)
!389 = !DILocation(line: 602, column: 1, scope: !202)
!390 = distinct !DISubprogram(name: "bpf_map_lookup_elem_", scope: !57, file: !57, line: 606, type: !391, isLocal: true, isDefinition: true, scopeLine: 606, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !396)
!391 = !DISubroutineType(types: !392)
!392 = !{!28, !393, !28}
!393 = !DIDerivedType(tag: DW_TAG_typedef, name: "uintptr_t", file: !394, line: 36, baseType: !395)
!394 = !DIFile(filename: "/lib/modules/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f/build/include/linux/types.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "e4f53ccffd110a0de86c3253570771c8", source: "#ifndef _LINUX_TYPES_H\0A#define _LINUX_TYPES_H\0A\0A#define __EXPORTED_HEADERS__\0A#include <uapi/linux/types.h>\0A\0A#ifndef __ASSEMBLY__\0A\0A#define DECLARE_BITMAP(name,bits) \5C\0A\09unsigned long name[BITS_TO_LONGS(bits)]\0A\0Atypedef __u32 __kernel_dev_t;\0A\0Atypedef __kernel_fd_set\09\09fd_set;\0Atypedef __kernel_dev_t\09\09dev_t;\0Atypedef __kernel_ino_t\09\09ino_t;\0Atypedef __kernel_mode_t\09\09mode_t;\0Atypedef unsigned short\09\09umode_t;\0Atypedef __u32\09\09\09nlink_t;\0Atypedef __kernel_off_t\09\09off_t;\0Atypedef __kernel_pid_t\09\09pid_t;\0Atypedef __kernel_daddr_t\09daddr_t;\0Atypedef __kernel_key_t\09\09key_t;\0Atypedef __kernel_suseconds_t\09suseconds_t;\0Atypedef __kernel_timer_t\09timer_t;\0Atypedef __kernel_clockid_t\09clockid_t;\0Atypedef __kernel_mqd_t\09\09mqd_t;\0A\0Atypedef _Bool\09\09\09bool;\0A\0Atypedef __kernel_uid32_t\09uid_t;\0Atypedef __kernel_gid32_t\09gid_t;\0Atypedef __kernel_uid16_t        uid16_t;\0Atypedef __kernel_gid16_t        gid16_t;\0A\0Atypedef unsigned long\09\09uintptr_t;\0A\0A#ifdef CONFIG_HAVE_UID16\0A/* This is defined by include/asm-{arch}/posix_types.h */\0Atypedef __kernel_old_uid_t\09old_uid_t;\0Atypedef __kernel_old_gid_t\09old_gid_t;\0A#endif /* CONFIG_UID16 */\0A\0A#if defined(__GNUC__)\0Atypedef __kernel_loff_t\09\09loff_t;\0A#endif\0A\0A/*\0A * The following typedefs are also protected by individual ifdefs for\0A * historical reasons:\0A */\0A#ifndef _SIZE_T\0A#define _SIZE_T\0Atypedef __kernel_size_t\09\09size_t;\0A#endif\0A\0A#ifndef _SSIZE_T\0A#define _SSIZE_T\0Atypedef __kernel_ssize_t\09ssize_t;\0A#endif\0A\0A#ifndef _PTRDIFF_T\0A#define _PTRDIFF_T\0Atypedef __kernel_ptrdiff_t\09ptrdiff_t;\0A#endif\0A\0A#ifndef _TIME_T\0A#define _TIME_T\0Atypedef __kernel_time_t\09\09time_t;\0A#endif\0A\0A#ifndef _CLOCK_T\0A#define _CLOCK_T\0Atypedef __kernel_clock_t\09clock_t;\0A#endif\0A\0A#ifndef _CADDR_T\0A#define _CADDR_T\0Atypedef __kernel_caddr_t\09caddr_t;\0A#endif\0A\0A/* bsd */\0Atypedef unsigned char\09\09u_char;\0Atypedef unsigned short\09\09u_short;\0Atypedef unsigned int\09\09u_int;\0Atypedef unsigned long\09\09u_long;\0A\0A/* sysv */\0Atypedef unsigned char\09\09unchar;\0Atypedef unsigned short\09\09ushort;\0Atypedef unsigned int\09\09uint;\0Atypedef unsigned long\09\09ulong;\0A\0A#ifndef __BIT_TYPES_DEFINED__\0A#define __BIT_TYPES_DEFINED__\0A\0Atypedef\09\09__u8\09\09u_int8_t;\0Atypedef\09\09__s8\09\09int8_t;\0Atypedef\09\09__u16\09\09u_int16_t;\0Atypedef\09\09__s16\09\09int16_t;\0Atypedef\09\09__u32\09\09u_int32_t;\0Atypedef\09\09__s32\09\09int32_t;\0A\0A#endif /* !(__BIT_TYPES_DEFINED__) */\0A\0Atypedef\09\09__u8\09\09uint8_t;\0Atypedef\09\09__u16\09\09uint16_t;\0Atypedef\09\09__u32\09\09uint32_t;\0A\0A#if defined(__GNUC__)\0Atypedef\09\09__u64\09\09uint64_t;\0Atypedef\09\09__u64\09\09u_int64_t;\0Atypedef\09\09__s64\09\09int64_t;\0A#endif\0A\0A/* this is a special 64bit data type that is 8-byte aligned */\0A#define aligned_u64 __u64 __attribute__((aligned(8)))\0A#define aligned_be64 __be64 __attribute__((aligned(8)))\0A#define aligned_le64 __le64 __attribute__((aligned(8)))\0A\0A/**\0A * The type used for indexing onto a disc or disc partition.\0A *\0A * Linux always considers sectors to be 512 bytes long independently\0A * of the devices real block size.\0A *\0A * blkcnt_t is the type of the inode's block count.\0A */\0A#ifdef CONFIG_LBDAF\0Atypedef u64 sector_t;\0Atypedef u64 blkcnt_t;\0A#else\0Atypedef unsigned long sector_t;\0Atypedef unsigned long blkcnt_t;\0A#endif\0A\0A/*\0A * The type of an index into the pagecache.\0A */\0A#define pgoff_t unsigned long\0A\0A/*\0A * A dma_addr_t can hold any valid DMA address, i.e., any address returned\0A * by the DMA API.\0A *\0A * If the DMA API only uses 32-bit addresses, dma_addr_t need only be 32\0A * bits wide.  Bus addresses, e.g., PCI BARs, may be wider than 32 bits,\0A * but drivers do memory-mapped I/O to ioremapped kernel virtual addresses,\0A * so they don't care about the size of the actual bus addresses.\0A */\0A#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\0Atypedef u64 dma_addr_t;\0A#else\0Atypedef u32 dma_addr_t;\0A#endif\0A\0Atypedef unsigned __bitwise gfp_t;\0Atypedef unsigned __bitwise fmode_t;\0A\0A#ifdef CONFIG_PHYS_ADDR_T_64BIT\0Atypedef u64 phys_addr_t;\0A#else\0Atypedef u32 phys_addr_t;\0A#endif\0A\0Atypedef phys_addr_t resource_size_t;\0A\0A/*\0A * This type is the placeholder for a hardware interrupt number. It has to be\0A * big enough to enclose whatever representation is used by a given platform.\0A */\0Atypedef unsigned long irq_hw_number_t;\0A\0Atypedef struct {\0A\09int counter;\0A} atomic_t;\0A\0A#ifdef CONFIG_64BIT\0Atypedef struct {\0A\09long counter;\0A} atomic64_t;\0A#endif\0A\0Astruct list_head {\0A\09struct list_head *next, *prev;\0A};\0A\0Astruct hlist_head {\0A\09struct hlist_node *first;\0A};\0A\0Astruct hlist_node {\0A\09struct hlist_node *next, **pprev;\0A};\0A\0Astruct ustat {\0A\09__kernel_daddr_t\09f_tfree;\0A\09__kernel_ino_t\09\09f_tinode;\0A\09char\09\09\09f_fname[6];\0A\09char\09\09\09f_fpack[6];\0A};\0A\0A/**\0A * struct callback_head - callback structure for use with RCU and task_work\0A * @next: next update requests in a list\0A * @func: actual update function to call after the grace period.\0A *\0A * The struct is aligned to size of pointer. On most architectures it happens\0A * naturally due ABI requirements, but some architectures (like CRIS) have\0A * weird ABI and we need to ask it explicitly.\0A *\0A * The alignment is required to guarantee that bits 0 and 1 of @next will be\0A * clear under normal conditions -- as long as we use call_rcu(),\0A * call_rcu_bh(), call_rcu_sched(), or call_srcu() to queue callback.\0A *\0A * This guarantee is important for few reasons:\0A *  - future call_rcu_lazy() will make use of lower bits in the pointer;\0A *  - the structure shares storage spacer in struct page with @compound_head,\0A *    which encode PageTail() in bit 0. The guarantee is needed to avoid\0A *    false-positive PageTail().\0A */\0Astruct callback_head {\0A\09struct callback_head *next;\0A\09void (*func)(struct callback_head *head);\0A} __attribute__((aligned(sizeof(void *))));\0A#define rcu_head callback_head\0A\0Atypedef void (*rcu_callback_t)(struct rcu_head *head);\0Atypedef void (*call_rcu_func_t)(struct rcu_head *head, rcu_callback_t func);\0A\0A#endif /*  __ASSEMBLY__ */\0A#endif /* _LINUX_TYPES_H */\0A")
!395 = !DIBasicType(name: "long unsigned int", size: 64, encoding: DW_ATE_unsigned)
!396 = !{!397, !398}
!397 = !DILocalVariable(name: "map", arg: 1, scope: !390, file: !57, line: 606, type: !393)
!398 = !DILocalVariable(name: "key", arg: 2, scope: !390, file: !57, line: 606, type: !28)
!399 = !DILocation(line: 606, column: 39, scope: !390)
!400 = !DILocation(line: 606, column: 50, scope: !390)
!401 = !DILocation(line: 607, column: 30, scope: !390)
!402 = !DILocation(line: 607, column: 10, scope: !390)
!403 = !DILocation(line: 607, column: 3, scope: !390)
!404 = distinct !DISubprogram(name: "bpf_map_update_elem_", scope: !57, file: !57, line: 612, type: !405, isLocal: true, isDefinition: true, scopeLine: 612, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !407)
!405 = !DISubroutineType(types: !406)
!406 = !{!36, !393, !28, !28, !25}
!407 = !{!408, !409, !410, !411}
!408 = !DILocalVariable(name: "map", arg: 1, scope: !404, file: !57, line: 612, type: !393)
!409 = !DILocalVariable(name: "key", arg: 2, scope: !404, file: !57, line: 612, type: !28)
!410 = !DILocalVariable(name: "value", arg: 3, scope: !404, file: !57, line: 612, type: !28)
!411 = !DILocalVariable(name: "flags", arg: 4, scope: !404, file: !57, line: 612, type: !25)
!412 = !DILocation(line: 612, column: 36, scope: !404)
!413 = !DILocation(line: 612, column: 47, scope: !404)
!414 = !DILocation(line: 612, column: 58, scope: !404)
!415 = !DILocation(line: 612, column: 69, scope: !404)
!416 = !DILocation(line: 613, column: 30, scope: !404)
!417 = !DILocation(line: 613, column: 10, scope: !404)
!418 = !DILocation(line: 613, column: 3, scope: !404)
!419 = distinct !DISubprogram(name: "bpf_map_delete_elem_", scope: !57, file: !57, line: 618, type: !420, isLocal: true, isDefinition: true, scopeLine: 618, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !422)
!420 = !DISubroutineType(types: !421)
!421 = !{!36, !393, !28}
!422 = !{!423, !424}
!423 = !DILocalVariable(name: "map", arg: 1, scope: !419, file: !57, line: 618, type: !393)
!424 = !DILocalVariable(name: "key", arg: 2, scope: !419, file: !57, line: 618, type: !28)
!425 = !DILocation(line: 618, column: 36, scope: !419)
!426 = !DILocation(line: 618, column: 47, scope: !419)
!427 = !DILocation(line: 619, column: 30, scope: !419)
!428 = !DILocation(line: 619, column: 10, scope: !419)
!429 = !DILocation(line: 619, column: 3, scope: !419)
!430 = distinct !DISubprogram(name: "bpf_l3_csum_replace_", scope: !57, file: !57, line: 624, type: !431, isLocal: true, isDefinition: true, scopeLine: 624, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !433)
!431 = !DISubroutineType(types: !432)
!432 = !{!36, !28, !25, !25, !25, !25}
!433 = !{!434, !435, !436, !437, !438}
!434 = !DILocalVariable(name: "ctx", arg: 1, scope: !430, file: !57, line: 624, type: !28)
!435 = !DILocalVariable(name: "off", arg: 2, scope: !430, file: !57, line: 624, type: !25)
!436 = !DILocalVariable(name: "from", arg: 3, scope: !430, file: !57, line: 624, type: !25)
!437 = !DILocalVariable(name: "to", arg: 4, scope: !430, file: !57, line: 624, type: !25)
!438 = !DILocalVariable(name: "flags", arg: 5, scope: !430, file: !57, line: 624, type: !25)
!439 = !DILocation(line: 624, column: 32, scope: !430)
!440 = !DILocation(line: 624, column: 41, scope: !430)
!441 = !DILocation(line: 624, column: 50, scope: !430)
!442 = !DILocation(line: 624, column: 60, scope: !430)
!443 = !DILocation(line: 624, column: 68, scope: !430)
!444 = !DILocation(line: 625, column: 3, scope: !430)
!445 = !DILocation(line: 627, column: 54, scope: !446)
!446 = distinct !DILexicalBlock(scope: !430, file: !57, line: 625, column: 24)
!447 = !DILocation(line: 627, column: 44, scope: !446)
!448 = !DILocation(line: 627, column: 71, scope: !446)
!449 = !DILocation(line: 627, column: 61, scope: !446)
!450 = !DILocation(line: 627, column: 14, scope: !446)
!451 = !DILocation(line: 627, column: 7, scope: !446)
!452 = !DILocation(line: 629, column: 54, scope: !446)
!453 = !DILocation(line: 479, column: 19, scope: !301, inlinedAt: !454)
!454 = distinct !DILocation(line: 629, column: 44, scope: !446)
!455 = !DILocation(line: 457, column: 19, scope: !308, inlinedAt: !456)
!456 = distinct !DILocation(line: 480, column: 10, scope: !301, inlinedAt: !454)
!457 = !DILocation(line: 459, column: 10, scope: !308, inlinedAt: !456)
!458 = !DILocation(line: 629, column: 44, scope: !446)
!459 = !DILocation(line: 629, column: 71, scope: !446)
!460 = !DILocation(line: 479, column: 19, scope: !301, inlinedAt: !461)
!461 = distinct !DILocation(line: 629, column: 61, scope: !446)
!462 = !DILocation(line: 457, column: 19, scope: !308, inlinedAt: !463)
!463 = distinct !DILocation(line: 480, column: 10, scope: !301, inlinedAt: !461)
!464 = !DILocation(line: 459, column: 10, scope: !308, inlinedAt: !463)
!465 = !DILocation(line: 629, column: 61, scope: !446)
!466 = !DILocation(line: 629, column: 14, scope: !446)
!467 = !DILocation(line: 629, column: 7, scope: !446)
!468 = !DILocation(line: 484, column: 20, scope: !344, inlinedAt: !469)
!469 = distinct !DILocation(line: 631, column: 44, scope: !446)
!470 = !DILocation(line: 463, column: 20, scope: !351, inlinedAt: !471)
!471 = distinct !DILocation(line: 485, column: 10, scope: !344, inlinedAt: !469)
!472 = !DILocation(line: 465, column: 10, scope: !351, inlinedAt: !471)
!473 = !DILocation(line: 484, column: 20, scope: !344, inlinedAt: !474)
!474 = distinct !DILocation(line: 631, column: 62, scope: !446)
!475 = !DILocation(line: 463, column: 20, scope: !351, inlinedAt: !476)
!476 = distinct !DILocation(line: 485, column: 10, scope: !344, inlinedAt: !474)
!477 = !DILocation(line: 465, column: 10, scope: !351, inlinedAt: !476)
!478 = !DILocation(line: 631, column: 14, scope: !446)
!479 = !DILocation(line: 631, column: 7, scope: !446)
!480 = !DILocation(line: 635, column: 10, scope: !430)
!481 = !DILocation(line: 635, column: 3, scope: !430)
!482 = !DILocation(line: 0, scope: !430)
!483 = !DILocation(line: 636, column: 1, scope: !430)
!484 = distinct !DISubprogram(name: "bpf_l4_csum_replace_", scope: !57, file: !57, line: 640, type: !431, isLocal: true, isDefinition: true, scopeLine: 640, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !485)
!485 = !{!486, !487, !488, !489, !490}
!486 = !DILocalVariable(name: "ctx", arg: 1, scope: !484, file: !57, line: 640, type: !28)
!487 = !DILocalVariable(name: "off", arg: 2, scope: !484, file: !57, line: 640, type: !25)
!488 = !DILocalVariable(name: "from", arg: 3, scope: !484, file: !57, line: 640, type: !25)
!489 = !DILocalVariable(name: "to", arg: 4, scope: !484, file: !57, line: 640, type: !25)
!490 = !DILocalVariable(name: "flags", arg: 5, scope: !484, file: !57, line: 640, type: !25)
!491 = !DILocation(line: 640, column: 32, scope: !484)
!492 = !DILocation(line: 640, column: 41, scope: !484)
!493 = !DILocation(line: 640, column: 50, scope: !484)
!494 = !DILocation(line: 640, column: 60, scope: !484)
!495 = !DILocation(line: 640, column: 68, scope: !484)
!496 = !DILocation(line: 641, column: 3, scope: !484)
!497 = !DILocation(line: 643, column: 54, scope: !498)
!498 = distinct !DILexicalBlock(scope: !484, file: !57, line: 641, column: 24)
!499 = !DILocation(line: 643, column: 44, scope: !498)
!500 = !DILocation(line: 643, column: 71, scope: !498)
!501 = !DILocation(line: 643, column: 61, scope: !498)
!502 = !DILocation(line: 643, column: 14, scope: !498)
!503 = !DILocation(line: 643, column: 7, scope: !498)
!504 = !DILocation(line: 645, column: 54, scope: !498)
!505 = !DILocation(line: 479, column: 19, scope: !301, inlinedAt: !506)
!506 = distinct !DILocation(line: 645, column: 44, scope: !498)
!507 = !DILocation(line: 457, column: 19, scope: !308, inlinedAt: !508)
!508 = distinct !DILocation(line: 480, column: 10, scope: !301, inlinedAt: !506)
!509 = !DILocation(line: 459, column: 10, scope: !308, inlinedAt: !508)
!510 = !DILocation(line: 645, column: 44, scope: !498)
!511 = !DILocation(line: 645, column: 71, scope: !498)
!512 = !DILocation(line: 479, column: 19, scope: !301, inlinedAt: !513)
!513 = distinct !DILocation(line: 645, column: 61, scope: !498)
!514 = !DILocation(line: 457, column: 19, scope: !308, inlinedAt: !515)
!515 = distinct !DILocation(line: 480, column: 10, scope: !301, inlinedAt: !513)
!516 = !DILocation(line: 459, column: 10, scope: !308, inlinedAt: !515)
!517 = !DILocation(line: 645, column: 61, scope: !498)
!518 = !DILocation(line: 645, column: 14, scope: !498)
!519 = !DILocation(line: 645, column: 7, scope: !498)
!520 = !DILocation(line: 484, column: 20, scope: !344, inlinedAt: !521)
!521 = distinct !DILocation(line: 647, column: 44, scope: !498)
!522 = !DILocation(line: 463, column: 20, scope: !351, inlinedAt: !523)
!523 = distinct !DILocation(line: 485, column: 10, scope: !344, inlinedAt: !521)
!524 = !DILocation(line: 465, column: 10, scope: !351, inlinedAt: !523)
!525 = !DILocation(line: 484, column: 20, scope: !344, inlinedAt: !526)
!526 = distinct !DILocation(line: 647, column: 62, scope: !498)
!527 = !DILocation(line: 463, column: 20, scope: !351, inlinedAt: !528)
!528 = distinct !DILocation(line: 485, column: 10, scope: !344, inlinedAt: !526)
!529 = !DILocation(line: 465, column: 10, scope: !351, inlinedAt: !528)
!530 = !DILocation(line: 647, column: 14, scope: !498)
!531 = !DILocation(line: 647, column: 7, scope: !498)
!532 = !DILocation(line: 651, column: 10, scope: !484)
!533 = !DILocation(line: 651, column: 3, scope: !484)
!534 = !DILocation(line: 0, scope: !484)
!535 = !DILocation(line: 652, column: 1, scope: !484)
!536 = distinct !DISubprogram(name: "probe_SyS_nanosleep_1", scope: !32, file: !32, line: 32, type: !537, isLocal: false, isDefinition: true, scopeLine: 33, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, isOptimized: true, unit: !2, retainedNodes: !564)
!537 = !DISubroutineType(types: !538)
!538 = !{!36, !539}
!539 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !540, size: 64)
!540 = distinct !DICompositeType(tag: DW_TAG_structure_type, name: "pt_regs", file: !541, line: 33, size: 1344, elements: !542)
!541 = !DIFile(filename: "/lib/modules/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f/build/arch/x86/include/asm/ptrace.h", directory: "/usr/src/kernels/4.11.3-70_fbk18_4116_g1cf3f1a0ca4f", checksumkind: CSK_MD5, checksum: "c955743f9819d098f16bdeec2ba8d718", source: "#ifndef _ASM_X86_PTRACE_H\0A#define _ASM_X86_PTRACE_H\0A\0A#include <asm/segment.h>\0A#include <asm/page_types.h>\0A#include <uapi/asm/ptrace.h>\0A\0A#ifndef __ASSEMBLY__\0A#ifdef __i386__\0A\0Astruct pt_regs {\0A\09unsigned long bx;\0A\09unsigned long cx;\0A\09unsigned long dx;\0A\09unsigned long si;\0A\09unsigned long di;\0A\09unsigned long bp;\0A\09unsigned long ax;\0A\09unsigned long ds;\0A\09unsigned long es;\0A\09unsigned long fs;\0A\09unsigned long gs;\0A\09unsigned long orig_ax;\0A\09unsigned long ip;\0A\09unsigned long cs;\0A\09unsigned long flags;\0A\09unsigned long sp;\0A\09unsigned long ss;\0A};\0A\0A#else /* __i386__ */\0A\0Astruct pt_regs {\0A/*\0A * C ABI says these regs are callee-preserved. They aren't saved on kernel entry\0A * unless syscall needs a complete, fully filled \22struct pt_regs\22.\0A */\0A\09unsigned long r15;\0A\09unsigned long r14;\0A\09unsigned long r13;\0A\09unsigned long r12;\0A\09unsigned long bp;\0A\09unsigned long bx;\0A/* These regs are callee-clobbered. Always saved on kernel entry. */\0A\09unsigned long r11;\0A\09unsigned long r10;\0A\09unsigned long r9;\0A\09unsigned long r8;\0A\09unsigned long ax;\0A\09unsigned long cx;\0A\09unsigned long dx;\0A\09unsigned long si;\0A\09unsigned long di;\0A/*\0A * On syscall entry, this is syscall#. On CPU exception, this is error code.\0A * On hw interrupt, it's IRQ number:\0A */\0A\09unsigned long orig_ax;\0A/* Return frame for iretq */\0A\09unsigned long ip;\0A\09unsigned long cs;\0A\09unsigned long flags;\0A\09unsigned long sp;\0A\09unsigned long ss;\0A/* top of stack page */\0A};\0A\0A#endif /* !__i386__ */\0A\0A#ifdef CONFIG_PARAVIRT\0A#include <asm/paravirt_types.h>\0A#endif\0A\0Astruct cpuinfo_x86;\0Astruct task_struct;\0A\0Aextern unsigned long profile_pc(struct pt_regs *regs);\0A#define profile_pc profile_pc\0A\0Aextern unsigned long\0Aconvert_ip_to_linear(struct task_struct *child, struct pt_regs *regs);\0Aextern void send_sigtrap(struct task_struct *tsk, struct pt_regs *regs,\0A\09\09\09 int error_code, int si_code);\0A\0A\0Astatic inline unsigned long regs_return_value(struct pt_regs *regs)\0A{\0A\09return regs->ax;\0A}\0A\0Astatic inline void regs_set_return_value(struct pt_regs *regs, unsigned long rc)\0A{\0A\09regs->ax = rc;\0A}\0A\0A/*\0A * user_mode(regs) determines whether a register set came from user\0A * mode.  On x86_32, this is true if V8086 mode was enabled OR if the\0A * register set was from protected mode with RPL-3 CS value.  This\0A * tricky test checks that with one comparison.\0A *\0A * On x86_64, vm86 mode is mercifully nonexistent, and we don't need\0A * the extra check.\0A */\0Astatic inline int user_mode(struct pt_regs *regs)\0A{\0A#ifdef CONFIG_X86_32\0A\09return ((regs->cs & SEGMENT_RPL_MASK) | (regs->flags & X86_VM_MASK)) >= USER_RPL;\0A#else\0A\09return !!(regs->cs & 3);\0A#endif\0A}\0A\0Astatic inline int v8086_mode(struct pt_regs *regs)\0A{\0A#ifdef CONFIG_X86_32\0A\09return (regs->flags & X86_VM_MASK);\0A#else\0A\09return 0;\09/* No V86 mode support in long mode */\0A#endif\0A}\0A\0A#ifdef CONFIG_X86_64\0Astatic inline bool user_64bit_mode(struct pt_regs *regs)\0A{\0A#ifndef CONFIG_PARAVIRT\0A\09/*\0A\09 * On non-paravirt systems, this is the only long mode CPL 3\0A\09 * selector.  We do not allow long mode selectors in the LDT.\0A\09 */\0A\09return regs->cs == __USER_CS;\0A#else\0A\09/* Headers are too twisted for this to go in paravirt.h. */\0A\09return regs->cs == __USER_CS || regs->cs == pv_info.extra_user_64bit_cs;\0A#endif\0A}\0A\0A#define current_user_stack_pointer()\09current_pt_regs()->sp\0A#define compat_user_stack_pointer()\09current_pt_regs()->sp\0A#endif\0A\0A#ifdef CONFIG_X86_32\0Aextern unsigned long kernel_stack_pointer(struct pt_regs *regs);\0A#else\0Astatic inline unsigned long kernel_stack_pointer(struct pt_regs *regs)\0A{\0A\09return regs->sp;\0A}\0A#endif\0A\0A#define GET_IP(regs) ((regs)->ip)\0A#define GET_FP(regs) ((regs)->bp)\0A#define GET_USP(regs) ((regs)->sp)\0A\0A#include <asm-generic/ptrace.h>\0A\0A/* Query offset/name of register from its name/offset */\0Aextern int regs_query_register_offset(const char *name);\0Aextern const char *regs_query_register_name(unsigned int offset);\0A#define MAX_REG_OFFSET (offsetof(struct pt_regs, ss))\0A\0A/**\0A * regs_get_register() - get register value from its offset\0A * @regs:\09pt_regs from which register value is gotten.\0A * @offset:\09offset number of the register.\0A *\0A * regs_get_register returns the value of a register. The @offset is the\0A * offset of the register in struct pt_regs address which specified by @regs.\0A * If @offset is bigger than MAX_REG_OFFSET, this returns 0.\0A */\0Astatic inline unsigned long regs_get_register(struct pt_regs *regs,\0A\09\09\09\09\09      unsigned int offset)\0A{\0A\09if (unlikely(offset > MAX_REG_OFFSET))\0A\09\09return 0;\0A#ifdef CONFIG_X86_32\0A\09/*\0A\09 * Traps from the kernel do not save sp and ss.\0A\09 * Use the helper function to retrieve sp.\0A\09 */\0A\09if (offset == offsetof(struct pt_regs, sp) &&\0A\09    regs->cs == __KERNEL_CS)\0A\09\09return kernel_stack_pointer(regs);\0A#endif\0A\09return *(unsigned long *)((unsigned long)regs + offset);\0A}\0A\0A/**\0A * regs_within_kernel_stack() - check the address in the stack\0A * @regs:\09pt_regs which contains kernel stack pointer.\0A * @addr:\09address which is checked.\0A *\0A * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\0A * If @addr is within the kernel stack, it returns true. If not, returns false.\0A */\0Astatic inline int regs_within_kernel_stack(struct pt_regs *regs,\0A\09\09\09\09\09   unsigned long addr)\0A{\0A\09return ((addr & ~(THREAD_SIZE - 1))  ==\0A\09\09(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\0A}\0A\0A/**\0A * regs_get_kernel_stack_nth() - get Nth entry of the stack\0A * @regs:\09pt_regs which contains kernel stack pointer.\0A * @n:\09\09stack entry number.\0A *\0A * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\0A * is specified by @regs. If the @n th entry is NOT in the kernel stack,\0A * this returns 0.\0A */\0Astatic inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,\0A\09\09\09\09\09\09      unsigned int n)\0A{\0A\09unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\0A\09addr += n;\0A\09if (regs_within_kernel_stack(regs, (unsigned long)addr))\0A\09\09return *addr;\0A\09else\0A\09\09return 0;\0A}\0A\0A#define arch_has_single_step()\09(1)\0A#ifdef CONFIG_X86_DEBUGCTLMSR\0A#define arch_has_block_step()\09(1)\0A#else\0A#define arch_has_block_step()\09(boot_cpu_data.x86 >= 6)\0A#endif\0A\0A#define ARCH_HAS_USER_SINGLE_STEP_INFO\0A\0A/*\0A * When hitting ptrace_stop(), we cannot return using SYSRET because\0A * that does not restore the full CPU state, only a minimal set.  The\0A * ptracer can change arbitrary register values, which is usually okay\0A * because the usual ptrace stops run off the signal delivery path which\0A * forces IRET; however, ptrace_event() stops happen in arbitrary places\0A * in the kernel and don't force IRET path.\0A *\0A * So force IRET path after a ptrace stop.\0A */\0A#define arch_ptrace_stop_needed(code, info)\09\09\09\09\5C\0A({\09\09\09\09\09\09\09\09\09\5C\0A\09force_iret();\09\09\09\09\09\09\09\5C\0A\09false;\09\09\09\09\09\09\09\09\5C\0A})\0A\0Astruct user_desc;\0Aextern int do_get_thread_area(struct task_struct *p, int idx,\0A\09\09\09      struct user_desc __user *info);\0Aextern int do_set_thread_area(struct task_struct *p, int idx,\0A\09\09\09      struct user_desc __user *info, int can_allocate);\0A\0A#endif /* !__ASSEMBLY__ */\0A#endif /* _ASM_X86_PTRACE_H */\0A")
!542 = !{!543, !544, !545, !546, !547, !548, !549, !550, !551, !552, !553, !554, !555, !556, !557, !558, !559, !560, !561, !562, !563}
!543 = !DIDerivedType(tag: DW_TAG_member, name: "r15", scope: !540, file: !541, line: 38, baseType: !395, size: 64)
!544 = !DIDerivedType(tag: DW_TAG_member, name: "r14", scope: !540, file: !541, line: 39, baseType: !395, size: 64, offset: 64)
!545 = !DIDerivedType(tag: DW_TAG_member, name: "r13", scope: !540, file: !541, line: 40, baseType: !395, size: 64, offset: 128)
!546 = !DIDerivedType(tag: DW_TAG_member, name: "r12", scope: !540, file: !541, line: 41, baseType: !395, size: 64, offset: 192)
!547 = !DIDerivedType(tag: DW_TAG_member, name: "bp", scope: !540, file: !541, line: 42, baseType: !395, size: 64, offset: 256)
!548 = !DIDerivedType(tag: DW_TAG_member, name: "bx", scope: !540, file: !541, line: 43, baseType: !395, size: 64, offset: 320)
!549 = !DIDerivedType(tag: DW_TAG_member, name: "r11", scope: !540, file: !541, line: 45, baseType: !395, size: 64, offset: 384)
!550 = !DIDerivedType(tag: DW_TAG_member, name: "r10", scope: !540, file: !541, line: 46, baseType: !395, size: 64, offset: 448)
!551 = !DIDerivedType(tag: DW_TAG_member, name: "r9", scope: !540, file: !541, line: 47, baseType: !395, size: 64, offset: 512)
!552 = !DIDerivedType(tag: DW_TAG_member, name: "r8", scope: !540, file: !541, line: 48, baseType: !395, size: 64, offset: 576)
!553 = !DIDerivedType(tag: DW_TAG_member, name: "ax", scope: !540, file: !541, line: 49, baseType: !395, size: 64, offset: 640)
!554 = !DIDerivedType(tag: DW_TAG_member, name: "cx", scope: !540, file: !541, line: 50, baseType: !395, size: 64, offset: 704)
!555 = !DIDerivedType(tag: DW_TAG_member, name: "dx", scope: !540, file: !541, line: 51, baseType: !395, size: 64, offset: 768)
!556 = !DIDerivedType(tag: DW_TAG_member, name: "si", scope: !540, file: !541, line: 52, baseType: !395, size: 64, offset: 832)
!557 = !DIDerivedType(tag: DW_TAG_member, name: "di", scope: !540, file: !541, line: 53, baseType: !395, size: 64, offset: 896)
!558 = !DIDerivedType(tag: DW_TAG_member, name: "orig_ax", scope: !540, file: !541, line: 58, baseType: !395, size: 64, offset: 960)
!559 = !DIDerivedType(tag: DW_TAG_member, name: "ip", scope: !540, file: !541, line: 60, baseType: !395, size: 64, offset: 1024)
!560 = !DIDerivedType(tag: DW_TAG_member, name: "cs", scope: !540, file: !541, line: 61, baseType: !395, size: 64, offset: 1088)
!561 = !DIDerivedType(tag: DW_TAG_member, name: "flags", scope: !540, file: !541, line: 62, baseType: !395, size: 64, offset: 1152)
!562 = !DIDerivedType(tag: DW_TAG_member, name: "sp", scope: !540, file: !541, line: 63, baseType: !395, size: 64, offset: 1216)
!563 = !DIDerivedType(tag: DW_TAG_member, name: "ss", scope: !540, file: !541, line: 64, baseType: !395, size: 64, offset: 1280)
!564 = !{!565, !566, !567, !568, !569}
!565 = !DILocalVariable(name: "ctx", arg: 1, scope: !536, file: !32, line: 32, type: !539)
!566 = !DILocalVariable(name: "__pid_tgid", scope: !536, file: !32, line: 35, type: !25)
!567 = !DILocalVariable(name: "__tgid", scope: !536, file: !32, line: 36, type: !38)
!568 = !DILocalVariable(name: "__pid", scope: !536, file: !32, line: 37, type: !38)
!569 = !DILocalVariable(name: "__data", scope: !536, file: !32, line: 45, type: !570)
!570 = distinct !DICompositeType(tag: DW_TAG_structure_type, name: "probe_SyS_nanosleep_1_data_t", file: !32, line: 16, size: 192, elements: !571)
!571 = !{!572, !573, !574}
!572 = !DIDerivedType(tag: DW_TAG_member, name: "tgid", scope: !570, file: !32, line: 20, baseType: !38, size: 32)
!573 = !DIDerivedType(tag: DW_TAG_member, name: "pid", scope: !570, file: !32, line: 21, baseType: !38, size: 32, offset: 32)
!574 = !DIDerivedType(tag: DW_TAG_member, name: "comm", scope: !570, file: !32, line: 22, baseType: !575, size: 128, offset: 64)
!575 = !DICompositeType(tag: DW_TAG_array_type, baseType: !52, size: 128, elements: !576)
!576 = !{!577}
!577 = !DISubrange(count: 16)
!578 = !DILocation(line: 32, column: 43, scope: !536)
!579 = !DILocation(line: 35, column: 26, scope: !536)
!580 = !DILocation(line: 35, column: 13, scope: !536)
!581 = !DILocation(line: 36, column: 33, scope: !536)
!582 = !DILocation(line: 36, column: 22, scope: !536)
!583 = !DILocation(line: 36, column: 13, scope: !536)
!584 = !DILocation(line: 39, column: 20, scope: !585)
!585 = distinct !DILexicalBlock(scope: !536, file: !32, line: 39, column: 13)
!586 = !DILocation(line: 39, column: 13, scope: !536)
!587 = !DILocation(line: 37, column: 21, scope: !536)
!588 = !DILocation(line: 37, column: 13, scope: !536)
!589 = !DILocation(line: 45, column: 9, scope: !536)
!590 = !DILocation(line: 45, column: 45, scope: !536)
!591 = !DILocation(line: 48, column: 16, scope: !536)
!592 = !DILocation(line: 48, column: 21, scope: !536)
!593 = !{!594, !315, i64 0}
!594 = !{!"probe_SyS_nanosleep_1_data_t", !315, i64 0, !315, i64 4, !244, i64 8}
!595 = !DILocation(line: 49, column: 16, scope: !536)
!596 = !DILocation(line: 49, column: 20, scope: !536)
!597 = !{!594, !315, i64 4}
!598 = !DILocation(line: 50, column: 9, scope: !536)
!599 = !DILocation(line: 53, column: 31, scope: !536)
!600 = !DILocation(line: 53, column: 36, scope: !536)
!601 = !DILocation(line: 53, column: 9, scope: !536)
!602 = !DILocation(line: 55, column: 1, scope: !536)
